<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# 롤링 확산 모델\n' +
      '\n' +
      'David Ruhe\n' +
      '\n' +
      'Jonathan Heek\n' +
      '\n' +
      'Tim Salimans\n' +
      '\n' +
      'Emiel Hoogeboom\n' +
      '\n' +
      '구글에서 학생 연구원으로 일한 일. \\ ({}^{1}\\) Google Deepmind, Amsterdam, Netherlands \\({}^{2}\\)University of Amsterdam, Netherlands. 대응: David Ruhe \\(<\\)david.ruhe@gmail.com\\(>\\), Jonathan Heek, Tim Salimans, Emiel Hoogeboom \\(<\\){jheek, salimans, emieth}@google.com\\(>\\).\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '확산 모델은 최근 비디오, 유체 역학 시뮬레이션 또는 기후 데이터와 같은 시간 데이터에 점점 더 많이 적용되고 있다. 이들 방법들은 일반적으로 확산 프로세스에서의 잡음의 양에 관하여 후속 프레임들을 동등하게 취급한다. 본 논문은 슬라이딩 윈도우 잡음제거 과정을 이용한 새로운 접근방법인 _Rolling Diffusion_을 탐구한다. 생성 프로세스가 전개됨에 따라 미래에 대한 더 큰 불확실성을 반영하여 시퀀스에서 나중에 나타나는 프레임에 더 많은 노이즈를 할당함으로써 확산 프로세스가 시간을 통해 점진적으로 손상되도록 한다. 경험적으로, 우리는 시간적 동역학이 복잡할 때, 롤링 확산이 표준 확산보다 우수하다는 것을 보여준다. 특히, 이 결과는 Kinetics-600 비디오 데이터 세트를 사용한 비디오 예측 작업과 혼돈 유체 역학 예측 실험에서 입증된다.\n' +
      '\n' +
      '머신러닝, ICML\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '확산 모델(Sohl-Dickstein et al., 2015; Song and Ermon, 2019; Ho et al., 2020)은 생성 모델링 분야를 크게 부스팅하였다. 그들은 DALL-E 2 (Ramesh et al., 2022), Imagen (Saharia et al., 2022), Parti (Yu et al., 2022), Stable Diffusion (Rombach et al., 2022)과 같은 대규모 텍스트-이미지 시스템을 위한 펀드를 제공했다. 확산 모델의 다른 응용은 밀도 추정, 텍스트 투 스피치, 및 이미지 편집을 포함한다(Kingma et al., 2021; Gao et al., 2023; Kawar et al., 2023).\n' +
      '\n' +
      '이러한 도메인에서 이러한 성공을 거둔 후 시간 서열에 대한 확산 모델 개발에 대한 관심이 높아졌다. 최근 주목받는 대규모 작품으로는 Imagen Video(Ho et al., 2022), Stable Diffusion Video(StabilityAI, 2023) 등이 있다. 비디오 데이터를 생성하기 위한 다른 인상적인 결과들이 예를 들어, (Blattmann et al., 2023; Ge et al., 2023; Harvey et al., 2022; Singer et al., 2022; Ho et al., 2022)에 의해 달성되었다. 비디오 외부에서의 순차 생성 모델링의 적용은 예를 들어, 유체 역학 또는 날씨 및 기후 모델링을 포함한다(Price et al., 2023; Meng et al., 2022; Lippe et al., 2023).\n' +
      '\n' +
      '이 작품들 중 많은 곳에서 흔히 볼 수 있는 것은 시간적 축을 "외적 공간 차원"으로 취급한다는 점이다. 즉, 그들은 비디오를 3D 텐서 모양\\(K\\times H\\times W\\)으로 취급한다. 이것은 몇 가지 단점이 있다. 첫째, 메모리 및 계산 요구 사항은 긴 시퀀스를 생성하고자 하는 경우 빠르게 실행할 수 없게 될 수 있다. 둘째, 일반적으로 가변적인 수의 시간 단계에 대해 _rollout_ 생성을 할 수 있는 것에 관심이 있다. 따라서, 대안적인 각도는 입력 프레임들의 시퀀스에 컨디셔닝하고 단일 출력 프레임을 시뮬레이션함으로써 완전히 자기회귀적인 접근법이며, 이는 그 후 입력 프레임들에 연결되고, 그 위에서 재귀가 계속될 수 있다. 이 경우, 계산 집약적인 매 프레임마다 노이즈 제거 확산 체인 전체를 횡단해야 한다. 추가적으로, 단일 프레임들을 반복적으로 샘플링하는 것은 빠른 자기회귀 에러 누적으로 이어진다. 프레임 블록을 공동으로 생성함으로써 중간 지면을 찾을 수 있다. 그러나, 이러한 _block-autoregressive_ 경우에, 확산 모델은 매 프레임마다 동일한 수의 잡음 제거 단계를 사용할 것이다. 이는 입력 프레임의 시퀀스가 주어지면 처음 다가오는 소수의 불확실성은 이후 프레임보다 훨씬 낮기 때문에 차선책이다. 마지막으로 두 방법 모두 이전 프레임과 공동으로만 프레임을 샘플링하며, 이는 잠재적으로 차선책 매개변수화이다.\n' +
      '\n' +
      '본 논문에서는 과거로부터 미래까지의 데이터를 명시적으로 손상시키는 방법인 _Rolling Diffusion_이라는 새로운 프레임워크를 제안한다. 이는 글로벌 확산 시간을 각 프레임에 대해 _local time_로 재매개변수화함으로써 달성된다. 이렇게 함으로써, (경계 조건으로부터 벗어나) 국부 슬라이딩 윈도우 순차 잡음 제거 프로세스에 완전히 초점을 맞출 수 있다는 것이 밝혀졌다. 이것은 위에서 언급한 문제 중 일부를 완화시키는 몇 가지 시간적 귀납적 편향을 가지고 있다.\n' +
      '\n' +
      '1. 모델은 멀리 떨어진 프레임들에 대해 낮은 주파수들(그들의 전역적 시각적 특징들에 대응하는)만을 예측하면 되는 반면, 시간적으로 가까운 프레임들에 대해 높은 주파수 세부사항들이 생성된다.\n' +
      '2. 각각의 프레임은 다수의 선행 및 후행 프레임과 함께 생성된다.\n' +
      '3. 로컬 슬라이딩 윈도우 시점으로 인해, 모든 프레임은 동일한 귀납적 바이어스를 즐기고 비디오 내의 절대 위치에 관계없이 유사한 샘플링 절차를 거친다.\n' +
      '\n' +
      '이러한 장점은 키네틱스-600 비디오 데이터 세트를 사용한 비디오 예측 실험과 카오스 유체 역학 시뮬레이션을 포함하는 실험에서 경험적으로 입증된다.\n' +
      '\n' +
      '##2 배경 : 확산 모델\n' +
      '\n' +
      '### Diffusion\n' +
      '\n' +
      '확산 모델은 확산 프로세스라고 명명된 데이터를 확률적으로 파괴하는 프로세스와 잡음 제거 프로세스라고 불리는 생성 프로세스로 구성된다. \\(\\mathbf{z}_{t}\\in\\mathbb{R}^{D}\\)는 확산 차원 \\(t\\in[0,1]\\)에 대한 잠재변수를 나타낸다. 이 작업 전반에 걸쳐 우리는 \\(t\\)을 _global (diffusion) time_라고 부를 것이다. datapoint \\(\\mathbf{x}\\in\\mathbbb{R}^{D}\\), \\(\\mathbf{x}\\sim q(\\mathbf{x})\\), 확산 과정은 \\(\\mathbf{z}_{0}\\approx\\mathbf{x}\\) 및 \\(\\mathbf{z}_{1}\\sim\\mathcal{N}(0,1)\\)의 분포를 통해 설계된다.\n' +
      '\n' +
      '\\mathcal{N}(\\mathbf{z}_{t}|\\mathbf{x}):=\\mathcal{N}(\\mathbf{z}_{t}|\\alpha_{t}\\mathbf{x},\\sigma_{t}^{2} \\mathbf{I}\\, \\tag{1}\\)\n' +
      '\n' +
      '여기서 \\(a_{t}\\)와 \\(\\sigma_{t}^{2}\\)는 \\(t\\)의 엄밀한 양의 스칼라 함수이다. We define their _signal-to-noise ratio_\n' +
      '\n' +
      '\\[\\mathrm{SNR}(t):=\\frac{\\alpha_{t}^{2}}{\\sigma_{t}^{2}}\\,, \\tag{2}\\]\n' +
      '\n' +
      '\\(t\\)에서 단조롭게 감소하다 마지막으로, \\(\\alpha_{t}^{2}+\\sigma_{t}^{2}=1\\)은 \\(a_{t}^{2}\\in(0,1]\\)과 \\(\\sigma_{t}^{2}\\in(0,1]\\)을 포함하는 분산 보존 과정에 해당한다.\n' +
      '\n' +
      '노이즈화 과정이 주어지면, 시간\\(t\\)에서 시간\\(s\\)까지 단일 데이터포인트\\(\\mathbf{x}\\)에 대한 _true_(즉, 최적) 노이즈화 분포가 주어짐을 알 수 있다(Sohl-Dickstein et al., 2015).\n' +
      '\n' +
      '\\mathbf{z}_{s}|\\mathbf{z}_{t},\\mathbf{x})=\\mathcal{N}(\\mathbf{z}_{s}|\\mu_{t\\to s}(\\mathbf{z}_{t},\\mathbf{x}),\\sigma_{t\\to s}^{2}\\mathbf{I}\\, \\tag{3}\\t}\\t},\\mathbf{n}(\\mathbf{z}_{s}|\\mu_{t},\\mathbf{x})\\mathcal{N}(\\mathbf{z}_{s}|\\mu_{t},\\mathbf{x}),\\sigma_{t\\to s}^{2}\\mathbf{I}\\,\\tag{3}\\t}\n' +
      '\n' +
      '여기서 \\(\\mu\\) 및 \\(\\sigma^{2}\\)은 \\(t\\), \\(s\\), \\(\\mathbf{x}\\) 및 \\(\\mathbf{z}_{t}\\)의 _analytical_mean 및 variance 함수이다. 매개변수화된 생성과정 \\(p_{\\theta}(\\mathbf{z}_{s}|\\mathbf{z}_{t})\\)은 신경망 \\(f_{\\theta}:\\mathbb{R}^{D}\\times[0,1]\\to\\mathbb{R}^{D}\\)을 통해 \\(\\mathbf{x}\\)으로 정의된다. 즉, 우리는 설정\n' +
      '\n' +
      '\\[p_{\\theta}(\\mathbf{z}_{s}|\\mathbf{z}_{t}):=q(\\mathbf{z}_{s}|\\mathbf{z}_{t},\\mathbf{x}=f_{\\theta}(\\mathbf{z}_{t},t))\\, \\tag{4}\\)\n' +
      '\n' +
      '확산 목표는 확산 프로세스와 잡음 제거 프로세스 사이의 KL-확산, 즉 \\(\\mathrm{KL}(q(\\mathbf{x},\\mathbf{z}_{0},\\dots,\\mathbf{z}_{1})|p(\\mathbf{x},\\mathbf{z}_{0},\\dots,\\mathbff{z}_{1}))로 간단히 표현할 수 있다(Kingma et al., 2021):\n' +
      '\n' +
      '=\\mathbb{E}_{t\\sim U(0,1),\\mathbf{e}\\sim\\mathcal{N}(0,1)}\\left[w(t)||\\mathbf{x}-f_{\\theta}(\\mathbf{z}_{t,\\mathbf{e},t||^{2}\\right]\\]\\[\\quad+\\mathcal{L}_{\\mathrm{prior}+\\mathcal{L}_{\\mathrm{data}\\],\\tag{5}\\t}\\t}\n' +
      '\n' +
      '여기서 \\(\\mathcal{L}_{\\mathrm{prior}\\) 및 \\(\\mathcal{L}_{\\mathrm{data}\\)은 통상적으로 무시할 수 있다. 가중치 \\(w(t)\\)는 자유롭게 지정될 수 있다. 실제로 손실의 특정 가중치는 더 나은 샘플 품질을 초래하는 것으로 밝혀졌다(Ho et al., 2020). 이는 예를 들어, \\(w(t)=\\mathrm{SNR}(t)\\에 해당하는 _e-loss_(Ho et al., 2020)의 경우이다.\n' +
      '\n' +
      '### 시간 데이터 확산\n' +
      '\n' +
      '잠재적으로 무한한 시간 데이터 생성에 관심이 있다면, 이전에 생성된 데이터의 조건부 확장(autoregressive)을 고려해야 한다. 즉, 시간 인덱스 \\(k\\)에서 초기 샘플 \\(\\mathbf{x}^{k}\\sim q(\\mathbf{x})\\)이 주어지면, 우리는 조건부 분포 \\(p(\\mathbf{x}^{k+1}|\\mathbf{x}^{k})\\을 추정하여 샘플링하고자 한다. 그런 다음 이 프로세스를 임의의 길이의 비디오로 확장할 수 있습니다. 제1절에서 논의한 바와 같이, 이러한 조건부 분포를 추정하기 위해 어떤 종류의 모수화 선택이 최적인지는 아직 명확하지 않다. 또한, 시간적 유도 바이어스는 전형적으로 잡음 제거 프로세스로 베이킹되지 않는다.\n' +
      '\n' +
      '##3 롤링 확산 모델\n' +
      '\n' +
      '우리는 시간의 화살표를 (de)noising 과정과 병합하는 _rolling diffusion model_을 소개한다. 이를 공식화하기 위해서는 먼저 글로벌 확산 모델에 대해 논의해야 한다. 우리는 글로벌 프로세스의 사소한 부분만이 현지에서 발생한다는 것을 알게 될 것이다. 잡음 스케줄을 국부적으로 정의하는 것은 결과 모델이 프레임 수\\(K\\)에 의존하지 않고 무한히 풀릴 수 있기 때문에 유리하다.\n' +
      '\n' +
      '세계적인 관점에서\n' +
      '\n' +
      '\\(\\mathbf{x}\\in\\mathbb{R}^{D\\times K}\\)을 시계열 데이터포인트로 하자. 여기서 \\(K\\)은 프레임의 수와 각 프레임의 차원성을 나타낸다. 롤링 확산을 허용하는 핵심 아이디어는 확산 시간\\(t\\)을 프레임-의존적_로컬(프레임-의존적) 시간_: 즉, 확산 시간\\(t\\)의 _재매개변수화_이다.\n' +
      '\n' +
      '\\[t\\mapsto t_{k}\\,. \\tag{6}\\]\n' +
      '\n' +
      '우리는 여전히 모든 \\(k\\in\\{0,\\dots,K-1\\}\\)에 \\(t_{k}\\in[0,1]\\)을 필요로 한다. 또한, 우리는 여전히 신호 대 잡음 스케줄이 단조롭게 감소하여 잘 정의된 확산 프로세스를 보장한다. 그러나 우리는 이제(효과적으로) 각 프레임에 대해 다른 신호 대 잡음 스케줄을 가지고 있다. 본 논문에서는 또한 항상 \\(t_{k}\\leq t_{k+1}\\)을 갖는다. 즉, 프레임의 로컬 시간은 다음 프레임의 로컬 시간보다 작다. 이는 자연스러운 시간적 귀납적 편향이라는 미래 프레임에 더 많은 노이즈를 추가한다는 것을 의미한다. 이것은 엄격하게 요구되지 않는다; 또한 역-시간 유도성 바이어스 또는 혼합물을 가질 수 있다. 이러한 재매개변수화의 예가 도 1(좌측)에 도시되어 있다. 우리는 전역 확산 시간\\(t\\)(수직축)과 프레임 인덱스\\(k\\)(수평축)을 갖는 지도를 묘사하고, 색강도로 표시된 국부 시간\\(t_{k}\\)을 계산한다.\n' +
      '\n' +
      'Forward process 이제 local time을 이용하여 forward process를 재정의한다:\n' +
      '\n' +
      '[q(\\mathbf{z}_{t}|\\mathbf{x}):=\\prod_{k=0}^{K-1}\\mathcal{N}(\\mathbf{z}_{t}^{k}|\\alpha_{t_{k}}}\\mathbf{x}^{k},\\sigma_{t_{k}}^{2}\\mathbf{I}\\, \\tag{7}\\t}\\mathcal{N}(\\mathbf{z}_{t}}}\\mathbf{x}^{k},\\sigma_{t}{t}}^{2}\\mathbf{I})\\,\\tag{7}\\t}\\mathccal{N}(\\mathbf{z}_{t}}}\\mathbf{x}}\\alpha_{t}}}\\mathbf{x}}\\k},\\sigma_{t}}^{k}}\\mathbf{i}}\\t\n' +
      '\n' +
      '여기서 우리는 이전으로부터 \\(\\alpha\\) 및 \\(\\sigma\\) 함수(현재 \\(t_{k}\\)에서 국부적으로 평가됨)를 _reuse_the\\(\\alpha\\) 및 \\(\\sigma\\)할 수 있다. 여기서 \\(\\mathbf{x}^{k}\\)는 \\(\\mathbf{x}\\)의 \\(k\\)번째 프레임을 의미한다.\n' +
      '\n' +
      'Tuple \\((s,t)\\), \\(s\\in[0,1]\\), \\(t\\in[0,1]\\), \\(s\\leq t\\), \\(k\\in\\{0,\\dots,K-1\\}\\)을 세 가지 범주로 나눌 수 있다.\n' +
      '\n' +
      '(s,t) :=\\left\\{k\\mid s_{k}=t_{k}=0\\right\\}, \\tag{8}\\] \\[\\text{noise}(s,t) :=\\left\\{k\\mid s_{k}=t_{k}=1\\right\\},\\](9) \\[\\text{win}(s,t) :=\\left\\{k\\mid s_{k}\\in[0,1),t_{k}\\in(s_{k},1]\\right\\}. \\tag{10}\\\\text{noise}(s,t) :=\\left\\{k\\mid s_{k}=t_{k}=1\\right\\},\\](9) \\[\\text{win}(s,t) :=\\left\\{k\\mid s_{k}\\in[0,1),t_{k}\\in(s_{k},1]\\right\\}.\n' +
      '\n' +
      '이 분류는 그림 1에 표시된 일정을 사용하여 동기화할 수 있다. 예를 들어 \\(t=0.5\\) 및 \\(s=0.375\\)이 주어지면 첫 번째 프레임 \\(k=0\\)이 첫 번째 범주에 속함을 알 수 있다. 이 시점에서 \\(\\mathbf{z}_{t_{0}}=\\mathbf{z}_{s_{0}}\\)는 \\(\\lim_{t\\to 0^{+}}\\log\\operatorname{SNR}(t)=\\infty\\임을 감안할 때 동일하다. 반면에 마지막 프레임인 \\(k=K-1\\)(그림의 31\\)은 두 번째 범주, 즉 \\(\\mathbf{z}_{t_{K-1}}\\)과 \\(\\mathbf{z}_{s_{K-1}}\\)은 독립적인 표준 가우시안으로서 분포하는데, 이는 \\(\\lim_{t\\to 1^{-}}\\log\\operatorname{SNR}(t)==-\\infty\\이다. 마지막으로 프레임\\(k=16\\)은 세 번째, 가장 흥미로운 범주인 슬라이딩 윈도우에 속한다. 이와 같이, _true_ denoising 과정이 다음과 같이 인수분해될 수 있음을 관찰한다:\n' +
      '\n' +
      '\\mathbf{z}_{s}|\\mathbf{z}_{s},\\mathbf{x})=q(\\mathbf{z}_{s}^{\\text{clean}|\\mathbf{z}_{t},\\mathbf{x})q(\\mathbf{z}_{s}^{\\text{noise}|\\mathbf{z}_{t},\\mathbf{x})q(\\mathbf{z}_{s}^{\\text{win}|\\mathbf{z}_{t},\\mathbf{x})=q(\\mathbf{z}_{s}^{\\text{win}|\\mathbf{z}_{t},\\mathbf{x}}q(\\mathbf{z}_{s}}}|\\tag{11}\\mathbf{z}_{s}}}\n' +
      '\n' +
      '이는 모델링이 필요한 프레임만 창에 있음을 알 수 있기 때문에 도움이 된다. 즉, 첫 번째 요인은\n' +
      '\n' +
      '\\[q(\\mathbf{z}_{s}^{\\text{clean}}|\\mathbf{z}_{t},\\mathbf{x})=\\prod_{k\\in\\text{clean}(s,t}}\\delta(\\mathbf{z}_{s}^{k}|\\mathbf{z}_{t}^{k}},\\tag{12}\\t}},\\prod_{k\\in\\text{clean}(s,t}}\\delta(\\mathbf{z}_{s}^{k}}|\\mathbf{z}_{t}^{k}},\\tag{12}\\t}}\n' +
      '\n' +
      '즉, \\(\\mathbf{z}_{t}^{k}\\)가 이미 잡음이 없다면, \\(\\mathbf{z}_{s}^{k}\\)도 잡음이 없을 것이다. 두 번째 요인과 관련하여, 우리는 그것들이 모두 독립적으로 정규 분포를 따르는 것을 본다:\n' +
      '\n' +
      '\\[q(\\mathbf{z}_{s}^{\\text{noise}}|\\mathbf{z}_{t},\\mathbf{x})=\\prod_{k\\in\\text{noise}(s,t}\\mathcal{N}(\\mathbf{z}_{s}^{k}|0,\\mathbf{I}). \\tag{13}\\t}\n' +
      '\n' +
      '간단히 말해서, 이 경우 \\(\\mathbf{z}_{s}^{k}\\)는 독립적인 잡음이며 데이터에 전혀 의존하지 않는다. 마지막으로, 세 번째 팩터는 진정한 비-사소한 잡음 제거 프로세스를 갖는다:\n' +
      '\n' +
      '\\mathbf{z}_{s}^{\\text{win}|\\mathbf{z}_{s},\\mathbf{x})=\\prod_{k\\in\\text{win}(s,t}}\\mathcal{N}(\\mathbf{z}_{s}^{k}|\\mu_{t_{k}\\to s_{k}}(\\mathbf{z}_{t}^{k},\\mathbf{x}^{k}},\\sigma_{t}_{k}\\to s_{k}}^{2}\\mathbf{I})\\mathcal{N}(\\mathbf{z}_{z}_{s}^{k}\\to s_{k}}(\\mathbf{z}_{t}^{k},\\mathbf{x}\\to s_{k}}^{i})\\mathbf{z}_{z}^{k},\\sigma_{t}\\to s_{\n' +
      '\n' +
      '여기서 \\(\\mu_{t_{k}\\to s_{k}}\\) 및 \\(\\sigma_{t_{k}\\to s_{k}^{2}\\)는 분석 평균 및 분산 함수이다. 그런 다음 생성 프로세스를 최적으로(w.r.t. a KL-발산) 유사하게 인수분해할 수 있다는 점에 유의하라:\n' +
      '\n' +
      '|\\mathbf{z}_{s}|\\mathbf{z}_{s}:=p(\\mathbf{z}_{s}^{\\text{clean}|\\mathbf{z}_{t})p(\\mathbf{z}_{s}^{\\text{noise}|\\mathbf{z}_{t})p_{\\theta}(\\mathbf{z}_{s}^{\\text{win}|\\mathbf{z}_{t}}|\\tag{14}\\t}\n' +
      '\n' +
      'with \\(p(\\mathbf{z}_{s}^{\\text{clean}|\\mathbf{z}_{t}):=\\prod_{k\\in\\text{clean}(s,t)}\\delta(\\mathbf{z}_{s}^{k}|\\mathbf{z}_{t}^{k})) 및 \\(p(\\mathbf{z}_{s}^{text{noise}|\\mathbf{z}_{t}):=\\prod_{k\\in\\text{noise}(s,t)}\\mathcal{N}(\\mathbf{z}_{s}^{k}|0,\\mathbf{I})\\. 그 후 생성 과정의 유일한 \'흥미롭기\' 매개변수화된 부분은\n' +
      '\n' +
      '|\\mathbf{z}_{s}^{\\text{win}|\\mathbf{z}_{z}_{t}):=\\prod_{k\\in\\text{win}(s,t}q(\\mathbf{z}_{s}^{k}|\\mathbf{z}_{t},\\mathbf{x}^{k}=f_{\\theta}(\\mathbf{z}_{t},t_{k}))\\tag{15}\\t}\n' +
      '\n' +
      '그림 1: 왼쪽: 글로벌 롤링 확산 프로세스 및 로컬 시간 재매개변수화의 그림입니다. 전체 확산 시간\\(t\\)(수직축)은 프레임\\(k\\)(수평축)에 대한 국부 시간\\(t_{k}\\)에 매핑된다. 그리고 확산 파라미터 \\(\\alpha_{t_{k}}\\)와 \\(\\sigma_{t_{k}}\\)을 계산하기 위해 로컬 시간을 사용한다. 오른쪽에서, 우리는 프레임 인덱스 \\(w\\)에 기초하여 프레임들의 각각의 시퀀스에 동일한 _local_ 스케줄이 어떻게 적용될 수 있는지를 보여준다. 생성 과정을 샘플링하는 데 있어 자명하지 않은 부분은 시퀀스 위로 이동함에 따라 슬라이딩 윈도우에서만 발생한다.\n' +
      '\n' +
      '즉, 우리는 생성 과정을 슬라이딩 윈도우_에 있는 프레임들에만 집중할 수 있다. 마지막으로, 과거에는 멀리 떨어져 있는 프레임들이 현재 프레임과 독립적일 가능성이 있고, 이러한 과도한 컨디셔닝은 계산상의 제약을 초과하기 때문에, \\(t_{k}\\) = 0인 모든 \\(\\mathbf{z}_{t}^{k}\\)에 모델을 조정하지 않는 _choose_를 선택할 수 있다는 점에 유의한다. 그래서 우리는\n' +
      '\n' +
      '\\[p_{\\theta}(\\mathbf{z}_{s}^{\\text{win}}|\\mathbf{z}_{t})=p_{\\theta}(\\mathbf{z}_{s}^{\\text{win}}|\\mathbf{z}_{t}^{\\text{clean},\\mathbf{z}_{t}^{\\text{win}}},\\tag{16}\\\n' +
      '\n' +
      '실제로, 우리는 여분의 컨디셔닝에 사용되는 \\(\\mathbf{z}_{s}^{\\text{win}}|\\mathbf{z}_{t}^{\\text{clean},\\mathbf{z}_{t}^{text{win}}):=p_{\\theta}(\\mathbf{z}_{s}^{\\text{win}}|\\mathbf{z}^{\\widehat{\\text{clean}},\\mathbf{z}_{t}^{\\text{win}})의 특정(잠재적 빈) 서브세트를 나타낸다. 이는 통상적으로 현재의 슬라이딩 윈도우보다 약간 앞선 몇 개의 프레임들을 포함한다.\n' +
      '\n' +
      '부록 D에서 우리는 위의 내용을 공식화하고 객관적인 결과를 보여준다.\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\text{win},\\theta}(\\mathbf{x}):= \\mathbb{E}_{t\\sim U(0,1),\\mathbf{\\epsilon}\\sim\\mathcal{N}(0,1}\\left[L_{\\text{win},\\theta}(\\mathbf{x};t,\\mathbf{\\epsilon})\\right] \\tag{17}\\t.\n' +
      '\n' +
      'with\n' +
      '\n' +
      '\\sum_{k\\in\\text{win}(t)}a(L_{\\text{win},\\theta}:=\\sum_{k\\in\\text{win}(t)}a(t_{k})||\\mathbf{x}^{k}-f_{\\theta}^{k}(\\mathbf{z}_{t,\\mathbf{\\epsilon}}^{\\text{win},\\widehat{\\mathbf{z}_{t,\\mathbf{\\epsilon}}^{\\text{clean}},t)||^{2}\\,\\heta}}\n' +
      '\n' +
      '명시적 편의를 위해 몇 가지 논증을 억제하는 곳.\n' +
      '\n' +
      '그림 1을 다시 관찰합니다. 트레이닝이 완료된 후, 우리는 이미지를 왼쪽 상단에서 오른쪽 하단으로 슬라이딩 윈도우로 횡단함으로써 생성 모델에서 본질적으로 샘플링할 수 있다. 이를 통해 다음 섹션에서 논의한 바와 같이 지역 환경에 완전히 집중할 수 있다.\n' +
      '\n' +
      '지역적 관점\n' +
      '\n' +
      '마지막 섹션에서 롤링 확산을 통해 생성 과정에 집중할 수 있을 뿐만 아니라 슬라이딩 창에 있는 프레임에만 훈련할 수 있음을 확인했다. 따라서, 본 논문에서는 기존의 제약조건을 만족하는 \\(t_{\\text{win}}(t):[0,1]\\rightarrow[0,1]\\)는 시간재매개변수를 나타낸다. 그러나 공유된 시간 성분 \\(t\\)이 이제 로컬로 정의됩니다. 구체적으로, 잡음 제거 체인을 \\(t=1\\)에서 \\(t=0\\)으로 실행하는 것은 첫 번째 프레임이 완전히 잡음이 없고 다음 프레임들은 여전히 약간의 잡음을 포함하도록 롤링 윈도우를 샘플링할 뿐이다. 대조적으로, 앞서 설명한 글로벌 프로세스는 전체 비디오를 잡음 제거한다.\n' +
      '\n' +
      '순수 압연 확산 모델의 설계 공간은 원칙적으로 방대하다. 이러한 이유로 우리는 슬라이딩 윈도우 샘플링 절차를 허용하기 위해 신호 대 잡음 일정에 대한 초기 제약 사항 외에도 다음과 같은 합리적인 가정을 한다. 우리는 \\(t_{\\text{win}}\\)을 원한다:\n' +
      '\n' +
      '1. _local_, 즉 슬라이딩 윈도우의 다양한 위치들에 걸친 파라미터화를 그들의 절대 위치들과 무관하게 공유 및 재사용할 수 있게 한다는 것을 의미한다.\n' +
      '2. 윈도우_의 이동 하에서 일치하며, 이는 다음 프레임의 \\(t_{\\text{win}}(1)\\)이 \\(t_{\\text{win}}(0)\\)과 같아야 함을 의미한다.\n' +
      '\n' +
      '슬라이딩 윈도우의 크기는 \\(W<K\\), 프레임의 국부지수는 \\(w\\in\\{0,\\dots,W-1\\}\\)으로 한다. 첫 번째 가정을 만족하기 위해 지역 지수 \\(w\\)로 일정을 정의한다(그림 1(오른쪽) 참조).\n' +
      '\n' +
      '\\[t_{\\text{win}}:=t_{w}^{W}, \\tag{18}\\]\n' +
      '\n' +
      '여기서 \\(t_{w}^{W}:[0,1]\\rightarrow[0,1]\\)는 단조롭게 증가하는 함수이다. 둘째로, 우리는 \\(t_{w}^{W}\\)가 반드시 가져야 한다는 것을 안다.\n' +
      '\n' +
      '\\[t_{w}^{W}=g((w+t)/W), \\tag{19}\\]\n' +
      '\n' +
      '(g:[0,1]\\rightarrow[0,1]\\). 우리는 때때로 표기적 편의를 위해 \\(W\\)를 억제할 것이다. 파라미터화의 로컬리티로 인해, 프로세스는 테스트 시간에 무한정 펼쳐질 수 있다는 점에 유의한다.\n' +
      '\n' +
      '이 작업에서 선형 재매개변수는 일반적으로 \\(g:=\\mathrm{id}\\)을 넣는다.\n' +
      '\n' +
      '\\[t_{w}^{\\text{lin}}=(w+t)/W\\,. \\tag{20}\\]\n' +
      '\n' +
      '여기서 \\(w\\in\\{0,\\dots,W-1\\}\\). 이 로컬 스케줄이 프레임의 각 시퀀스에 적용되는 방법에 대한 설명은 그림 1(오른쪽)을 참조하십시오. 잘 봐둬\n' +
      '\n' +
      '\\[t_{w}^{\\text{lin}}\\in[w/W,(w+1)/W]\\subseteq[0,1]\\,. \\tag{21}\\]\n' +
      '\n' +
      '깨끗한 컨디셔닝 프레임을 포함하도록 선형 로컬 시간을 연장할 수 있다. \\(n_{\\text{cln}}\\)는 깨끗한 프레임의 수(하이퍼파라미터로서 선택됨)를 나타내고, 그 다음 프레임에 대한 로컬 시간\\(w\\)은 다음과 같다.\n' +
      '\n' +
      '\\[t_{w}^{\\text{lin}}(n_{\\text{cln}}):=\\mathrm{clip}\\left(\\frac{w+t-n_{\\text{cln}}}{W-n_{\\text{cln}}\\right)\\, \\tag{22}\\\n' +
      '\n' +
      '여기서 \\(\\mathrm{clip}:\\mathbb{R}\\rightarrow[0,1]\\) 클립 값은 0에서 1 사이이다. 이것이 식 (19)와 모순되는 것처럼 보일 수 있지만, 이것은 간단한 재해석이다.\n' +
      '\n' +
      '### Boundary conditions\n' +
      '\n' +
      '순수하게 지역적 관점에서 프레이밍 롤링 확산은 훈련 및 샘플링에 편리하지만 슬라이딩 창 경계에 합병증을 도입한다. 즉, 선형 로컬 시간 재매개변수화\\(t_{w}^{\\text{lin}\\)가 주어지면, 확산시간\\(t\\)이 1에서 0까지, 로컬 시간이\\((\\frac{1}{W},\\frac{2}{W},\\dots\\frac{W}{W})\\)에서 \\((\\frac{0}{W},\\frac{1}{W},\\dots,\\frac{W-1}{W})\\)으로 주어진다. 시각적으로 그림 1에서 롤링 샘플링 절차는 이동 시 프레임의 로컬 시간이 불변으로 유지되도록 대각선 위에서 왼쪽에서 오른쪽으로 선형으로 슬라이딩 창을 이동하는 것으로 볼 수 있다. 그러나, 이것은 윈도우를 매우 좌측 에지에 배치하는 것이 여전히 부분적으로 잡음 제거된 프레임들을 갖는 결과를 초래한다는 것을 의미한다. 이것은 이 로컬 설정에서 신호 대 잡음비가 결코 최소, 즉 전체 잡음에서 최소화되지 않는다는 것을 의미한다.\n' +
      '\n' +
      '이를 고려하기 위해 이 경계 조건을 처리할 수 있는 추가 스케줄이 있는 롤링 확산 모델을 _co-train_이다.\n' +
      '\n' +
      '\\[t_{w}^{\\text{init}}:=\\operatorname{clip}\\left(\\frac{w}{W}+t\\right) \\tag{23}\\]\n' +
      '\n' +
      '이 _init_ 노이즈 스케줄은 랜덤 노이즈로부터 시작할 수 있고, "롤링 상태"의 비디오를 생성한다. 이 스케줄은 슬라이딩 윈도우 노이즈 스케줄로 사용될 수 없지만, 나중에 볼 수 있듯이 \\(t_{w}^{\\text{lin}}\\)을 포함한다. 즉, 확산 시간 \\(t=1\\)에서, 이것은 모든 프레임들을 최대 노이즈에 넣을 것이고, \\(t=0\\)에서 프레임들은 롤링 상태에 있을 것이다. 정확하게는 국부적 시간\\((1,1,\\dots 1)\\)에서 시작하여 \\((0,\\frac{1}{W},\\frac{2}{W},\\dots,\\frac{W-1}{W})\\)으로 시작한다. 시각적 관점에서 보면, 그림 1에서 이는 윈도우를 좌측 상단 모서리에 위치시키고 대각선으로 계속하기 위해 사용될 수 있는 롤링 상태에 도달할 때까지 _vertically_ 아래로 이동시키는 것에 해당한다.\n' +
      '\n' +
      '도메인\\([0,\\frac{1}{W}]\\)에서 이 스케줄은 이전의 로컬 스케줄\\(t_{w}^{\\text{lin}\\)을 특수한 경우로 포함하고 있다. 이는 이 모델이 무한정 롤아웃할 수 있을 뿐만 아니라 경계를 처리할 수 있도록 \\(t_{w}^{\\text{init}\\)만으로 훈련될 수 있음을 의미한다. 그러나, \\(t_{w}^{\\text{lin}}\\)의 일정은 훈련 동안 (t\\sim U(0,1)\\)의 시간 중 \\(1/W\\)만 선택된다는 점에 유의한다. 대조적으로, 이 스케줄은 첫 번째 \\(W\\) 프레임에서의 경계 조건을 제외하고 테스트 시간에 거의 독점적으로 사용된다. 따라서 훈련 중 두 일정을 모두 포함하는 것이 유익하다는 것을 알게 되었습니다. 이 혼합은 두 스케줄 중 선택 확률을 제어하는 베르누이 하이퍼파라미터 \\(\\beta\\)를 기반으로 한 스케줄 또는 다른 스케줄을 샘플링함으로써 달성된다.\n' +
      '\n' +
      '### Local training\n' +
      '\n' +
      '우리는 임의의 SNR 스케쥴과 앞서 언급한 재매개변수를 이용하여 \\(\\alpha_{t_{w}\\)과 \\(\\sigma_{t_{w}\\)을 계산할 수 있는 \\(\\mathbf{x}\\in\\mathbb{R}^{D\\times W}\\) 프레임들로 비디오들을 버퍼링한다. (n_{text{cln}}>0\\)일 때 우리는 암묵적으로 여분의 깨끗한 컨디셔닝을 얻는다. \\(\\mathbf{z}\\in\\mathbb{R}^{D\\times W}\\)으로 하자.\n' +
      '\n' +
      '[q(\\mathbf{z}_{t}|\\mathbf{x}):=\\prod_{w=0}^{W}\\mathcal{N}(\\mathbf{z}_{t}^{w}|\\alpha_{t}{w}}\\mathbf{x}^{w},\\sigma_{t}{w}}^{2}\\mathbf{I}), \\tag{24}\\text}\\text}\\text}\n' +
      '\n' +
      '목표는 이제 된다.\n' +
      '\n' +
      '\\mathcal{L}_{\\text{loc},\\theta}(\\mathbf{x}):=\\mathbb{E}_{t\\sim U(0,1),\\mathbf{e}\\sim\\mathcal{N}(0,1)}\\left[L_{\\text{loc},\\theta}(\\mathbf{x};t,\\mathbf{\\epsilon})\\right], \\tag{25}\\t.\n' +
      '\n' +
      '어디에 두었는지\n' +
      '\n' +
      '\\[L_{\\text{loc},\\theta}(\\mathbf{x},t,\\mathbf{\\epsilon}):=\\sum_{w=0}^{W}a(t_{w})||\\mathbf{x}^{w}-f_{\\theta}^{w}(\\mathbf{z}_{t,\\mathbf{\\epsilon};t)||^{2}\\,.\\tag{26}\\)\n' +
      '\n' +
      '학습 및 샘플링 절차는 알고리즘 2, 알고리즘 3, 알고리즘 1에 요약되어 있으며, 그림 6의 롤링 샘플링 루프를 시각적으로 제공한다.\n' +
      '\n' +
      '```\n' +
      '(p_{\\theta}\\), (n_{\\text{cln}\\), \\(\\mathbf{z}_{0}\\leftarrow\\(t=1,(T-1)/T,\\dots,\\mathbf{z}_{0}\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\cup\\\n' +
      '```\n' +
      '\n' +
      '**알고리즘 1** 롤링확산 : 롤아웃\n' +
      '\n' +
      '## 4 비디오 확산\n' +
      '\n' +
      '비디오 확산은 픽셀 공간(Ho et al., 2022; Singer et al., 2022) 및 잠재 공간(Blattmann et al., 2023; Ge et al., 2023; He et al., 2022; Yu et al., 2023c)에서 직접 연구 및 적용되었으며, 후자는 전형적으로 경험적으로 약간 더 효과적이다. 또한, 이러한 영상들은 자기회귀적 확장을 고려하지 않고 2차원 영상 설정을 3(공간 차원 2개, 시간 차원 1개)으로 확장한다.\n' +
      '\n' +
      '도 2: 샘플 콜모고로프 유동 롤아웃. 우리는 지상-진실 구조가 초기에 보존되지만 모델이 나중에 실제 데이터와 다르다는 것을 관찰한다. 그럼에도 불구하고 모델은 훨씬 나중에 시퀀스에서 새로운 난류 역학을 생성할 수 있다.\n' +
      '\n' +
      '비디오 생성의 테스트-타임 언롤링을 구체적으로 처리하는 방법에는 Yang et al.(2023); Harvey et al.(2022)이 있다. 과거 프레임이 주어진 미래 프레임의 조건부 분포를 직접 매개변수화하는 것이 무조건 확산 모델의 잡음 제거 스케줄을 적응시키는 것보다 바람직하다(Harvey et al., 2022; Tashiro et al., 2021). 이러한 이전 접근 방식은 롤링 확산과 달리 훈련 절차에서 시간 개념을 명시적으로 도입한 적이 없다. 구체적으로, Harvey et al.(2022)은 이러한 다양한 컨디셔닝 스킴들을 비교하지만, 시간적으로 적응된 잡음 스케줄을 명시적으로 고려하지 않는다.\n' +
      '\n' +
      '### 기타 시계열 확산 모형\n' +
      '\n' +
      '비디오와는 별개로, 순차 확산 모델은 오디오(Kong et al., 2021), 텍스트(Li et al., 2022)와 같은 다른 시계열 데이터에도 적용되었지만, 기상 데이터(Price et al., 2023) 또는 유체 역학(Kohl et al., 2023)에도 과학적으로 적용되었다. Lippe et al.(2023)은 확산-모티브 디노이징 절차를 통합하면 학습된 수치 PDE 솔버 에뮬레이터를 사용할 때 일반적으로 손실되는 고주파 정보를 복구하는 데 도움이 될 수 있음을 보여준다. 마지막으로 Wu et al.(2023)은 텍스트 생성에 중점을 두고 특화된 소음 스케줄을 가진 자기회귀 모델도 연구한다.\n' +
      '\n' +
      '## 5 Experiments\n' +
      '\n' +
      '다양한 도메인의 데이터를 사용하여 실험을 수행하고 여러 컨디셔닝 설정을 탐색한다. 우리의 모든 실험에서, 우리는 표준 확산과 롤링 확산 모두에 대해 동일한 파라미터를 갖는 _Simple Diffusion_ 아키텍처(Hoogeboom et al., 2023)를 사용한다. 특히, 2차원 공간 컨볼루션 블록을 사용한 후, 가장 깊은 층에서, 공간적 및 시간적으로 동작(즉, 참석)하는 트랜스포머 블록을 갖는다.\n' +
      '\n' +
      '### Kolmogorov Flow\n' +
      '\n' +
      '먼저 JaxCFD(Kochkov et al., 2021; Dresdner et al., 2022)로부터 시뮬레이션된 유체 역학에 대한 실험을 실행한다. 구체적으로 비압축성 Navier-Stokes 방정식의 인스턴스인 _Kolmogorov flow_를 사용한다. 최근 기계 학습 모델을 사용하여 고전적인 수치 PDE 적분기를 에뮬레이트하는 것에 대한 관심이 증가하고 있다. 다양한 결과들은 이들이 초기 조건들이 복잡한 시스템들로부터 높은 정밀도로 시뮬레이션하는 능력을 갖는다는 것을 보여주었다(예를 들어, Li 등(2020)). 비슷한 목적을 위해 생성 모델은 몇 가지 이점을 제공하기 때문에 관심이 증가하고 있다. 첫째, 초기 조건의 앙상블을 수치적으로 전개하는 것과는 반대로 물리 시스템의 미래 상태에 대한 한계 분포를 직접 얻을 수 있는 방법을 제공한다. 특히 기상 또는 기후 모델링, 유체 역학 분석 및 확률 미분 방정식 연구에서 사용 사례가 있다. 둘째, 평균 제곱 오차 목표를 기반으로 한 접근법에 비해 높은 데이터 빈도를 모델링하는 것을 개선할 수 있다.\n' +
      '\n' +
      '시뮬레이션은 다음과 같은 편미분방정식 \\(\\frac{\\partial\\mathbff{u}}{\\partial\\tau}+\\nabla\\cdot(\\mathbff{u}))=\\nu\\nabla^{2}\\mathbff{u}-\\frac{1}{\\rho}\\nabla p+\\mathbf{f}\\, \\(\\rho\\) 유체밀도, \\(\\rho\\) 압력장, \\(\\rho\\) 유체밀도, \\(\\rho\\) 유체밀도, \\(\\rho\\) 유체밀도, \\(\\rho\\) 유체밀도, \\(\\rho\\) 유체밀도, \\(\\rho\\) 유체밀도, \\(\\rho\\\\mathcal{T}\\times\\mathbbb{R}^{2}\\rightarrow\\mathbbb{R}^{2}\\\\(\\rho\\) 유체밀도, \\(\\rho\\) 유체밀도, \\(\\r 5\\(5\\cdot 10^{-4}\\)와 \\(5\\cdot 10^{-3}\\) 사이의 점도와 \\(0.5\\)과 \\(2\\) 사이의 밀도를 무작위로 샘플링하여 동역학을 예측하는 작업을 비결정적으로 만든다. 지상진실자료는 유한체적 기반 직접수치모사(Finite Volume-based Direct Numerical Simulation, DNS)를 이용하여 생성되며, 최대 시간 스텝은 \\(0.05\\)이고, 시뮬레이션 시간은 1.5초마다 300초에 해당하는 6,000시간이다. 이는 수십 초 정도의 시뮬레이션 시간만을 고려하는 일반적인 시뮬레이션보다 훨씬 길다는 점에 유의한다. 우리는 2 테라바이트의 데이터에 해당하는 \\(64\\times 64\\)에서 200,000개의 샘플을 시뮬레이션한다. 혼돈적인 특성과 긴 시뮬레이션 시간으로 인해 모델은 정확한 미래 상태를 예측할 수 없으므로 긴 시간 롤아웃을 테스트하는 데 이상적인 데이터 세트이다. 시뮬레이션 설정에 대한 자세한 내용은 부록 B에서 확인할 수 있으며, 모델은 수평 및 수직 속도를 포함하는 2개의 입력 프레임을 제공한다. 큰 1.5s의 보폭과 함께 긴 롤아웃 길이(최대 \\(\\pm\\) 60초)는 이것을 일반적인 "신경 에뮬레이터" 작업보다 더 도전적인 작업으로 만든다. 예를 들어, Lippe et al.(2023); Sun et al.(2023)은 훨씬 더 작은 보폭으로 \\(\\pm 15\\)s까지만 올라간다.\n' +
      '\n' +
      '평가 생성된 데이터가 지면 진리 분포와 얼마나 잘 일치하는지 측정하기 위해, 우리는 _Frechet Inception Distance_(FID) 또는 FVD와 유사한 방법을 제안한다. 우리는 공간 주파수 강도가 프로라는 사실을 이용한다.\n' +
      '\n' +
      '그림 3: 콜모고로프 흐름 롤아웃 실험의 FSD 결과. 낮을수록 좋다.\n' +
      '\n' +
      'vide a good summary statistic (Sun et al., 2023; Dresdner et al., 2022; Kochkov et al., 2021). [\\(\\mathbf{f_{\\mathbf{x}}}\\in\\mathbb{R}^{F}\\)는 변수 \\(\\mathbf{x}\\)으로부터 (2차원) _Discrete Fourier Transform_ (DFT)를 사용하여 계산된 공간-스펙트럴 크기의 벡터를 나타낸다. 그리고 나서, \\(\\mathbf{F}_{\\mathcal{D}\\in N\\times F\\)는 (test-time) dataset \\(\\mathcal{D}:=\\{\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{N}\\}\\})의 모든 빈도를 나타낸다. 생성 모델의 푸리에 크기를 \\(\\mathbf{F}_{\\theta}\\라 하자. 이제 생성된 샘플과 실제 데이터 간의 프리셰트 거리를 설정하여 계산합니다.\n' +
      '\n' +
      '\\mathrm{FSD}(\\mathcal{D},\\theta}=\\|\\mathbff{f_{\\mathcal{D}}-\\mathbf{bar{f}_{\\theta}}\\|^{2}+\\mathrm{tr}\\Big{(}\\mathbf{\\Sigma}_{\\mathcal{D}+\\mathbf{\\Sigma}_{\\theta}-2(\\mathbf{\\Sigma}_{\\theta}}\\mathbf{\\Sigma}_{\\theta}}}\\mathbf{\\Sigma}_{\\theta}}^{1/2}\\Big{}}\\tag{27}}\\mathbf{\\Sigma}_{\\theta}}\n' +
      '\n' +
      '여기서 \\(\\mathbf{\\bar{f}}\\)와 \\(\\mathbf{\\Sigma}\\)는 각각 주파수의 평균과 공분산을 나타낸다. 우리는 이 메트릭을 _Frechet Spectral Distance_(FSD)라고 부른다.\n' +
      '\n' +
      '그림 3에서 우리는 유체의 수평 속도 유체로부터 계산된 FSD를 제시한다. 압연확산에 대해서는 \\(t_{w}^{\\text{init}}(n_{\\text{cln}})\\)의 재매개변수를 \\(n_{\\text{cln}}=2\\)으로 사용하고, 긴 롤아웃에 \\(t_{w}^{\\text{fin}}(n_{\\text{cln}})\\을 사용한다. 문헌에서 일반적으로 사용되는 자기회귀 MSE 기반 모델은 이 작업에 적합하지 않다는 것이 분명하다. 표준 확산을 위해 반복해서 \\(W-n_{\\text{cln}}\\) 프레임을 생성한 후 이를 컨디셔닝에 연결하고 롤아웃을 계속한다. 롤링 확산은 항상 앞서 설명한 프로세스를 사용하여 샘플링하면서 윈도우를 하나씩 이동시킨다. 다양한 조건 설정과 윈도우 크기에 관계없이 롤링 확산이 표준 확산 방법을 일관되게 능가할 수 있음을 알 수 있으며, \'\\((n_{\\text{cln}},W-n_{\\text{cln}})\\)\'으로 표시된다. 우리는 속도장의 _vorticity_를 나타내는 그림 2의 예시적인 롤아웃을 제공한다. 우리는 부록 C에 우리의 결과를 수치적으로 제시한다.\n' +
      '\n' +
      'BAIR 로봇 푸시 데이터세트\n' +
      '\n' +
      '버클리 AI 연구(BAIR) 로봇 푸싱 데이터세트(Ebert et al., 2017)는 비디오 예측을 위한 표준 벤치마크이다. 이 비디오는 44,000개의 비디오를 포함하고 있다. 기존의 방법들에 따라 1 프레임 단위로 조건을 설정하고 다음 15 프레임을 예측하며, _Frechet Video Distance_ (FVD) (Unterthiner et al., 2019)를 이용하여 기존 연구들과 일관성 있게 평가한다. FVD의 경우, I3D 네트워크(Carreira and Zisserman, 2017)를 사용하여 평가 세트의 256개의 예제와 \\(100\\times 256\\) 모델 샘플을 비교한다.\n' +
      '\n' +
      '롤링 확산과 관련하여, 우리는 \\(t_{w}^{\\text{init}}(n_{\\text{cln}})\\) 재매개변수화를 사용하여 \\(W=16\\)(\\(n_{\\text{cln}}=1\\)) 프레임을 부분적으로 잡음 제거된 상태로 샘플링하고, 롤아웃 및 샘플링을 완료한다. 표준 확산은 한 번에 15프레임 모두를 샘플링하며 자기회귀 확장을 고려하지 않기 때문에 이점이 있을 수 있다.\n' +
      '\n' +
      '그 결과를 표 1에 나타내었다. 우리는 동일한 (Simple Diffusion) 아키텍처를 사용하여 표준 확산과 롤링 확산 모두 이전 방법을 능가하는 것을 관찰한다. 또한 이 설정에서 표준 프레임워크와 롤링 프레임워크 간에 큰 차이가 없음을 알 수 있다. 샘플링된 시퀀스는 두 경우 모두 실제 데이터와 구별할 수 없기 때문에 그림 4. 모델이 이 작업을 완전히 해결할 수 있어 성능에 큰 차이가 없다고 말할 수 있다. 그러나 두 모델 모두 기존 방법을 능가할 수 있다는 점은 여전히 흥미롭다.\n' +
      '\n' +
      '### Kinetics-600\n' +
      '\n' +
      '마지막으로, Kinetics-600 벤치마크에 대한 비디오 예측을 평가한다(Kay et al., 2017; Carreira et al., 2018). 600개를 묘사한 약 400,000개의 훈련 비디오를 포함하고 있습니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline \\hline Method & FVD (\\(\\downarrow\\)) \\\\ \\hline DVD-GAN (Clark et al., 2019) & 109.8 \\\\ VideoGPT (Yan et al., 2021) & 103.3 \\\\ TriVD-GAN-FP & 103.3 \\\\ Transframer (Nash et al., 2022) & 100 \\\\ CCVS (Le Moing et al., 2021) & 99 \\\\ VideoTransformer (Weissenborn et al., 2019) & 94 \\\\ FitVid (Babeizadeh et al., 2021) & 93.6 \\\\ NUWA (Wu et al., 2022) & 86.9 \\\\ Video Diffusion (Ho et al., 2022b) & 66.9 \\\\ \\hline Standard Diffusion (Ours) & **59.7** \\\\ Rolling Diffusion (Ours) & **59.6** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: BAIR 로봇 밀기 기준선 실험의 결과.\n' +
      '\n' +
      '그림 4: 상단: BAIR 로봇 푸시 데이터 세트 상의 롤링 확산 롤아웃. 결론은 진실이다\n' +
      '\n' +
      '다른 활동들은 \\(64\\times 64\\)으로 축소되었다. 이 데이터 세트를 사용하여 두 가지 실험을 실행합니다. 첫 번째는 이전에 발표된 작업과 동일한 설정에서 기본 실험이다. 다음 순서는 긴 시퀀스에 대해 자동으로 롤아웃하는 롤링 디퓨전의 능력을 구체적으로 테스트한다.\n' +
      '\n' +
      '기준은 5개의 입력 프레임과 11개의 출력 프레임을 사용하여 이전 방법과 비교하고 섹션 5.2에서 결과를 보여준다. 평가 메트릭은 다시 FVD이다. 우리는 현재 SOTA 방법 중 많은 방법이 _2-stage_라는 점에 주목하며, 이는 그들이 오토인코더를 사용하고 잠재 공간에서 확산을 실행한다는 것을 의미한다. 경험적으로 설득력이 있지만(Rombach et al., 2022), 이것은 확산 모델 자체의 효과를 분리하기 어렵게 만든다. 오토인코더 파라미터가 2단계 메서드에 대한 파라미터 카운트에 포함되는지, 또는 어떤 데이터가 미리 트레이닝되는지에 대해서는 항상 명확하지 않다는 점에 유의한다. 표준 확산 U-ViT 아키텍처를 사용한 실행 확산은 3.9의 FVD를 달성하며, 이는 최상의 2단계 방법에 필적한다. 롤링 확산은 이 경우에 강한 단점을 갖는다: (1) 베이스라인은 모든 프레임들을 한번에 생성하고; (2) 스트라이드가 1인 경우, 16 프레임들에서 동역학이 거의 없으며, 대부분 표준 확산 모델에 적합하다. 여전히, 롤링 확산은 5.2의 FVD를 달성한다.\n' +
      '\n' +
      '다음으로, 모델의 성능을 자동으로 롤아웃하는 것과 비교하여 표 3의 결과를 보여준다. 모든 설정은 윈도우 크기\\(W=16\\) 프레임을 사용하며, 여기서 설정\\(n_{\\text{ch}\\)은 \'cond-gen\'으로 표시된다. 또한, 기본 스케줄과 _rescaled_ 스케줄의 혼합으로 롤링 확산을 훈련한다. 기본 롤링 확산 스케줄 \\(t_{w}^{\\text{lin}}\\)은 \\(\\beta\\)의 속도로 오버샘플링된다.\n' +
      '\n' +
      '우리는 보폭(프레임 생략 또는 프레임 단계라고도 함)이 1인 두 가지 설정, 64단계에 대해 롤링 아웃(롤아웃) 및 24단계에 대해 보폭 8에 대해 롤링 아웃(롤아웃)을 분석하여 \\(192\\)번째 프레임까지 전방을 효과적으로 예측한다. 첫 번째 설정에서 표준 확산은 데이터의 불변성으로 인해 더 잘 수행된다. 높은 테스트 시간 단계를 고려하기 위해 \\(\\beta=0.9\\)의 비율로 선형 롤링 스케줄을 오버샘플링한다. 롤링 확산은 두 번째 설정에서 일관되게 승리하며, 이는 훨씬 더 역동적이다. 또한 단일 프레임 확산은 여기에서 상당히 저성과 더 큰 블록 자동 회귀가 유리하다는 점에 유의해야 한다. 그림 5의 예시적인 롤아웃을 참조. 부록 F에서 우리는 롤링 확산의 성능에 대한 오버샘플링 속도의 영향을 나타낸다. 이 경우, t_{w}^{\\text{lin}}\\(t_{w}^{\\text{lin}}\\)의 오버샘플링이 가장 좋은 결과를 얻을 수 있다.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '우리는 시간이 지남에 따라 점진적으로 잡음(및 잡음) 데이터를 제거하는 새로운 DDPM 프레임워크인 _Rolling Diffusion Models_를 제시하였다. 비디오 및 유체 역학 데이터에 대한 방법을 검증한 결과, 롤링 확산의 자연 유도 편향은 데이터가 매우 동적일 때 가장 효과적으로 이용된다는 것을 관찰했다. 이는 예를 들어 비디오, 오디오 및 날씨 또는 기후 모델링에서 흥미로운 미래 방향을 허용한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c} \\hline \\hline Method & FVD (\\(\\downarrow\\)) \\\\ \\hline Phenaki (Villegas et al., 2022) & 36.4 \\\\ TrIVD-GAN-FP (Luc et al., 2020) & 25.7\\({}^{\\dagger}\\) \\\\ Video Diffusion (Ho et al., 2022) & 16.2 \\\\ RIN (Jabri et al., 2022) & 10.8 \\\\ MAGVIT (Yu et al., 2023) & 9.9\\({}^{\\dagger}\\) \\\\ MAGVITv2 (Yu et al., 2023) & 4.3\\({}^{\\dagger}\\) \\\\ W.A.L.T.-L (Gupta et al., 2023) & **3.3\\({}^{\\dagger}\\)** \\\\ \\hline Rolling Diffusion (Ours) & \\(5.2\\) \\\\ Standard Diffusion (Ours) & **3.9** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: Kinetics-600 베이스라인 태스크(stride 1)의 FVD 결과. 2단계 방법은 \\({}^{\\dagger}\\)로 표시된다.\n' +
      '\n' +
      '그림 5: 상단: Kinetics-600 데이터 세트 상의 롤링 확산 롤아웃. 결론은 진실이다\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l} \\hline \\hline  & Method & cond-gen & FVD \\\\ \\hline _stride=8_ & Standard Diffusion & (5-11) & 58.1 \\\\ _steps=24_ & Rolling \\(\\beta=0.1\\) & (5-11) & **39.8** \\\\ \\hline _stride=1_ & Standard Diffusion & (15-1) & 1369 \\\\ _steps=64_ & Standard Diffusion & (8-8) & 157.1 \\\\  & Standard Diffusion & (5-11) & **123.7** \\\\  & Rolling \\(\\beta=0.9\\) & (5-11) & 211.2 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: Kinetics-600(Rollout) with 8192 FVD samples, 100 sampling steps per 11 frame. 300k 반복 훈련을 받았습니다.\n' +
      '\n' +
      '## Broader Impact\n' +
      '\n' +
      '확산 모델을 포함한 순차적 생성 모델은 빠르고 상세한 샘플링을 가능하게 함으로써 비디오 생성 및 과학 연구의 응용 프로그램과 함께 사회적으로 상당한 영향을 미친다. 기후 모델링에서 의료 이미징에 이르기까지 다양한 분야에서 보다 정확하고 강력한 합성을 창출하는 장점을 제공하지만 디지털 미디어의 콘텐츠 진정성과 독창성에 대해서는 눈에 띄는 단점이 있다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Babaeizadeh et al.(2021) Babaeizadeh, M., Saffar, M. T., Nair, S., Levine, S., Finn, C., and Erhan, D. Fitvid: Overfitting in pixel-level video prediction. _ arXiv preprint arXiv:2106.13195_, 2021.\n' +
      '* Blattmann et al. (2023) Blattmann, A., Rombach, R., Ling, H., Dockhorn, T., Kim, S. W., Fidler, S., and Kreis, K. 잠복기 정렬: 잠재 확산 모델과 고해상도 비디오 합성 In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 22563-22575, 2023.\n' +
      '* Carreira & Zisserman (2017) Carreira, J. and Zisserman, A. Quo vadis, action recognition? 새로운 모델과 동역학 데이터 세트입니다. In _proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pp. 6299-6308, 2017.\n' +
      '* Carreira et al. (2018) Carreira, J., Noland, E., Banki-Horvath, A., Hillier, C., and Zisserman, A. A short note about kinetics-600. _arXiv preprint arXiv:1808.01340_, 2018.\n' +
      '* Clark et al. (2019) Clark, A., Donahue, J., and Simonyan, K. 복잡한 데이터셋에 대한 적대적 비디오 생성 ArXiv preprint arXiv:1907.06571_, 2019.\n' +
      '* Dresdner et al. (2022) Dresdner, G., Kochkov, D., Norgaard, P., Zepeda-Nunez, L., Smith, J. A., Brenner, M. P., and Hoyer, S. 난류 흐름을 시뮬레이션하기 위한 스펙트럼 방법을 수정하는 학습. 2022. doi: 10.48550/ARXIV.2207.00556. URL[https://arxiv.org/abs/2207.00556](https://arxiv.org/abs/2207.00556).\n' +
      '*Ebert et al. (2017) Ebert, F., Finn, C., Lee, A. X., and Levine, S. 시간 스킵 연결이 있는 자체 감독 시각 계획. _ CoRL_, 12:16, 2017.\n' +
      '*Gao et al. (2023) Gao, Y., Morioka, N., Zhang, Y., and Chen, N. E3 tts: 쉬운 종단간 확산 기반 텍스트 대 스피치. _2023 IEEE Automatic Speech Recognition and Understanding Workshop(ASRU)_, pp. 1-8. IEEE, 2023.\n' +
      '* Ge et al. (2023) Ge, S., Nah, S., Liu, G., Poon, T., Tao, A., Catanzaro, B., Jacobs, D., Huang, J.-B., Liu, M. - Y, Balaji, Y. 자신의 상관 관계를 보존합니다: 비디오 확산 모델에 대한 노이즈 이전입니다. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 22930-22941, 2023.\n' +
      '* Gupta et al. (2023) Gupta, A., Yu, L., Sohn, K., Gu, X., Hahn, M., Fei-Fei, L., Essa, I., Jiang, L., and Lezama, J. Photorealistic video generation with diffusion models. _ arXiv preprint arXiv:2312.06602_, 2023.\n' +
      '* Harvey et al. (2022) Harvey, W., Naderiparizi, S., Masrani, V., Weilbach, C., and Wood, F. Flexible diffusion modeling of long video. _ 신경 정보 처리 시스템_, 35:27953-27965, 2022에서의 발전.\n' +
      '* He et al. (2022) He, Y., Yang, T., Zhang, Y., Shan, Y., and Chen, Q. 임의 길이를 갖는 고충실도 비디오 생성을 위한 잠재 비디오 확산 모델. _ ArXiv:2211.13221_, 2022.\n' +
      '* Ho et al. (2020) Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS_, 2020.\n' +
      '* Ho et al. (2022a) Ho, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko, A., Kingma, D. P., Poole, B., Norouzi, M., Fleet, D. J., et al. Imagen video: High definition video generation with diffusion models. _ arXiv preprint arXiv:2210.02303_, 2022a.\n' +
      '* Ho et al. (2022b) Ho, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi, M., and Fleet, D. J. Video diffusion models. _ arXiv:2204.03458_, 2022b.\n' +
      '* Hoogeboom et al. (2023) Hoogeboom, E., Heek, J., and Salimans, T. simple diffusion: End-to-end diffusion for high resolution image. _ arXiv preprint arXiv:2301.11093_, 2023.\n' +
      '* Jabri et al. (2022) Jabri, A., Fleet, D. J., and Chen, T. 반복 생성을 위한 확장 가능한 적응 계산 _ CoRR_, abs/2212.11972, 2022.\n' +
      '* Kawar et al. (2017) Kawar, B., Zada, S., Lang, O., Tov, O., Chang, H., Dekel, T., Mosseri, I., and Irani, M. 이미직: 확산 모델을 사용한 텍스트 기반 실제 이미지 편집. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 6007-6017, 2023.\n' +
      '* Kay et al. (2017) Kay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan, S., Viola, F., Green, T., Back, T., Natsev, P., et al. The kinetics human action video dataset. _ ArXiv:1705.06950_, 2017.\n' +
      '* Kingma et al. (2021) Kingma, D. P., Salimans, T., Poole, B., and Ho, J. Variational diffusion models. _ CoRR_, abs/2107.00630, 2021.\n' +
      '* Kochkov et al. (2021) Kochkov, D., Smith, J. A., Alieva, A., Wang, Q., Brenner, M. P., and Hoyer, S. 기계 학습이 가속화된 전산 유체 역학. _ National Academy of Sciences_, 118(21), 2021. ISSN 0027-8424. doi: 10.1073/pnas.2101784118. URL[https://www.pnas.org/content/118/21/e2101784118](https://www.pnas.org/content/118/21/e2101784118)\n' +
      '* Kohl et al. (2023) Kohl, G., Chen, L. - W., and Thuerey, N. 자기회귀 조건부 확산 모델을 이용한 난류유동 시뮬레이션 arXiv preprint arXiv:2309.01745_, 2023.\n' +
      '* Kong et al. (2021) Kong, Z., Ping, W., Huang, J., Zhao, K., and Catanzaro, B. DiffWave: A versatile diffusion model for audio synthesis. _9th International Conference on Learning Representations, ICLR_, 2021.\n' +
      '* Le Moing et al. (2021) Le Moing, G., Ponce, J., and Schmid, C. Ccvs: context-aware controllable video synthesis. _ 신경 정보 처리 시스템_, 34:14042-14055, 2021에서의 발전.\n' +
      '* Le et al. (2021)* Li et al. (2022) Li, X., Thickstun, J., Gulrajani, I., Liang, P. S., and Hashimoto, T. B. Diffusion-lm improves controllable text generation. _ 신경 정보 처리 시스템_, 35:4328-4343, 2022에서의 발전.\n' +
      '* Li et al. (2020) Li, Z., Kovachki, N., Azizzadenesheli, K., Liu, B., Bhattacharya, K., Stuart, A., and Anandkumar, A. Fourier neural operator for parametric partial differential equations. _ arXiv preprint arXiv:2010.08895_, 2020.\n' +
      '* Lippe et al. (2023) Lippe, P., Veeling, B. S., Perdikaris, P., Turner, R. E., and Brandstetter, J. Pde-refiner: The accurate long rollout with neural pde solvers. _ arXiv preprint arXiv:2308.05732_, 2023.\n' +
      '* Luc et al. (2020) Luc, P., Clark, A., Dieleman, S., Casas, D. d. L., Doron, Y., Cassirer, A., and Simonyan, K. 대용량 데이터에 대한 변환 기반 적대적 비디오 예측 _ arXiv preprint arXiv:2003.04035_, 2020.\n' +
      '* Meng et al. (2022) Meng, C., Gao, R., Kingma, D. P., Ermon, S., Ho, J., and Salimans, T. 유도 확산 모델의 증류에서 _ CoRR_, abs/2210.03142, 2022.\n' +
      '* Nash et al. (2022) Nash, C., Carreira, J., Walker, J., Barr, I., Jaegle, A., Malinowski, M., and Battaglia, P. Transframer: ARbitrary frame prediction with generatingative models. _ ArXiv:2203.09494_, 2022.\n' +
      '* Price et al. (2023) Price, I., Sanchez-Gonzalez, A., Alet, F., Ewalds, T., El-Kadi, A., Stott, J., Mohamed, S., Battaglia, P., Lam, R., and Willson, M. Gencast: 중거리 날씨에 대한 확산 기반 앙상블 예측. _ arXiv preprint arXiv:2312.15796_, 2023.\n' +
      '* Ramesh et al. (2022) Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M. CLIP Latents를 이용한 계층적 텍스트 조건 이미지 생성 CoRR_, abs/2204.06125, 2022년\n' +
      '* Rombach et al. (2022) Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022_, pp. 10674-10685. IEEE, 2022.\n' +
      '* Saharia et al. (2022) Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., Ghasemipour, S. K. S., Ayan, B. K., Mahdavi, S. S., Lopes, R. G., Salimans, T., Ho, J., Fleet, D. J., and Norouzi, M. 깊은 언어 이해를 가진 사실적 텍스트-이미지 확산 모델. _ CoRR_, abs/2205.11487, 2022년\n' +
      '* Singer et al. (2022) Singer, U., Polyak, A., Hayes, T., Yin, X., An, J., Zhang, S., Hu, Q., Yang, H., Ashual, O., Gafni, O., Parikh, D., Gupta, S., and Taigman, Y. Make-a-video: text-video 데이터가 없는 text-to-video 생성. _ CoRR_, abs/2209.14792, 2022년\n' +
      '* Sohl-Dickstein et al. (2015) Sohl-Dickstein, J., Weiss, E. A., Maheswaranathan, N., and Ganguli, S. 평형 열역학을 이용한 심층 비지도 학습 Bach, F. R. and Blei, D. M. (eds.), _Proceedings of the 32nd International Conference on Machine Learning, ICML_, 2015.\n' +
      '*Song & Ermon(2019) Song, Y. 및 Ermon, S. 데이터 분포의 기울기를 추정하여 생성 모델링 In _Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS_, 2019.\n' +
      '* StabilityAI(2023) StabilityAI. 안정적인 비디오 확산을 소개합니다. Nov 2023. URL[https://stability.ai/news/stable-video-diffusion-open-ai-video-model](https://stability.ai/news/stable-video-diffusion-open-ai-video-model) 접속: 2024-01-25\n' +
      '* Sun et al. (2023) Sun, Z., Yang, Y., and Yoo, S. 시간 스텐실 모델링을 이용한 신경 pde solver. _ arXiv preprint arXiv:2302.08105_, 2023.\n' +
      '* Tashiro et al. (2021) Tashiro, Y., Song, J., Song, Y., and Ermon, S. Csdi: 확률적 시계열 치환을 위한 조건부 점수 기반 확산 모델. _ 신경 정보 처리 시스템_, 34:24804-24816, 2021에서의 발전.\n' +
      '* Unterthiner et al. (2019) Unterthiner, T., van Steenkiste, S., Kurach, K., Marinier, R., Michalski, M., and Gelly, S. Fvd: 비디오 생성을 위한 새로운 메트릭. 2019년\n' +
      '* Villegas et al. (2022) Villegas, R., Babaeizadeh, M., Kindermans, P.-J., Moraldo, H., Zhang, H., Saffar, M. T., Castro, S., Kunze, J., and Erhan, D. Phenaki: Variable length video generation from open domain textual description. _ arXiv preprint arXiv:2210.02399_, 2022.\n' +
      '* Weissenborn et al. (2019) Weissenborn, D., Tackstrom, O., and Uszkoreit, J. Scaling autoregressive video models. _ ArXiv preprint arXiv:1906.02634_, 2019.\n' +
      '* Wu et al. (2022) Wu, C., Liang, J., Ji, L., Yang, F., Fang, Y., Jiang, D., and Duan, N. Niwa: 신경 시각 세계 생성을 위한 시각 합성 사전 훈련. _European conference on computer vision_, pp. 720-736. Springer, 2022.\n' +
      '* Wu et al. (2023) Wu, T., Fan, Z., Liu, X., Gong, Y., Shen, Y., Jiao, J., Zheng, H.-T., Li, J., Wei, Z., Guo, J., et al. Ar-diffusion: Autoregressive diffusion model for text generation. _ arXiv preprint arXiv:2305.09515_, 2023.\n' +
      '* Yan et al. (2021) Yan, W., Zhang, Y., Abbeel, P., and Srinivas, A. Videogpt: video generation using vq-vae and transformer. _ arXiv preprint arXiv:2104.10157_, 2021.\n' +
      '* Yang et al. (2023) Yang, R., Srivastava, P., and Mandt, S. 비디오 생성을 위한 확산 확률 모델링. _ Entropy_, 25(10):1469, 2023.\n' +
      '* Zhang et al. (2021)Yu, J., Xu, Y., Koh, J. Y., Luong, T., Baid, G., Wang, Z., Vasudevan, V., Ku, A., Yang, Y., Ayan, B. K., Hutchinson, B., Han, W., Parekh, Z., Li, X., Zhang, H., Baldridge, J., and Wu, Y. 콘텐츠가 풍부한 텍스트-이미지 생성을 위한 자기회귀 모델의 스케일링 _ CoRR_, abs/2206.10789, 2022.\n' +
      '* Yu et al. (2023a) Yu, L., Cheng, Y., Sohn, K., Lezama, J., Zhang, H., Chang, H., Hauptmann, A. G., Yang, M. - H., Hao, Y., Essa, I., et al. Magvit: Masked Generative Video Transformer. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 10459-10469, 2023a.\n' +
      '* Yu et al. (2023b) Yu, L., Lezama, J., Gundavarapu, N. B., Versari, L., Sohn, K., Minnen, D., Cheng, Y., Gupta, A., Gu, X., Hauptmann, A. G., et al. Language model beats diffusion-tokenizer is key to visual generation. _ arXiv preprint arXiv:2310.05737_, 2023b.\n' +
      '* Yu et al. (2023c) Yu, S., Sohn, K., Kim, S., and Shin, J. Video probabilistic diffusion models in projected latent space. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 18456-18466, 2023c.\n' +
      '\n' +
      'A Hyperparameters\n' +
      '\n' +
      '이 섹션에서는 다양한 실험에 대한 하이퍼파라미터를 나타낸다. 실험 전반에 걸쳐 우리는 블록에서 자기 주의를 사용할 때 컨볼루션 레이어 대신 MLP 블록이 있는 본질적으로 U-Nets인 U-ViT를 사용한다. PDE 실험에서는 비교적 작은 구조가 사용된다. BAIR의 경우 더 큰 아키텍처를 사용하여 채널 수와 블록 수를 모두 증가시켰다. K600의 경우 이 데이터 세트가 가장 적합하기 어려운 것으로 밝혀졌기 때문에 더 큰 아키텍처를 사용했다. 모든 사양은 표 4, 5 및 6을 참조하십시오.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline Parameter & Value \\\\ \\hline Blocks & [4 + 4, 4 + 4, 5 + 5, 8] \\\\ Channels & [256, 512, 2048, 4096] \\\\ Block Type & [Conv2D, Conv2D, Transformer (axial), Transformer] \\\\ Block Type & [Conv2D, Conv2D, Transformer (axial), Transformer] \\\\ Head Dim & 128 \\\\ Dropout & [0, 0, 0.1, 0.1] \\\\ Downsample & (1, 2, 2) \\\\ Model parametrization & \\(v\\) \\\\ Loss & \\(\\epsilon\\)-loss (\\(x\\)-loss with SNR + 1 weighting) \\\\ Model parametrization & \\(v\\) \\\\ Loss & \\(\\epsilon\\)-loss (\\(x\\)-loss with SNR + 1 weighting) \\\\ Number of Steps & 300 000 (rollout experiments) / 570 000 (standard experiments) \\\\ EMA decay & 0.9999 \\\\ EMA decay & 0.9999 \\\\ learning rate & 1e-4 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: PDE 실험\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline Parameter & Value \\\\ \\hline Blocks & [3 + 3, 3 + 3, 3 + 3, 8] \\\\ Channels & [128, 256, 512, 1024] \\\\ Block Type & [Conv2D, Conv2D, Transformer (axial), Transformer] \\\\ Head Dim & 128 \\\\ Dropout & [0, 0.1, 0.1, 0.1] \\\\ Downsample & (1, 2, 2) \\\\ Loss & \\(\\epsilon\\)-loss (\\(x\\)-loss with SNR + 1 weighting) \\\\ Number of Steps & 100 000 (rollout experiments) / 570 000 (standard experiments) \\\\ EMA decay & 0.9999 \\\\ learning rate & 1e-4 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: BAIR 실험\n' +
      '\n' +
      '## 부록 B 시뮬레이션 상세\n' +
      '\n' +
      '이 섹션에서는 콜모고로프 흐름 시뮬레이션 데이터를 생성하기 위한 하이퍼파라미터를 제시하며 표 7을 참조한다. 그렇지 않은 결정론적 시스템에 불확실성을 도입하려면 점도 및 밀도 파라미터를 변경해야 하며, 이를 데이터에서 추론해야 한다. 이것은 또한 표준 해결사를 그러한 환경에서 사용하기 매우 어렵게 만들 것이다. 또한 시스템의 혼란스러운 특성으로 인해 임의 정밀도까지 결정적으로 예측할 수 없다.\n' +
      '\n' +
      '우리는 2차원으로 난류를 시뮬레이션할 수 있도록 구동력과 감쇠항을 결합하는 "단순 난류 강제" 옵션을 사용한다.\n' +
      '\n' +
      '## 부록 C 추가 결과\n' +
      '\n' +
      '다양한 시간 단계에서 MSE 및 FSE 오류를 표 8에 보여준다. MSE 모델은 MSE 손실 측면에서 항상 최적이며, 이는 예상대로이다. 그러나, 주파수 분포의 매칭의 관점에서, FSD에 의해 측정된 바와 같이, 표준 확산, 특히 압연 확산이 최적이다.\n' +
      '\n' +
      '## 부록 D 롤링 확산 목표\n' +
      '\n' +
      '완전성을 위한 표준 확산, 우리는 확산 목표에 대해 간략하게 검토한다. [\\(T\\in\\mathbb{N}\\)을 유한한 확산 단계로 하고, \\(i\\in\\{0,\\ldots,T\\}\\)을 확산 시간 단계로 한다. Kingma et al. (2021)은 \\(q(\\mathbf{x},\\mathbf{z}_{0},\\ldots,\\mathbf{z}_{T})\\)와 \\(p(\\mathbf{x},\\mathbf{z}_{0},\\ldots,\\mathbf{z}_{T})\\) 사이의 이산시간 \\(D_{\\mathrm{KL}}\\)을 다음과 같이 분해할 수 있음을 보여준다.\n' +
      '\n' +
      '\\mathbf{z}_{0},\\mathbf{z}_{T})||p(\\mathbf{x},\\mathbf{z}_{T})\\right) =\\mathbb{E}_{q(\\mathbf{x},\\mathbf{z}_{0},\\mathbf{z}_{T})}\\left[-\\logp(\\mathbf{x},\\mathbf{z}_{0},\\ldots,\\mathbf{z}_{T})}\\left[-\\mathbf{z}_{0},\\mathbf{z}_{t}}}\\text{reconstruction Loss}+\\mathbbrace{E}_{q(\\mathbf{z}_{z}_{0},\\tag{29}\\mathbf{z}_{T}}}\\text{preconstruction Loss}+\\mathbbrace{E}_{z}_{x},\\mathbf{z}\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline Parameter & Value \\\\ \\hline Size & 256 \\\\ Viscosity & Uniform random in \\([5.0\\times 10^{-4},5.0\\times 10^{-3}]\\) \\\\ Density & Uniform random in \\([2^{-1},2^{1}]\\) \\\\ Maximum Velocity & 7.0 \\\\ CFL Safety Factor & 0.5 \\\\ Max \\(\\Delta t\\) & 0.05 \\\\ Outer Steps & 6000 \\\\ Grid & 256 \\(\\times\\) 256, domain \\([0,2\\pi]\\times[0,2\\pi]\\) \\\\ Initial Velocity & Filtered velocity field, 3 iterations \\\\ Forcing & Simple turbulence, magnitude=2.5, linear coefficient=-0.1, wavenumber=4 \\\\ Total Simulations & 200,000 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: JaxCFD를 이용한 Kolmogorov 유동 시뮬레이션의 파라미터\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l l l l} \\hline \\hline  & & \\multicolumn{6}{c}{FSD/MSE \\(\\oplus\\)} \\\\ Method & 1 & 2 & 4 & 8 & 12 & 24 & 48 \\\\ \\hline MSE (2-1) & 304.7 / **14.64** & 687.1 / **137.15** & 1007 / **407.0** & 1649 / **407.0** & 2230 / **407.0** & 5453 / **407.0** & 7504 / **407.0** \\\\ MSE (2-2) & 531.3 / 20.1 & 7205 / 148.3 & 5996 / 407.0 & 7277 / 407.0 & 8996 / 406.1 & 1 \\(\\cdot 10^{4}\\) / 407.0 & 1 \\(\\cdot 10^{4}\\) / 407.0 \\\\ MSE (2-4) & 304.7 / 21.7 & 6684 / 148.9 & 6- 104 / 378.9 & 3 - 10\\(\\cdot 10^{4}\\) / 407.0 & 2 \\(\\cdot 10^{4}\\) / 407.0 & 2 \\(\\cdot 10^{4}\\) / 407.0 \\\\ \\hline Standard Diffusion (2-1) & 39.9 / 267.6 & 573.7 / 192.6 & 142.0 / 710.0 & 297.7 / 794.6 & 399.4 / 773.1 & 442.5 / 758.3 & 763.6 / 732.5 \\\\ Standard Diffusion (2-2) & 59.1 / 47.88 & 86.1 / 334.9 & 112.6 / 167.6 & 32.4 / 81.8 / 15.3 & 3147 / 755.5 & 403.3 / 725.0 & 726.3 / 695.4 \\\\ Standard Diffusion (2-4) & 86.19 / 49.93 & 141.6 / 355.6 & 246.6 / 753.2 & 397.8 / 758.0 & 555.6 / 726.9 & 1094.0 / 701.1 & 2401.0 / 666.3 \\\\ Standard Diffusion (2-8) & 87.0 / 54.0 & 137.7 / 399.2 & 288.3 / 770.1 & 338.7 / 725.3 & 355.5 / 713.6 & 530.6 / 705.5 & 1159 / 748.3 \\\\ \\hline Rolling Diffusion (init noise) (2-4) & 29.59 / 39.72 & **47.44** / 287.8 & **43.39** / 738.4 & **61.93** / 769.0 & 214.32 / 735.7 & 648.53 / 699.1 & 1238.44 / 670.0 \\\\ Rolling Diffusion (init noise) (2-8) & **27.68** / 41.22 & 52.41 / 316.9 & 53.47 / 768.0 & 98.29 / 777.2 & **187.03** / 74.8 & **34.89** / 737.6 & **417.99** / 719.4 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: Kolmogorov Flow Resultswhere \\(c\\)는 데이터 엔트로피 항이다. 이전 및 재구성 손실 항은 일반적으로 무시할 수 있습니다. \\ (\\mathcal{L}_{D}\\)는 _diffusion loss_로 정의되며,\n' +
      '\n' +
      '\\mathcal{L}_{D}=\\sum_{z}_{i}|\\mathbbb{E}_{q(\\mathbf{z}_{i}|\\mathbf{z}_{i}}\\left[D_{\\rm KL}(q(\\mathbf{z}_{s_{i}}|\\mathbf{z}_{s_{i}}||p(\\mathbf{z}_{z}_{t_{i}}}|\\mathbf{z}_{t_{i}}}\\right]\\, \\tag{30}\\L}}\\left[D_{\\rm KL}(q(\\mathbf{z}_{s_{i}}}||p(\\mathbf{z}_{z}_{i}}}|\\mathbf{z}_{t_{i}}}}\\t{i}}}\\left[D_{\\rm KL}(q(\\mathbf{z}_{i\n' +
      '\n' +
      '또한, \\(T\\to\\infty\\)일 때, \\(s\\to t\\)을 주의하면 식 (30)의 _continuous_Analog를 얻는다(Kingma et al., 2021):\n' +
      '\n' +
      '\\frac{L}_{w}:=\\frac{E}_{t\\sim U(0,1),\\mathbf{\\epsilon}\\sim\\mathcal{N}(0,1)}\\left[w(\\lambda)\\cdot-\\frac{d\\lambda_{t}{dt}\\|\\hat{\\mathbf{e}_{\\theta}(\\mathbf{z}_{t};\\lambda_{t})-\\mathbf{\\epsilon}\\|^{2}\\right]\\,\\tag{31}\\frac{n}(0,1)\\left[w(\\lambda)\\cdot-\\frac{d\\lambda_{t}}{d\\hat{\\mathbf{e}_{\\theta}(\\mathbf{z}_{t};\\lambda_{t})-\\mathbf{\\epsilon}\\|^{2}\\right]\\,\\tag{31}\\frac{d\\\n' +
      '\n' +
      'with \\(w(\\lambda_{t})=1\\), 여기서 \\(\\lambda_{t}=\\log{\\rm SNR}(t)\\). 가중함수 \\(w(\\lambda_{t})=1\\)는 영상의 질을 향상시키기 위해 종종 변화되는데, 예를 들어 역(-1/\\frac{d\\lambda_{t}{dt}\\)이므로 목적이 \\(\\mathbf{\\epsilon}\\) 손실에 대해 단순히 일정하다.\n' +
      '\n' +
      '롤링 확산의 목적은 롤링 확산에서 신호 대 잡음비는 고정된 상태로 유지되지만 로컬 시간 재매개변수를 고려해야 한다. \\(t_{k}:=t_{k}(t)\\)는 로컬 시간 재매개변수를 나타낸다는 것을 기억하라. 간결하게 우리는 \\({\\rm SNR}_{k}(t):={\\rm SNR}(t_{k})\\이라고 말할 수 있다. 상기 롤링 연속 시간 목표는:\n' +
      '\n' +
      '\\mathcal{L}_{\\infty}=\\mathbb{E}_{t\\sim U(0,1),\\mathbf{\\epsilon}\\sim\\mathcal{N}(0,1}\\left[\\sum_{k=1}^{K}-\\frac{d\\lambda(t_{k}}}{dt}\\frac{t_{k}(t}}{dt}\\cdot\\\\mathbf{\\epsilon}^{e}_{\\theta}^{k}(\\mathbf{z}_{\\mathbf{\\epsilon},t};t}\\hat{\\mathbf{e}_{\\theta}^{k}(\\mathbf{\\epsilon},t};t}\\left[\\sum_{k=1}^{K}-\\frac{d\\lambda(t_{k}}}{dt}\\cdot\\cdot\\mathbf{\\epsilon}^{e}_{\\theta}^{k\n' +
      '\n' +
      '여기서 \\(\\lambda(t):=\\log{\\rm SNR}(t)\\이다.\n' +
      '\n' +
      '본 논문의 프레임 분류, 즉, 이를 상기한다.\n' +
      '\n' +
      '(s,t) :=\\left\\{k\\mid s_{k}=t_{k}=0\\right\\}, \\tag{33}\\] \\[\\text{noise}(s,t) :=\\left\\{k\\mid s_{k}=t_{k}=1\\right\\},\\](34) \\[\\text{win}(s,t) :=\\left\\{k\\mid s_{k}\\in[0,1],t_{k}\\in(s_{k},1]\\right\\}, \\tag{35}\\\\text{noise}(s,t) :=\\left\\{k\\mid s_{k}=t_{k}=1\\right\\},\\](34) \\[\\text{win}(s,t) :=\\left\\{k\\mid s_{k}\\in[0,1],t_{k}\\in(s_{k},1]\\right\\}, \\tag{35}\\\n' +
      '\n' +
      '우리는 \\(k\\in\\text{clean}(t-dt,t)\\)과 \\(k\\in\\text{noise}(t-dt,t)\\)에 대한 \\(t_{k}^{\\prime}(t)=0\\)을 볼 수 있으며, 따라서 목표는 윈도우 상에서 0이 아닌 손실만을 갖는다:\n' +
      '\n' +
      '\\mathcal{L}_{\\infty}=\\mathbb{E}_{t\\sim U(0,1),\\mathbf{\\epsilon}\\sim\\mathcal{N}(0,1)}left[\\sum_{k\\in\\text{win}(t)}-\\frac{d\\lambda(t_{k}}}\\frac{t_{k}(t)}{dt}\\cdot\\cdot\\mathbf{\\heat{\\mathbf{\\epsilon}^{e}_{\\theta}^{k}(\\mathbf{\\epsilon},t};t)}\\|^{n}(0,1)}left[\\sum_{k\\in\\text{win}(t)}-\\frac{d\\lambda(t_{k}}}{dt}\\cdot\\cdot\\mathbf{e}_{\\theta}^{k}(\\mathbf{z}_{\\mathbf{\\epsilon},t};t\n' +
      '\n' +
      '이 도출은 (위에서 사용된) 글로벌 관점과 로컬 관점의 미묘한 차이를 보여준다. 변분 바운드가 사용되는 경우, 그것들은 동등하고 시간 도함수의 재패러메트리화는 완전히 잡음이 있는 프레임들 및 완전히 결정된 프레임들과 같은 에지 조건들을 고려한다. 그러나 글로벌 관점에서의 상수\\(\\mathbf{\\epsilon}\\) 또는 \\(v\\) 손실은 \\(t=0\\) 및 \\(t=1\\)에서 0으로 사라지지 않기 때문에 로컬 관점과는 다르다.\n' +
      '\n' +
      '실증적으로 창문의 \\(\\mathbf{\\epsilon}\\) 손실은 \\(v\\) 손실에 대해 좋은 성능을 보였다. 우리의 롤링 확산 목표는 윈도우에 정의된 상수\\(\\mathbf{\\epsilon}\\)로서, 윈도우 이외의 모든 것을 가리는 전역적으로 정의된 상수\\(\\mathbf{\\epsilon}\\) 손실의 두 가지 방법으로 볼 수 있다.\n' +
      '\n' +
      '## 부록 E 알고리즘\n' +
      '\n' +
      '본 논문에서는 구름 확산 훈련과 경계에서의 샘플링을 위한 알고리즘 개요를 제시한다. 본문에서는 자기회귀 롤아웃에 대한 알고리즘을 제시하였다.\n' +
      '\n' +
      '## \\(\\beta\\)에 대한 부록 F 하이퍼파라미터 탐색\n' +
      '\n' +
      '## 부록 G Rescaled Noise Schedule\n' +
      '\n' +
      'Kinetics-600 실험을 위해, 확산 시간\\((\\frac{1}{W},\\frac{2}{W},\\ldots\\frac{W}{W})\\)에서 완전한 소음으로부터 "롤링 상태"로 샘플링할 수 있는 다른 소음 스케줄을 사용하였다. 그로부터 선형 롤링 샘플링 스케줄(t_{w}^{\\text{lin}})을 사용하여 롤아웃 세대를 생성할 수 있다. 그 이유는 잡음 스케줄 \\(t_{w}^{\\text{init}\\)이 \\(\\operatorname{clip}\\) 연산을 사용한다고 가정했기 때문인데, 이는 본 논문과 부록 D에 요약된 바와 같이 중복되는 \\(\\operatorname{clean}(s,t)\\)의 샘플링을 의미한다.\n' +
      '\n' +
      '완전한 소음으로부터 시작하여 롤링 상태에서 종료되는 또 다른 스케줄은 다음과 같다:\n' +
      '\n' +
      '\\[t_{w}^{\\text{init,resc}}(t):=\\frac{w}{W}+t\\cdot(1-\\frac{w}{W}) \\tag{37}\\]\n' +
      '\n' +
      '우리가 분명히 \\(t=\\frac{1}{W}\\)에서 지역 시간이 \\(\\big{(}\\frac{1}{W},\\frac{2}{W},\\ldots,\\frac{W}{W}\\big{)}\\)임을 알 수 있다. 이 일정은 \\(t\\)에 정비례하지 않습니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline\n' +
      '**Require:**\\(\\mathcal{D}_{\\text{tr}:=\\{\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{N}\\}), \\(\\mathbf{x}\\in\\mathbbb{R}^{D\\times W}\\), \\(n_{\\text{ch}\\), \\(\\beta\\), \\(f_{\\theta}\\)\\\\\\\n' +
      '*repeat** & Sample \\(\\mathcal{D}_{\\text{tr}}\\), \\(t\\sim U(0,1)\\), \\(y\\sim B(\\beta)\\)\\\\(\\mathbf{x}\\)\n' +
      '**if**\\(y\\)**then**\\\\\\. Compute local time \\(t_{w}^{\\text{init}}(n_{\\text{ch}})\\), \\(w=0,\\ldots,W-1\\) \\\\\\\n' +
      '**else**\\\\\\  Compute local time \\(t_{w}^{\\text{lin}}(n_{\\text{ch}})\\), \\(w=0,\\ldots,W-1\\) \\\\\\\n' +
      '모든 \\(w=0,\\ldots,W-1\\)에 대한 \\(\\mathbf{z}_{t}\\sim\\sigma(\\mathbf{z}_{t}|\\mathbf{x})\\)에 대한 \\(\\mathbf{z}_{t}\\sim\\sigma(\\mathbf{z}\\sim\\sigma(\\mathbf{t}|\\mathbf{x})\\)를 식 (24)를 이용하여 \\(\\text{loc},\\theta}(\\mathbf{x}\\leftarrow f_{\\theta}(\\mathbf{x};t,\\mathbf{\\epsilon})\\)을 이용한 \\\\(\\theta\\)의 갱신\n' +
      '**until** Converged \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 9: 11 프레임당 100 스텝인 8192 FVD 샘플과 Kinetics-600(stride 8 rollout) 상의 오버샘플링 롤링 v.s. init 노이즈의 비교.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:17]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>