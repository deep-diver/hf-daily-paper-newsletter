<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '#소라: 대형 비전 모델의 배경, 기술, 한계 및 기회에 대한 고찰\n' +
      '\n' +
      'Yixin Liu\\({}^{1}\\) Kai Zhang\\({}^{1}\\) Yuan Li\\({}^{1}\\)1 Zhiling Yan\\({}^{1}\\)1 Chujie Gao\\({}^{1}\\)1 Ruoxi Chen\\({}^{1}\\) Zhengqing Yuan\\({}^{1}\\)1 Yue Huang\\({}^{1}\\)1 Jianfeng Gao\\({}^{2}\\)Lichao Sun\\({}^{1}\\)1 Jianfeng Gao\\({}^{2}\\)Lichao Sun\\({}^{1}\\)2\n' +
      '\n' +
      '동등한 기부금 그 순서는 주사위를 굴리면서 결정되었다. 추제, 루옥시, 위안, 유에, 정칭은 르하이 대학의 LAIR 연구실에서 학생들을 방문하고 있다. GitHub 링크는 [https://github.com/lichao-sun/SoraReview](https://github.com/lichao-sun/SoraReview)이다.\n' +
      '\n' +
      '각주 2: Lichao Sun is co-corresponding author: lis221@lehigh.edu\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '소라는 2024년 2월 오픈AI가 발표한 텍스트-비디오 생성 인공지능 모델로, 텍스트 명령어로부터 사실적 또는 상상적 장면의 비디오를 생성하고 물리적 세계를 시뮬레이션하는 데 잠재력을 보여주도록 훈련된다. 본 논문은 공공 기술 보고서와 역공학을 기반으로 모델의 배경, 관련 기술, 응용 프로그램, 남은 과제 및 텍스트-비디오 AI 모델의 향후 방향에 대한 포괄적인 검토를 제시한다. 우리는 먼저 소라의 개발을 추적하고 이 "세계 시뮬레이터"를 구축하는 데 사용되는 기본 기술을 조사한다. 그런 다음 영화 제작과 교육, 마케팅에 이르기까지 여러 산업에서 소라의 적용과 잠재적 영향에 대해 자세히 설명한다. 우리는 안전하고 편견 없는 비디오 생성을 보장하는 것과 같이 소라를 널리 배포하기 위해 해결해야 할 주요 도전과 한계에 대해 논의한다. 마지막으로 소라 및 비디오 생성 모델의 향후 개발과 그 분야의 발전이 어떻게 인간-AI 상호작용의 새로운 방식을 가능하게 하여 비디오 생성의 생산성과 창의성을 높일 수 있는지에 대해 논의한다.\n' +
      '\n' +
      '그림 1: 소라: 인공지능 기반 비전 생성의 돌파구.\n' +
      '\n' +
      '###### Contents\n' +
      '\n' +
      '*1 소개\n' +
      '*2 배경\n' +
      '	* 2.1 히스토리\n' +
      '	* 2.2 Advanced Concepts\n' +
      '*3 기술\n' +
      '	*3.1 소라의 개요\n' +
      '	* 3.2 데이터 전처리\n' +
      '		* 3.2.1 가변 듀레이션, 결의안, 종횡비\n' +
      '		* 3.2.2 통합 시각적 표현\n' +
      '		* 3.2.3 비디오 압축 네트워크\n' +
      '		* 3.2.4 시공잠재 패치\n' +
      '		* 3.2.5 토론\n' +
      '		* 3.2.6 확산 변압기\n' +
      '	* 3.3 모델링\n' +
      '		* 3.3.1 토론\n' +
      '	* 3.4 언어 명령어 팔로우\n' +
      '		* 3.4.1 대형 언어 모델\n' +
      '		* 3.4.2 텍스트 투 이미지\n' +
      '		* 3.4.3 Text-to-Video\n' +
      '		* 3.4.4 토론\n' +
      '	* 3.5 프롬프트 엔지니어링\n' +
      '		* 3.5.1 텍스트 프롬프트\n' +
      '		* 3.5.2 이미지 프롬프트\n' +
      '		* 3.5.3 동영상 프롬프트\n' +
      '		* 3.5.4 토론\n' +
      '	* 3.6 신뢰성\n' +
      '		* 3.6.1 안전관심\n' +
      '		* 3.6.2 기타 착취\n' +
      '		* 3.6.3 정렬\n' +
      '		* 3.6.4 토론\n' +
      '*4 응용\n' +
      '	*4.1 영화\n' +
      '	* 4.2 교육\n' +
      '	* 4.3 게임\n' +
      '	* 4.4 헬스케어\n' +
      '	*4.5 로보틱스\n' +
      '*5 토론\n' +
      '	*5.1 한도\n' +
      '	* 5.2 기회\n' +
      '* 6 결론\n' +
      '* 관련 작품들\n' +
      '\n' +
      'Introduction\n' +
      '\n' +
      '2022년 11월 ChatGPT가 출시된 이후 AI 기술의 출현은 일상과 산업의 다양한 측면에 깊이 통합되고 상호 작용을 재편하며 상당한 변화를 가져왔다[1, 2]. 이러한 모멘텀을 바탕으로 오픈AI는 2024년 2월 텍스트 프롬프트로부터 사실적 또는 상상적 장면의 비디오를 생성할 수 있는 텍스트 대 비디오 생성 AI 모델인 소라를 출시했다. 소라는 이전 동영상 생성 모델에 비해 사용자의 텍스트 지시사항을 준수하면서 최고 1분 길이의 동영상을 고품질로 제작할 수 있는 능력으로 구별된다[3]. 소라의 이러한 진행은 AI 시스템(또는 AI 에이전트)이 움직이는 물리적 세계를 이해하고 상호작용할 수 있는 능력을 갖추는 오랜 AI 연구 임무의 구현이다. 여기에는 복잡한 사용자 지침을 해석할 뿐만 아니라 이러한 이해도를 적용하여 동적 및 맥락적으로 풍부한 시뮬레이션을 통해 실제 문제를 해결할 수 있는 AI 모델을 개발하는 것이 포함된다.\n' +
      '\n' +
      '소라는 그림 2와 같이 복잡한 인간의 지시를 정확하게 해석하고 실행할 수 있는 놀라운 능력을 보여준다. 이 모델은 복잡한 배경에 대해 특정 행동을 수행하는 여러 문자를 포함하는 세부 장면을 생성할 수 있다. 연구자들은 소라의 숙련도가 사용자 생성 텍스트 프롬프트를 처리하는 것뿐만 아니라 시나리오 내에서 요소의 복잡한 상호 작용을 식별하기 때문이라고 생각한다. 소라의 가장 두드러진 측면 중 하나는 높은 시각적 품질과 강력한 시각적 일관성을 유지하면서 최대 1분 길이의 비디오를 위한 용량이다. 짧은 영상만 생성할 수 있는 이전 모델들과 달리 소라의 분장 영상 생성은 첫 프레임부터 마지막까지 시각적으로 일관된 진행감과 여정을 가지고 있다. 또한, 소라의 발전은 움직임과 상호 작용에 대한 미묘한 묘사로 확장된 비디오 시퀀스를 생성하는 능력에서 분명하며, 이전 비디오 생성 모델을 특징짓는 더 짧은 클립과 더 간단한 시각적 렌더링의 제약을 극복한다. 이 기능은 AI 기반 창의적 도구의 도약을 나타내며, 사용자는 텍스트 내러티브를 풍부한 시각적 이야기로 변환할 수 있다. 전반적으로 이러한 발전은 묘사된 장면의 물리적 및 맥락적 역학에 대한 미묘한 통찰력을 제공하는 _world 시뮬레이터_로서의 소라의 잠재력을 보여준다. [3].\n' +
      '\n' +
      '**기술.** 소라의 중심에는 미리 훈련된 _확산 트랜스포머_[4]가 있다. 트랜스포머 모델은 많은 자연어 작업에 확장 가능하고 효과적인 것으로 입증되었습니다. GPT-4와 같은 강력한 대규모 언어 모델(LLM)과 유사하게 소라는 텍스트를 구문 분석하고 복잡한 사용자 지침을 이해할 수 있다. 비디오 생성을 계산적으로 효율적으로 하기 위해, Sora는 _spacetime latent patch_를 빌딩 블록으로 사용한다. 구체적으로, 소라는 원시 입력 비디오를 잠재 시공간 표현으로 압축한다. 그런 다음 압축된 비디오에서 잠재 시공간 패치의 시퀀스를 추출하여 짧은 간격에 걸쳐 시각적 외관과 움직임 동학을 모두 캡슐화한다. 언어 모델의 단어 토큰과 유사한 이러한 패치는 소라에게 비디오를 구성하는 데 사용할 상세한 _시각적 문구_를 제공한다. 소라의 텍스트-비디오 생성은 확산 변환기 모델에 의해 수행된다. 시각적 노이즈로 채워진 프레임으로 시작하여, 모델은 이미지를 반복적으로 디노이징하고 제공된 텍스트 프롬프트에 따라 특정 세부사항을 도입한다. 본질적으로\n' +
      '\n' +
      '도 2: 텍스트-비디오 생성에서의 소라의 예. 텍스트 명령어는 OpenAI 소라 모델에 주어지며, 명령어에 따라 3개의 비디오를 생성한다.\n' +
      '\n' +
      '생성된 비디오는 멀티-스텝 정제 프로세스를 통해 출현하며, 각각의 스텝은 원하는 콘텐츠 및 품질에 더 정렬되도록 비디오를 정제한다.\n' +
      '\n' +
      '**소라의 하이라이트.**소라의 역량은 다양한 측면에서 심오한 의미를 갖는다.\n' +
      '\n' +
      '*_시뮬레이션 능력 향상_: 규모에서 소라를 훈련시키는 것은 물리적 세계의 다양한 측면을 시뮬레이션하는 놀라운 능력에 기인한다. 명시적 3D 모델링이 부족함에도 불구하고, 소라는 객체 지속성을 포함하고 세계와의 간단한 상호 작용을 시뮬레이션하는 동적 카메라 모션 및 장거리 일관성과 함께 3D 일관성을 나타낸다. 또한, 소라는 시각적 충실도를 유지하면서 기본 정책에 의해 제어되는 마인크래프트와 같은 디지털 환경을 흥미롭게 시뮬레이션한다. 이러한 새로운 능력은 비디오 모델의 스케일링이 물리적 세계와 디지털 세계의 복잡성을 시뮬레이션하기 위한 AI 모델을 만드는 데 효과적임을 시사한다.\n' +
      '* _Boosting creativity_: 간단한 객체이든 전체 장면이든 텍스트를 통해 개념을 요약하고, 몇 초 이내에 렌더링되는 사실적이거나 고도로 양식화된 비디오를 보는 것을 상상해 보세요. 소라는 아이디어의 더 빠른 탐색과 개선을 위한 가속화된 디자인 프로세스를 허용하여 아티스트, 영화 제작자 및 디자이너의 창의성을 크게 향상시킵니다.\n' +
      '*_드라이브 교육 혁신_: 시각 보조 장치는 오랫동안 교육에서 중요한 개념을 이해하는 데 필수적이었다. 소라와 함께, 교육자들은 학생들의 관심을 사로잡고 학습 효율성을 향상시키기 위해 수업 계획을 텍스트에서 비디오로 쉽게 바꿀 수 있습니다. 과학 시뮬레이션에서 역사적 극화에 이르기까지 가능성은 무한하다.\n' +
      '* _Enhancing Accessibility_: 비주얼 도메인에서의 접근성 향상이 무엇보다 중요하다. 소라는 텍스트 설명을 시각적 내용으로 변환하여 혁신적인 솔루션을 제공합니다. 이 능력은 시각 장애인을 포함한 모든 개인이 콘텐츠 제작에 적극적으로 참여하고 보다 효과적인 방식으로 다른 사람과 상호 작용할 수 있도록 한다. 결과적으로 모든 사람이 비디오를 통해 자신의 아이디어를 표현할 수 있는 기회를 갖는 보다 포괄적인 환경을 허용한다.\n' +
      '*_Fostering emerging applications_: 소라의 애플리케이션은 방대하다. 예를 들어, 마케터는 특정 청중 설명에 맞춘 동적 광고를 만드는 데 사용할 수 있습니다. 게임 개발자는 이를 사용하여 플레이어 내러티브에서 맞춤형 비주얼 또는 캐릭터 동작을 생성할 수 있다.\n' +
      '\n' +
      '**제한과 기회** 소라의 업적은 AI의 중요한 발전을 강조하지만 도전은 여전히 남아 있습니다. 복잡한 행동을 묘사하거나 미묘한 표정을 포착하는 것은 모델이 향상될 수 있는 영역 중 하나이다. 또한 생성된 콘텐츠의 편향을 완화하고 유해한 시각적 출력을 방지하는 것과 같은 윤리적 고려 사항은 개발자, 연구원 및 광범위한 커뮤니티의 책임 있는 사용의 중요성을 강조한다. 소라의 산출물이 지속적으로 안전하고 편견 없는지 확인하는 것이 주요 과제이다. 학계 및 산업계 연구팀이 끈질기게 진보를 하는 등 영상 생성 분야가 빠르게 발전하고 있다. 경쟁하는 텍스트 대 비디오 모델의 출현은 소라가 곧 역동적인 생태계의 일부가 될 수 있음을 시사한다. 이러한 협력적이고 경쟁력 있는 환경은 혁신을 촉진하여 향상된 비디오 품질과 근로자의 생산성을 향상시키고 사람들의 삶을 더 즐겁게 만드는 데 도움이 되는 새로운 애플리케이션으로 이어집니다.\n' +
      '\n' +
      '**우리의 기여도** 출판된 기술 보고서와 역공학을 기반으로 이 논문은 소라의 배경, 관련 기술, 새로운 응용 프로그램, 현재 제한 사항 및 향후 기회에 대한 첫 번째 포괄적인 검토를 제시한다.\n' +
      '\n' +
      '## 2 Background\n' +
      '\n' +
      '### History\n' +
      '\n' +
      '컴퓨터 비전(CV)의 영역에서 딥러닝 혁명 이전에 전통적인 이미지 생성 기술은 손으로 조작한 특징을 기반으로 텍스처 합성[5] 및 텍스처 매핑[6]과 같은 방법에 의존했다. 그러나 이러한 방법은 복잡하고 생생한 이미지를 생성하는 능력에 한계가 있었다.\n' +
      '\n' +
      'GAN(Generative Adversarial Networks) [7]과 VAE(Variational Autoencoder) [8]의 도입은 다양한 응용 분야에 걸쳐 뛰어난 능력으로 인해 중요한 전환점을 맞이했다. 흐름 모델[9] 및 확산 모델[10]과 같은 후속 개발은 더 많은 세부 사항과 품질로 이미지 생성을 더욱 향상시켰다. 최근 인공지능 생성 콘텐츠(Artificial Intelligence Generated Content, AIGC) 기술의 발전으로 콘텐츠 생성이 민주화되어 사용자가 간단한 텍스트 명령어를 통해 원하는 콘텐츠를 생성할 수 있게 되었다[11].\n' +
      '\n' +
      '지난 10년 동안 생성 CV 모델의 개발은 그림 3과 같이 다양한 경로를 취했다. 이 경관은 BERT[13] 및 GPT[14]에 의해 입증된 바와 같이 NLP에서 트랜스포머 아키텍처[12]를 성공적으로 적용한 후 특히 이동하기 시작했다. CV에서 연구자들은 트랜스포머 아키텍처를 시각적 구성요소와 결합하여 비전 트랜스포머(ViT)[15] 및 스윈 트랜스포머[16]와 같은 다운스트림 CV 작업에 적용할 수 있도록 함으로써 이 개념을 훨씬 더 많이 취한다. 변압기의 성공과 병행하여, 확산 모델은 또한 이미지 및 비디오 생성 분야에서 상당한 진전을 이루었다[10]. 확산 모델은 노이즈를 U-Nets[17]로 이미지로 변환하기 위한 수학적으로 건전한 프레임워크를 제공하며, 여기서 U-Nets는 각 단계에서 노이즈를 예측하고 완화하도록 학습함으로써 이러한 프로세스를 용이하게 한다. 2021년부터 AI에서 가장 중요한 초점은 멀티모달 모델로 알려진 인간의 지시를 해석할 수 있는 생성 언어와 비전 모델에 있다. 예를 들어, CLIP[18]은 트랜스포머 아키텍처와 시각적 요소를 결합하여 방대한 텍스트 및 이미지 데이터 세트에 대한 교육을 용이하게 하는 선구적인 비전 언어 모델이다. CLIP는 처음부터 시각적, 언어적 지식을 통합함으로써 멀티모달 생성 프레임워크 내에서 이미지 인코더로 기능할 수 있다. 또 다른 주목할 만한 예는 적응성과 사용의 용이성으로 유명한 다재다능한 텍스트 대 이미지 AI 모델인 Stable Diffusion[19]이다. 텍스트 입력을 디코딩하고 다양한 스타일의 이미지를 생성하기 위해 트랜스포머 아키텍처와 잠재 확산 기술을 사용하여 멀티모달 AI의 발전을 더욱 보여준다.\n' +
      '\n' +
      '2022년 11월 ChatGPT 출시 이후, 우리는 2023년에 Stable Diffusion[19], Midjourney[20], DALL-E 3[21]과 같은 상업적 텍스트 이미지 제품의 출현을 목격했다. 이러한 도구를 통해 사용자는 간단한 텍스트 프롬프트로 고해상도 및 품질의 새로운 이미지를 생성할 수 있어 창의적인 이미지 생성에서 AI의 잠재력을 보여준다. 그러나, 텍스트-이미지로부터 텍스트-비디오로의 전환은 비디오의 시간적 복잡성으로 인해 어렵다. 산학계의 수많은 노력에도 불구하고 Pika[22], Gen-2[23] 등 현존하는 대부분의 영상 생성 도구는 불과 몇 초의 짧은 영상만을 생산하는 데 그치고 있다. 이러한 맥락에서 소라는 NLP 도메인에서 ChatGPT의 영향과 유사한 상당한 돌파구를 나타낸다. 소라는 인간의 지시에 따라 최대 1분까지 영상을 생성할 수 있는 최초의 모델로, 생성 AI의 연구 개발에 지대한 영향을 미치는 이정표를 표시했다. 비전 생성 모델의 최신 발전에 쉽게 접근할 수 있도록 가장 최근의 작업이 부록과 GitHub에서 컴파일 및 제공되었다.\n' +
      '\n' +
      '그림 3: 비전 도메인에서 생성 AI의 이력.\n' +
      '\n' +
      '### Advanced Concepts\n' +
      '\n' +
      '비전 모델에 대한 스케일링 법칙.LLM에 대한 스케일링 법칙과 함께, 비전 모델의 개발이 유사한 스케일링 법칙을 따르는지 묻는 것은 당연하다. 최근 Zhai et al. [24]는 충분한 훈련 데이터를 가진 ViT 모델에 대한 성능-계산 프론티어가 대략 (포화) 전력 법칙을 따른다는 것을 입증하였다. 그 뒤를 이어 구글 리서치[25]는 22B-파라미터 ViT의 고효율적이고 안정적인 훈련을 위한 레시피를 제시했다. 결과들은 동결된 모델을 사용하여 임베딩들을 생성하고, 그 위에 얇은 층들을 트레이닝하는 것이 큰 성능을 달성할 수 있음을 보여준다. 소라는 대형 비전 모델(LVM)로서 이러한 스케일링 원리와 일치하여 텍스트 대 비디오 생성에서 몇 가지 새로운 능력을 발견한다. 이 상당한 진행은 LVM이 LLM에서 볼 수 있는 것과 같은 발전을 달성할 가능성을 강조한다.\n' +
      '\n' +
      '**긴급 능력.** LLM의 긴급 능력은 특정 규모에서 나타나는 정교한 행동 또는 기능이며, 종종 모델의 매개변수 크기와 연결되어 개발자가 명시적으로 프로그래밍하거나 예상하지 못한 기능입니다. 이러한 능력은 광범위한 매개변수 수와 결합된 다양한 데이터 세트에 걸친 모델의 포괄적인 훈련에서 나타나기 때문에 "출현"이라고 한다. 이 조합을 통해 모델은 연결을 형성하고 단순한 패턴 인식이나 로트 암기를 능가하는 추론을 그릴 수 있다. 일반적으로 이러한 능력의 출현은 소규모 모델의 성능을 외삽하여 쉽게 예측할 수 없다. Chat-GPT 및 GPT-4와 같은 수많은 LLM이 새로운 능력을 나타내지만 소라가 등장할 때까지 유사한 능력을 보여주는 비전 모델은 부족했다. 소라의 기술 보고서에 따르면, 컴퓨터 비전 분야에서 중요한 이정표를 표시하면서 확인된 응급 능력을 보여주는 최초의 비전 모델이다.\n' +
      '\n' +
      '소라는 그 창발적 능력 외에도 수업 후행, 시각적 프롬프트 엔지니어링, 비디오 이해 등 다른 주목할 만한 능력을 발휘한다. 소라의 기능성의 이러한 측면은 비전 영역에서 중요한 발전을 나타내며 나머지 섹션에서 탐색 및 논의될 것이다.\n' +
      '\n' +
      '## 3 Technology\n' +
      '\n' +
      '소라에 대한 개요\n' +
      '\n' +
      '코어 에센스에서 소라는 그림 4와 같이 유연한 샘플링 치수를 가진 확산 트랜스포머[4]이다. (1) 시공간 압축기가 먼저 원본 비디오를 잠재 공간으로 매핑한다. (2) ViT는 그 후 토큰화된 잠재 표현을 처리하고, 잡음 제거된 잠재 표현을 출력한다. (3) CLIP-유사 [26] 컨디셔닝 메커니즘은 LLM-증강된 사용자 명령들 및 잠재적으로 시각적 프롬프트들을 수신하여 확산 모델이 스타일링된 또는 테마화된 비디오들을 생성하도록 안내한다. 많은 노이즈 제거 후\n' +
      '\n' +
      '도 4: **역공학: 소라 프레임워크**단계 개요, 생성된 비디오의 잠재 표현이 획득된 다음, 대응하는 디코더와 함께 픽셀 공간에 다시 매핑된다. 이 절에서는 소라가 사용하는 기술을 역공학하고 광범위한 관련 작업을 논의하는 것을 목표로 한다.\n' +
      '\n' +
      '### Data Pre-processing\n' +
      '\n' +
      '3.2.1 가변 듀레이션, 결의, 종횡비\n' +
      '\n' +
      '소라의 한 가지 구별되는 특징은 그림 5와 같이 고유 크기[3]에서 비디오와 이미지를 훈련, 이해 및 생성하는 능력이다. 전통적인 방법은 종종 고정된 낮은 해상도에서 정사각형 프레임을 갖는 균일한 표준 -전형적으로 짧은 클립에 맞게 비디오의 종횡비를 조정, 크롭 또는 조정한다[27][28][29]. 이러한 샘플은 종종 더 넓은 시간적 보폭에서 생성되고 최종 단계로서 별도로 훈련된 프레임 삽입 및 해상도 렌더링 모델에 의존하여 비디오 전반에 걸쳐 불일치를 생성한다. 확산 변압기 아키텍처[4]를 활용하여(섹션 3.2.4 참조), 소라는 시각적 데이터의 다양성을 수용하는 첫 번째 모델이며 광범위한 1920x1080p 비디오에서 수직 1080x1920p 비디오 및 그 사이의 모든 것을 원래의 치수에 손상 없이 광범위한 비디오 및 이미지 형식으로 샘플링할 수 있다.\n' +
      '\n' +
      '네이티브 크기의 데이터에 대한 트레이닝은 생성된 비디오에서 구성 및 프레이밍을 상당히 향상시킨다. 경험적 연구 결과는 원래 종횡비를 유지함으로써 소라가 보다 자연스럽고 일관된 시각적 내러티브를 달성함을 시사한다. 소라와 균일하게 크롭된 사각형 비디오에 대해 훈련된 모델의 비교는 그림 6과 같은 분명한 이점을 보여준다. 소라가 제작한 비디오는 더 나은 프레이밍을 보여주며, 피사체가 사각형 크롭으로 인해 때때로 잘린 뷰와 대조적으로 장면에서 완전히 캡처되도록 한다.\n' +
      '\n' +
      '원본 비디오 및 이미지 특성에 대한 이러한 미묘한 이해와 보존은 생성 모델 분야에서 상당한 발전을 나타낸다. 소라의 접근법은 보다 진정성 있고 매력적인 비디오 생성의 가능성을 보여줄 뿐만 아니라 생성 AI에서 고품질 결과를 달성하기 위한 훈련 데이터의 다양성의 중요성을 강조한다. 소라의 훈련 방법은\n' +
      '\n' +
      '그림 5: 소라는 1920x1080p에서 1080x1920p 사이의 유연한 크기 또는 해상도로 이미지를 생성할 수 있다.\n' +
      '\n' +
      '그림 6: 모델 훈련에서 일반적인 관행인 정사각형 모양으로 비디오를 크롭하는 소라(오른쪽)와 수정된 모델(왼쪽)의 비교를 통해 이점을 강조한다.\n' +
      '\n' +
      '리처드 서튼의 더 비터 레슨[30]의 핵심 교리는 인간이 설계한 기능보다 계산을 활용하는 것이 더 효과적이고 유연한 AI 시스템으로 이어진다는 것이다. 확산 변압기의 원래 디자인이 단순성과 확장성을 추구하는 것처럼[31], 소라의 고유 크기의 데이터에 대한 훈련 전략은 인간 유래 추상화에 대한 전통적인 AI 의존을 피하여 계산 능력으로 확장하는 일반주의적 방법을 선호한다. 이 섹션의 나머지 부분에서는 소라의 아키텍처 설계를 역공학하고 관련 기술에 대해 논의하여 이 놀라운 기능을 달성하려고 노력합니다.\n' +
      '\n' +
      '###### 3.2.2 통합 시각적 표현\n' +
      '\n' +
      '다양한 지속 시간, 해상도 및 종횡비를 갖는 이미지 및 비디오를 포함하는 다양한 시각적 입력을 효과적으로 처리하기 위해, 중요한 접근법은 모든 형태의 시각적 데이터를 통합된 표현으로 변환하는 것을 포함하며, 이는 생성 모델의 대규모 훈련을 용이하게 한다. 구체적으로 소라는 초기에 영상을 저차원 잠재공간으로 압축한 후, 표현을 시공간 패치로 분해하여 영상을 패치화한다. 그러나 소라의 기술 보고서 [3]은 수준 높은 아이디어를 제시할 뿐이어서 재생산이 연구 커뮤니티에 도전적이다. 이 섹션에서는 잠재적인 성분과 기술 경로를 역공학하려고 한다. 또한 기존 문헌의 통찰력을 바탕으로 소라의 기능을 복제할 수 있는 실행 가능한 대안을 논의할 것이다.\n' +
      '\n' +
      '####3.2.3 비디오 압축 네트워크\n' +
      '\n' +
      '소라의 비디오 압축 네트워크(또는 비주얼 인코더)는 입력 데이터, 특히 원시 비디오의 차원을 감소시키고, 도 7에 도시된 바와 같이 시간적으로 그리고 공간적으로 압축되는 잠재 표현을 출력하는 것을 목표로 한다. 기술 보고서의 참조에 따르면, 압축 네트워크는 VAE 또는 벡터 양자화된-VAE(VQ-VAE)에 기초하여 구축된다[32]. 그러나 VAE는 기술 보고서에서 언급한 대로 크기 조정 및 크롭을 사용하지 않으면 모든 크기의 시각적 데이터를 통일되고 고정된 크기의 잠재 공간에 매핑하는 것이 어렵다. 이 문제를 해결하기 위해 두 가지 별개의 구현을 요약합니다.\n' +
      '\n' +
      '**Spatial-patch Compression.** 이것은 비디오 프레임들을 ViT[15] 및 MAE[33](도 8 참조)에서 채용된 방법론들과 유사한 고정된 크기의 패치로 변환하는 것을 포함하며, 이들을 잠재 공간으로 인코딩하기 전에. 이 접근법은 개별 패치의 처리를 통해 전체 프레임을 인코딩하기 때문에 다양한 해상도 및 종횡비의 비디오를 수용하는데 특히 효과적이다. 이어서, 이러한 공간 토큰들은 공간적-시간적 잠재 표현을 생성하기 위해 시간적 시퀀스로 조직된다. 이 기술은 몇 가지 중요한 고려 사항을 강조한다: _Temporal dimension variability_ - 훈련 비디오의 가변 지속 시간, 시간 차원\n' +
      '\n' +
      '그림 8: ViT는 이미지를 고정된 크기의 패치로 분할하고, 각각을 선형적으로 임베딩하고, 위치 임베딩을 추가하고, 결과적인 벡터 시퀀스를 표준 트랜스포머 인코더에 공급한다.\n' +
      '\n' +
      '그림 7: 높은 수준에서 소라는 먼저 비디오를 낮은 차원의 잠재 공간으로 압축하고, 이어서 표현을 시공간 패치로 분해함으로써 비디오를 패치로 변환한다. 출처: 소라의 기술 보고서[3].\n' +
      '\n' +
      '잠재적 공간 표현은 고정될 수 없다. 이를 해결하기 위해, 특정 수의 프레임(패딩 또는 시간 보간[34]이 훨씬 더 짧은 비디오에 대해 필요할 수 있음)을 샘플링하거나 후속 처리를 위해 보편적으로 확장(초장)된 입력 길이를 정의할 수 있다(보다 상세한 내용은 섹션 3.2.4에서 설명됨); _Utilization of pre-trained visual encoders_ - for processing high resolution, leveraging existing pre-trained visual encoders, as a VAE encoder from Stable Diffusion [19]는 대부분의 연구자들에게 바람직하지만, 소라의 팀은 잠재 확산 모델[19, 35, 36]을 훈련시키는 방식을 통해 디코더(비디오 생성기)와 함께 자신의 압축 네트워크를 처음부터 훈련시킬 것으로 예상된다. 이러한 인코더는 큰 크기의 패치(예를 들어, \\(256\\times 256\\))를 효율적으로 압축할 수 있으며, 대규모 데이터의 관리를 용이하게 한다; _Temporal 정보 aggregation_ - 이 방법은 주로 공간 패치 압축에 초점을 맞추기 때문에 모델 내에서 시간 정보를 집계하기 위한 추가 메커니즘이 필요하다. 이 측면은 시간에 따른 동적 변화를 포착하는 데 중요하며 후속 섹션에서 추가로 정교화된다(섹션 3.2.6 및 그림 14의 세부사항 참조).\n' +
      '\n' +
      '**Spatial-temporal-patch Compression.** 이 기법은 비디오 데이터의 공간적 차원과 시간적 차원을 모두 캡슐화하도록 설계되어 포괄적인 표현을 제공한다. 이 기술은 단순히 프레임들에 걸친 움직임 및 변화들을 고려함으로써 정적 프레임들을 분석하는 것을 넘어 확장되어, 비디오의 동적 양상들을 캡처한다. 3D 컨볼루션의 활용은 이러한 통합을 달성하기 위한 간단하고 강력한 방법으로 등장한다[37]. 그림 9에 그림과 순수 공간-패치화에 대한 비교가 묘사되어 있다. 공간-패치 압축과 유사하게, 고정된 커널 크기, 스트라이드, 출력 채널과 같은 미리 결정된 컨볼루션 커널 파라미터를 갖는 공간-시간-패치 압축을 채용하는 것은 비디오 입력의 상이한 특성들로 인해 잠재 공간의 차원들의 변화를 초래한다. 이러한 변동성은 주로 처리되는 비디오의 다양한 지속 시간과 해상도에 의해 주도됩니다. 이 문제를 완화하기 위해 공간 패치화를 위해 채택된 접근법은 이 맥락에서 동일하게 적용 가능하고 효과적이다.\n' +
      '\n' +
      '요약하면, 우리는 패치에 대한 연산이 다른 유형의 비디오를 처리하기에 더 유연하기 때문에 VAE 또는 VQ-VQE와 같은 변형을 기반으로 두 패치 수준 압축 접근법을 역공학한다. 소라는 높은 충실도의 비디오를 생성하는 것을 목표로 하기 때문에, 효율적인 압축을 위해 큰 패치 크기 또는 커널 크기가 사용된다. 여기서는 단순성, 확장성 및 훈련 안정성을 위해 고정 크기 패치가 사용되기를 기대한다. 그러나 다양한 크기의 패치를 사용하여 잠재 공간의 전체 프레임 또는 비디오의 차원을 일관되게 만들 수도 있다[39]. 그러나, 이는 잘못된 위치 인코딩을 초래할 수 있고, 디코더가 다양한 크기의 잠재 패치들을 갖는 비디오들을 생성하기 위한 도전들을 야기한다.\n' +
      '\n' +
      '도 9: 비디오 압축을 위한 상이한 패치화 간의 비교. 출처: ViViT[38]. **(Left)** 공간 패치화는 단순히 \\(n_{t}\\) 프레임을 샘플링하고 ViT에 따라 각각의 2D 프레임을 독립적으로 임베딩한다. **(Right)** 시공간 패치화는 시공간 입력 볼륨에 걸쳐 있는 비중첩 또는 중첩 관을 추출하고 선형으로 임베딩한다.\n' +
      '\n' +
      '###### 3.2.4 시공간 잠재 패치\n' +
      '\n' +
      '압축 네트워크 부분에 남아있는 중추적인 우려가 있다: 확산 변압기의 입력 계층으로 패치를 공급하기 전에 잠재 공간 차원(즉, 상이한 비디오 유형으로부터의 잠재 특징 청크 또는 패치의 수)의 가변성을 처리하는 방법. 여기에서 우리는 몇 가지 해결책에 대해 논의한다.\n' +
      '\n' +
      '소라의 기술 보고서와 해당 참고 문헌에 따르면 **패치 n\' 팩(PNP)**[40]이 해결책일 가능성이 높다. PNP는 도 10에 도시된 바와 같이 단일 시퀀스로 상이한 이미지들로부터 다수의 패치들을 패킹한다. 이 방법은 토큰들을 드롭함으로써 가변 길이 입력들에 대한 효율적인 트레이닝을 수용하는 자연 언어 프로세싱[41]에서 사용되는 예시적인 패킹에 의해 영감을 받는다. 여기서, 패치화 및 토큰 임베딩 단계는 압축 네트워크에서 완료될 필요가 있지만, 소라는 확산 트랜스포머가 [4]와 같이 트랜스포머 토큰에 대한 잠재성을 더 패치화할 수 있다. 2차 패치화가 있든 없든 간에 우리는 두 가지 문제를 해결해야 한다. 즉, 이러한 토큰을 컴팩트하게 포장하는 방법과 어떤 토큰을 삭제해야 하는지 제어하는 방법. 첫 번째 관심사를 위해, 충분한 잔여 공간을 갖는 첫 번째 시퀀스에 예를 추가하는 간단한 탐욕 접근법이 사용된다. 더 이상의 예가 적합할 수 없으면 시퀀스는 패딩 토큰으로 채워져 배치 작업에 필요한 고정된 시퀀스 길이를 산출합니다. 이러한 간단한 패킹 알고리즘은 입력의 길이의 분포에 따라 상당한 패딩으로 이어질 수 있다. 한편, 시퀀스 길이를 조정하고 패딩을 제한함으로써 효율적인 패킹을 보장하기 위해 샘플링하는 해상도와 프레임을 제어할 수 있다. 두 번째 관심사에 대해 직관적인 접근 방식은 유사한 토큰[42, 43, 33, 44]을 삭제하거나 PNP와 마찬가지로 드롭 레이트 스케줄러를 적용하는 것이다. 그러나 _3D Consistency_는 소라의 좋은 속성 중 하나라는 점에 주목할 필요가 있다. 토큰을 삭제하는 것은 훈련 중에 세밀한 세부 사항을 무시할 수 있다. 따라서, 우리는 OpenAI가 매우 긴 컨텍스트 윈도우를 사용하고 비디오로부터 모든 토큰을 패킹하는 것이 계산적으로 비싸지만, 예를 들어 다중 헤드 주의[45, 46] 연산자는 시퀀스 길이에서 2차 비용을 나타낼 가능성이 높다고 믿는다. 구체적으로, 긴 시간 비디오의 시공간 잠재 패치는 하나의 시퀀스로 패킹될 수 있고, 여러 짧은 시간 비디오의 시공간 잠재 패치는 다른 시퀀스에 연결된다.\n' +
      '\n' +
      '#### 3.2.5 Discussion\n' +
      '\n' +
      '우리는 소라가 사용할 수 있는 데이터 전처리에 대한 두 가지 기술적 솔루션에 대해 논의한다. 두 솔루션 모두 모델링을 위한 유연성과 확장성의 특성으로 인해 패치 수준에서 수행된다. 비디오가 표준 크기로 크기 조정, 크롭 또는 트리밍되는 이전 접근법과 달리 소라는 원래 크기로 데이터를 훈련한다. 여러 가지 이점(섹션 3.2.1의 상세한 분석 참조)이 있지만 몇 가지 기술적 문제를 가져오며, 그 중 가장 중요한 것 중 하나는 신경망이 가변 지속 시간, 해상도 및 종횡비의 시각적 데이터를 본질적으로 처리할 수 없다는 것이다. 역공학을 통해 우리는 소라가 먼저 시각적 패치를 저차원 잠재 표현으로 압축하고, 그러한 잠재 패치 또는 추가로 패치화된 잠재 패치를 시퀀스로 배열한 후, 이러한 잠재 패치에 노이즈를 주입한다.\n' +
      '\n' +
      '그림 10: 패치 패킹은 종횡비가 보존된 가변 해상도 이미지 또는 비디오를 가능하게 한다.6 토큰 드롭은 어떻게든 데이터 증강으로 처리될 수 있다. 출처: NaViT[40].\n' +
      '\n' +
      '확산 변압기의 입력층으로 공급하기 전에. 시공간 패치화는 구현이 간단하여 Sora에 의해 채택되며, 고정보 밀도 토큰으로 컨텍스트 길이를 효과적으로 줄일 수 있고 시간 정보의 후속 모델링의 복잡성을 줄일 수 있다.\n' +
      '\n' +
      '연구 커뮤니티에는 사전 훈련된 체크포인트(예: 압축 네트워크)[47]를 활용하는 것, 컨텍스트 창을 단축하는 것, (그룹화된) 다중 쿼리 주의[48, 49] 또는 효율적인 아키텍처(예: 맘바[50])와 같은 경량 모델링 메커니즘을 사용하는 것, 필요한 경우 다운샘플링 데이터 및 드롭 토큰을 사용하는 것을 포함하여 비디오 압축 및 표현을 위한 비용 효율적인 대체 솔루션을 사용하는 것이 좋다. 비디오 모델링에 대한 효율성과 효과성의 절충은 탐구해야 할 중요한 연구 주제이다.\n' +
      '\n' +
      '###### 3.2.6 확산 변압기\n' +
      '\n' +
      '### Modeling\n' +
      '\n' +
      '**이미지 확산 트랜스포머.** 전통적인 확산 모델 [51, 52, 53]은 주로 잡음제거 네트워크 백본에 대한 다운샘플링 및 업샘플링 블록을 포함하는 컨볼루션 U-네트를 레버리지한다. 그러나 최근 연구에 따르면 U-Net 아키텍처는 확산 모델의 좋은 성능에 중요하지 않다. 보다 유연한 변압기 아키텍처를 통합함으로써, 변압기 기반 확산 모델은 더 많은 훈련 데이터와 더 큰 모델 파라미터를 사용할 수 있다. 이 라인을 따라 DiT[4]와 U-ViT[54]는 잠재 확산 모델에 비전 트랜스포머를 사용하는 첫 번째 작업 중 하나이다. ViT에서와 같이, DiT는 다중-헤드 자기-어텐션 계층 및 일부 계층 규범 및 스케일링 계층들과 인터레이스된 포인트-피드-포워드 네트워크를 채용한다. 더욱이, 도 11에 도시된 바와 같이, DiT는 제로-초기화를 위한 추가적인 MLP 계층과 적응 계층 규범(AdaLN)을 통한 컨디셔닝을 통합하며, 이는 각각의 잔차 블록을 아이덴티티 함수로서 초기화하고 따라서 트레이닝 프로세스를 크게 안정화시킨다. DiT의 확장성과 유연성은 실증적으로 검증된다. DiT는 확산 모델의 새로운 백본이 된다. U-ViT에서는 그림 11과 같이 시간, 조건 및 노이즈 이미지 패치를 포함한 모든 입력을 토큰으로 처리하고 얕은 변압기 층과 깊은 변압기 층 사이의 긴 스킵 연결을 제안한다. 결과는 다운샘플링 및 업샘플링 연산자가\n' +
      '\n' +
      '도 11: DiT(좌측) 및 U-ViT(우측)의 전체 프레임워크\n' +
      '\n' +
      'CNN 기반 U-Net에서 항상 필요한 것은 아니며, U-ViT는 이미지 및 텍스트 대 이미지 생성에서 기록적인 FID 점수를 달성한다.\n' +
      '\n' +
      'Masked AutoEncoder (MAE) [33], Masked Diffusion Transformer (MDT) [55]와 같이 마스크 잠재 모델링을 확산 프로세스에 통합하여 이미지 합성에서 객체 의미 부분 간의 문맥 관계 학습을 명시적으로 강화한다. 구체적으로, 도 12에 도시된 바와 같이, MDT는 트레이닝 동안 추가적인 마스킹된 토큰 재구성 태스크를 위해 측면 보간을 사용하여 트레이닝 효율을 높이고 추론을 위한 강력한 상황 인식 위치 임베딩을 학습한다. DiT[4]에 비해 MDT는 더 나은 성능과 더 빠른 학습 속도를 달성한다. 시간-조건화 모델링을 위해 AdaLN(즉, 시프트 및 스케일링)을 사용하는 대신에, Hatamizadeh et al. [56]은 샘플링 시간 단계에 걸쳐 동적 잡음제거 거동을 모델링하기 위해 시간-의존적 자기-집중(TMSA) 모듈을 사용하는 확산 비전 트랜스포머(DiffiT)를 도입한다. 또한 DiffiT는 픽셀 공간과 잠재 공간에서 각각 효율적인 디노이징을 위해 두 개의 하이브리드 계층 구조를 사용하며, 다양한 생성 작업에 걸쳐 새로운 최첨단 결과를 달성한다. 전반적으로, 이러한 연구는 이미지 잠재 확산을 위해 비전 트랜스포머를 사용하는 유망한 결과를 보여주며, 다른 양식에 대한 향후 연구의 길을 열어준다.\n' +
      '\n' +
      '**비디오 확산 트랜스포머.** 텍스트-이미지(Text-to-Image; T2I) 확산 모델 기반 작업을 기반으로 최근 연구는 텍스트-비디오(Text-to-Video; T2V) 생성 작업에 대한 확산 트랜스포머의 잠재력을 실현하는 데 초점을 맞추고 있다. 비디오의 시간적 특성으로 인해 비디오 도메인에서 DiT를 적용하기 위한 주요 과제는:_i) 비디오를 효율적인 잡음 제거를 위해 잠재 공간으로 공간적, 시간적으로 압축하는 방법; ii) 압축된 잠재성을 패치로 변환하여 트랜스포머에 공급하는 방법;_ 및 _iii) 장거리 시간적, 공간적 의존성을 처리하고 콘텐츠 일관성을 보장하는 방법이다. 첫 번째 도전은 섹션 3.2.3을 참조하십시오. 이 섹션에서는 공간적 및 시간적으로 압축된 잠재 공간에서 작동하도록 설계된 변압기 기반 잡음 제거 네트워크 아키텍처에 대한 논의를 집중한다. 우리는 OpenAI Sora 기술 보고서의 참조 목록에 설명된 두 가지 중요한 작업(Imagen Video[29] 및 Video LDM[36])에 대해 자세히 검토한다.\n' +
      '\n' +
      '구글 리서치가 개발한 텍스트-비디오 생성 시스템인 이마겐 비디오[29]는 텍스트 조건 비디오 생성, 공간 초해상도, 시간 초해상도를 수행하는 7개의 하위 모델로 구성된 확산 모델의 캐스케이드를 활용하여 텍스트 프롬프트를 고화질 비디오로 변환한다. 도 13에 도시된 바와 같이, 먼저, 동결된 T5 텍스트 인코더는 입력 텍스트 프롬프트로부터 문맥 임베딩을 생성한다. 이러한 임베딩은 생성된 비디오를 텍스트 프롬프트와 정렬하는 데 중요하며 기본 모델 외에도 캐스케이드의 모든 모델에 주입된다. 이어서, 상기 매립을 급전하여,\n' +
      '\n' +
      '도 12:MDT(Masked Diffusion Transformer)의 전체 프레임워크. 실선/점선은 각 시간 단계에 대한 훈련/추론 과정을 나타낸다. 마스킹과 사이드 인터폴레이터는 훈련 중에만 사용되며 추론 중에 제거된다.\n' +
      '\n' +
      '저해상도 비디오 생성을 위한 기본 모델은 해상도를 높이기 위해 캐스케이드 확산 모델에 의해 정제된다. 기본 비디오 및 초해상도 모델은 시공간 분리 가능한 방식으로 3D U-Net 아키텍처를 사용한다. 이 아키텍처는 프레임 간 의존성을 효율적으로 캡처하기 위해 공간적 대응물과 시간적 주의 및 컨볼루션 레이어를 엮는다. 모델 간 병렬 학습을 용이하게 하기 위해 수치 안정성 및 컨디셔닝 증강을 위한 v-예측 매개변수화를 사용한다. 프로세스는 이미지 및 비디오 둘 다에 대한 공동 트레이닝, 각 이미지를 더 큰 데이터 세트를 레버리지하기 위한 프레임으로 취급하고, 신속한 충실도를 향상시키기 위해 분류기 없는 안내(57)를 사용하는 것을 포함한다. 프로그레시브 증류[58]는 샘플링 공정을 간소화하기 위해 적용되어 지각 품질을 유지하면서 계산 부하를 크게 감소시킨다. 이러한 방법과 기술을 결합하면 다양한 비디오, 텍스트 애니메이션 및 콘텐츠를 다양한 예술적 스타일로 생산하는 능력으로 입증된 바와 같이, Imagen Video는 높은 충실도뿐만 아니라 현저한 제어성을 갖는 비디오를 생성할 수 있다.\n' +
      '\n' +
      'Blattmann et al. [36]은 2D Latent Diffusion Model을 Video LDM(Video Latent Diffusion Model)으로 바꾸는 것을 제안한다. 이들은 기존의 공간 계층 중 일부 사후 시간 계층을 U-Net 백본과 개별 프레임을 정렬하도록 학습하는 VAE 디코더 모두에 추가하여 이를 달성한다. 이들 시간적 계층들은 인코딩된 비디오 데이터에 대해 트레이닝되는 반면, 공간적 계층들은 고정된 채로 유지되어, 모델이 사전 트레이닝을 위해 큰 이미지 데이터세트들을 레버리지할 수 있게 한다. LDM의 디코더는 픽셀 공간의 시간적 일관성을 위해 미세 조정되고 향상된 공간 해상도를 위해 확산 모델 업샘플러를 시간적으로 정렬한다. 를 생성하는 단계\n' +
      '\n' +
      '도 14: Video LDM의 전체 프레임워크. 소스: 비디오 LDM[36].\n' +
      '\n' +
      '도 13: Imagen Video의 전체 프레임워크. 출처: Imagen Video[29].\n' +
      '\n' +
      '매우 긴 비디오들, 모델들은 다수의 컨텍스트 프레임들이 주어진 미래 프레임을 예측하도록 트레이닝되어, 샘플링 동안 분류기 없는 안내를 허용한다. 높은 시간 해상도를 달성하기 위해, 비디오 합성 프로세스는 키 프레임 생성 및 이들 키 프레임들 사이의 보간으로 분할된다. 캐스케이드 LDM에 이어, DM은 비디오 LDM 출력을 4배 더 확장하는데 사용되어, 시간 일관성을 유지하면서 높은 공간 해상도를 보장한다. 이 접근법은 계산적으로 효율적인 방식으로 전역적으로 일관성 있는 긴 비디오의 생성을 가능하게 한다. 또한, 시간 정렬 계층만을 학습하여 사전 학습된 이미지 LDM(예: Stable Diffusion)을 텍스트-비디오 모델로 변환하여 최대 해상도(1280\\times 2048\\)로 비디오 합성을 수행할 수 있음을 보인다.\n' +
      '\n' +
      '#### 3.3.1 Discussion\n' +
      '\n' +
      '**공간 및 시간 업샘플링을 위한 캐스케이드 확산 모델.** 소라는 고해상도 비디오를 생성할 수 있다. 기존 연구와 역공학을 검토하여 소라가 기저 모델과 많은 시공간 정제기 모델로 구성된 캐스케이드 확산 모델 아키텍처[59]를 활용한다고 추측한다. 어텐션 모듈은 높은 계산 비용과 고해상도 사례에서 어텐션 머신 사용의 제한된 성능 이득을 고려할 때 기반 확산 모델 및 저해상도 확산 모델에서 많이 사용되지 않을 것이다. 공간적, 시간적 장면의 일관성은 비디오/장면 생성을 위한 공간적 일관성보다 시간적 일관성이 더 중요하다는 것을 이전 연구에서 보여주듯이, 소라는 해상도가 낮은 긴 비디오(시간적 일관성을 위한)를 사용하여 효율적인 훈련 전략을 활용할 가능성이 높다. 또한 소라는 잠재 잠재량(x\\)이나 잡음(\\epsilon\\)을 예측하는 다른 변종에 비해 우수한 성능을 고려할 때 \\(v\\)-모수화 확산 모델[58]을 사용할 가능성이 높다.\n' +
      '\n' +
      '** latent encoder.** 훈련 효율성은 기존의 대부분의 작업들이 초기화된 모델 체크포인트로 사전 훈련된 2D 확산 모델인 Stable Diffusion [60, 61]의 사전 훈련된 VAE 인코더를 활용한다. 그러나, 인코더는 시간적 압축 능력이 부족하다. 비록 일부 연구들은 시간 정보를 처리하기 위해 디코더를 미세 조정하는 것만을 제안하지만, 압축된 잠재 공간에서 비디오 시간 데이터를 처리하는 디코더의 성능은 최적이 아니다. 기술보고서에 따르면, 역공학은 기존의 사전 훈련된 VAE 인코더를 사용하는 대신, 소라가 비디오 데이터에 대해 처음부터 훈련된 시공간 VAE 인코더를 사용할 가능성이 있으며, 이는 비디오 방향 압축 잠재 공간을 가진 기존 인코더보다 더 나은 성능을 보인다.\n' +
      '\n' +
      '### 언어 지도 후행\n' +
      '\n' +
      '사용자는 주로 텍스트 프롬프트(62, 63)로 알려진 자연 언어 지침을 통해 생성 AI 모델에 참여한다. 모델 명령어 튜닝은 AI 모델의 자연어 질의에 대한 인간의 반응과 유사한 출력을 생성하는 것을 목표로 한다. 본 논문에서는 DALL-E 3과 같은 대형 언어 모델(LLM)과 텍스트 이미지 모델(DALL-E 3)에 대한 명령어 추종 기법에 대한 검토를 통해 텍스트 명령어의 추종 능력을 향상시키기 위해, Sora는 DALL-E 3과 유사한 방법을 사용한다.\n' +
      '\n' +
      '###### 3.4.1 대형 언어 모델\n' +
      '\n' +
      'LLM이 지침을 따르는 능력은 광범위하게 탐구되었다[64, 65, 66]. 이 능력을 통해 LLM은 예제 없이 보이지 않는 작업을 설명하는 지침에 대해 읽고 이해하고 적절하게 응답할 수 있다. 명령어 튜닝으로 알려진 명령어[64, 66]로 포맷된 태스크들의 혼합물 상에서 LLM들을 미세 조정함으로써 프롬프트 팔로우 능력이 획득되고 향상된다. Wei et al. [65]는 명령어-튜닝된LLM이 보이지 않는 태스크에서 튜닝되지 않은 것을 상당히 능가한다는 것을 보여주었다. 교수-추종 능력은 LLM을 범용 과제 해결자로 전환하여 AI 개발 역사의 패러다임 전환을 나타낸다.\n' +
      '\n' +
      '#### 3.4.2 Text-to-Image\n' +
      '\n' +
      'DALL-E 3에서의 다음의 명령은 모델이 트레이닝된 텍스트-이미지 쌍들의 품질이 결과 텍스트-이미지 모델의 성능을 결정한다는 가설과 함께 캡션 개선 방법에 의해 다루어진다[67]. 데이터의 열악한 품질, 특히 많은 양의 시각적 정보를 생략하는 시끄러운 데이터 및 짧은 캡션의 보급은 키워드 및 어순을 소홀히 하고, 사용자의 의도를 오해하는 등의 많은 문제로 이어진다[21]. 캡션 개선 접근법은 기존의 이미지를 상세하고 서술적인 캡션으로 재캡션함으로써 이러한 문제를 해결한다. 이 접근법은 먼저 비전 언어 모델인 이미지 캡셔너를 훈련하여 정확하고 서술적인 이미지 캡션을 생성한다. 캡셔너에 의한 결과적인 설명 이미지 캡션은 텍스트-이미지 모델을 미세 조정하는 데 사용된다. 구체적으로, DALL-E 3은 CLIP[26] 아키텍처 및 언어 모델 목적을 갖는 이미지 캡셔너를 공동으로 훈련시키기 위해 대비 캡셔너들(CoCa)[68]을 따른다. 이 이미지 캡셔너는 언어 정보를 추출하기 위한 유니모달 텍스트 인코더와 멀티모달 텍스트 디코더를 통합한다. 먼저, 유니모달 이미지와 텍스트 임베딩 사이의 대조적 손실을 사용하고, 이어서 멀티모달 디코더의 출력에 대한 캡션 손실을 사용한다. 결과 이미지 캡셔너는 주요 객체, 주변 환경, 배경, 텍스트, 스타일 및 색상을 포함하는 이미지에 대한 매우 상세한 설명에 대해 추가로 미세 조정된다. 이 단계를 통해, 이미지 캡셔너는 이미지에 대한 상세한 설명 캡션을 생성할 수 있다. 텍스트-대-이미지 모델에 대한 트레이닝 데이터세트는 모델이 사용자 입력들을 캡처하는 것을 보장하기 위해 이미지 캡셔너에 의해 생성된 재-캡처된 데이터세트와 지상-진실 인간-작성된 데이터의 혼합물이다. 이 이미지 캡션 개선 방법은 학습 데이터로부터 실제 사용자 프롬프트와 설명 이미지 설명 사이의 불일치라는 잠재적인 문제를 도입한다. DALL-E 3은 _upampling_에 의해 이를 해결하며, 여기서 LLM은 짧은 사용자 프롬프트를 상세하고 긴 명령어로 재작성하는 데 사용된다. 이것은 추론 시간에 수신된 모델의 텍스트 입력이 모델 트레이닝에서의 입력과 일치함을 보장한다.\n' +
      '\n' +
      '#### 3.4.3 Text-to-Video\n' +
      '\n' +
      '소라는 다음과 같은 수업 능력을 향상시키기 위해 유사한 자막 개선 접근법을 채택한다. 이 방법은 비디오에 대한 상세한 설명을 제작할 수 있는 비디오 캡셔너를 먼저 트레이닝함으로써 달성된다. 그런 다음, 이 비디오 캡셔너를 훈련 데이터 내의 모든 비디오에 적용하여 고품질(비디오, 설명 캡션) 쌍을 생성하며, 이는 소라의 지시 추종 능력을 향상시키기 위해 미세 조정하는데 사용된다.\n' +
      '\n' +
      '소라의 기술 보고서 [3]은 비디오 캡셔너가 어떻게 훈련되는지에 대한 세부사항을 밝히지 않는다. 비디오 캡셔너가 비디오-텍스트 모델이라는 점을 감안할 때, 그것을 구축하기 위한 많은 접근법이 있다. 간단한 접근법은 비디오의 여러 프레임을 취하고 각 프레임을 비디오코카[69]로 알려진 이미지 인코더[68]에 공급함으로써 비디오 캡셔닝을 위해 CoCa 아키텍처를 활용하는 것이다. VideoCoCa는 CoCa를 기반으로 이미지 인코더에서 미리 훈련된 가중치를 재사용하고 샘플링된 비디오 프레임에 독립적으로 적용한다. 결과적인 프레임 토큰 임베딩들은 평탄화되고 비디오 표현들의 긴 시퀀스로 연결된다. 이러한 평탄화된 프레임 토큰은 생성 풀러 및 대비 풀러에 의해 처리되며, 이는 대비 손실 및 캡션 손실과 공동으로 훈련된다. 비디오 캡셔너를 구축하는 다른 대안으로는 mPLUG-2[70], GIT[71], FrozenBiLM[72] 등이 있다. 마지막으로, 사용자 프롬프트가 훈련 데이터에서 이러한 설명 캡션의 형식과 정렬되도록 하기 위해, 소라는 추가 프롬프트 확장 단계를 수행하며, 여기서 GPT-4V는 사용자 입력을 상세한 설명 프롬프트로 확장하는 데 사용된다.\n' +
      '\n' +
      '#### 3.4.4 Discussion\n' +
      '\n' +
      '소라가 사용자 의도에 충실한 복잡한 장면을 가진 1분 길이의 비디오를 생성하기 위해서는 지시-추종 능력이 중요하다. 소라의 기술 보고서 [3]에 따르면, 이 능력은 길고 상세한 캡션을 생성할 수 있는 캡셔너를 개발함으로써 얻어지며, 이 캡셔너를 사용하여 모델을 훈련시킨다. 그러나, 이러한 캡셔너를 훈련하기 위한 데이터를 수집하는 과정은 비디오에 대한 상세한 설명이 필요할 수 있기 때문에 알 수 없고 노동 집약적일 가능성이 높다. 또한, 서술형 비디오 캡셔너는 비디오의 중요한 세부 사항을 환각할 수 있다. 우리는 비디오 캡셔너를 개선하는 방법이 추가 조사를 보장하고 텍스트-이미지 모델의 지시-추종 능력을 향상시키는 데 중요하다고 믿는다.\n' +
      '\n' +
      '### Prompt Engineering\n' +
      '\n' +
      '신속 엔지니어링은 특히 생성 모델의 맥락에서 AI 시스템에 주어진 입력을 설계 및 정제하여 특정 또는 최적화된 출력[73, 74, 75]을 달성하는 프로세스를 지칭한다. 신속한 공학의 예술과 과학은 가능한 가장 정확하고 관련되며 일관된 응답을 생성하도록 모델을 안내하는 방식으로 이러한 입력을 만드는 것을 포함한다.\n' +
      '\n' +
      '###### 3.5.1 문자 프롬프트\n' +
      '\n' +
      '텍스트 프롬프트 엔지니어링은 사용자 사양을 정확하게 충족시키면서 시각적으로 눈에 띄는 비디오를 생성하기 위해 텍스트 대 비디오 모델(예: 소라[3])을 지시하는 데 필수적이다. 여기에는 인간의 창의성과 AI의 실행 능력 사이의 격차를 효과적으로 메우기 위해 모델을 지시하기 위해 상세한 설명을 만드는 것이 포함된다[76]. 소라에 대한 프롬프트는 다양한 시나리오를 포함합니다. 최근 작품(예: VoP[77], Make-A-Video[28], Tune-A-Video[78])은 엔지니어링이 복잡한 명령어를 해독하고 응집력 있고 생동감 있으며 고품질 비디오 서사로 렌더링하는 모델의 자연어 이해 능력을 얼마나 신속하게 활용하는지를 보여주었다. 그림 15에서 볼 수 있듯이 "네온 조명이 켜진 도쿄 거리를 걷는 세련된 여성..."은 소라가 예상 시각과 잘 일치하는 비디오를 생성할 수 있도록 세심하게 조작된 텍스트 프롬프트이다. 신속한 공학의 품질은 단어의 신중한 선택, 제공된 세부 정보의 특수성 및 모델의 출력에 미치는 영향에 대한 이해에 달려 있다. 예를 들어, 도 15의 프롬프트는 액션들, 설정들, 캐릭터 등장들, 심지어 장면의 원하는 무드 및 분위기를 상세하게 명시한다.\n' +
      '\n' +
      '###### 3.5.2 이미지 프롬프트\n' +
      '\n' +
      '이미지 프롬프트는 생성될 비디오의 콘텐츠 및 캐릭터, 설정, 및 기분과 같은 다른 요소에 대한 시각적 앵커 역할을 한다[79]. 또한, 텍스트 프롬프트는 예를 들어, 이동, 상호작용, 및 서사 진행의 층들을 추가하는 것에 의해 모델에게 이러한 요소들을 애니메이션하도록 지시할 수 있다.\n' +
      '\n' +
      '그림 15: 창의 과정을 묘사하기 위해 색상 코딩을 사용하는 텍스트-비디오 생성을 위한 프롬프트 엔지니어링에 대한 사례 연구. 파란색으로 강조된 텍스트는 세련된 여성의 묘사와 같이 소라가 생성한 요소를 묘사한다. 대조적으로, 노란색의 텍스트는 행동, 설정 및 캐릭터 출현에 대한 모델의 해석을 강조하여 세심하게 조작된 프롬프트가 얼마나 생생하고 역동적인 비디오 내러티브로 변환되는지 보여준다.\n' +
      '\n' +
      '정적 이미지 투 라이프[27, 80, 81]. 이미지 프롬프트를 사용하면 소라가 시각적 정보와 텍스트 정보를 모두 활용하여 정적 이미지를 동적 내러티브 기반 비디오로 변환할 수 있다. 그림 16에서는 "베레모와 터틀넥을 입은 시바 이누", "독특한 괴물 가족", "소라라는 단어를 형성하는 구름", "역사적인 홀 안에서 해일을 항해하는 서퍼들"의 AI 생성 영상을 보여준다. 이러한 예는 DALL-E 생성 이미지로 소라를 프롬프트함으로써 달성될 수 있는 것을 입증한다.\n' +
      '\n' +
      '###### 3.5.3 동영상 프롬프트\n' +
      '\n' +
      '비디오 프롬프트는 또한 [82, 83]에서 입증된 바와 같이 비디오 생성에 사용될 수 있다. 최근 작업(예: Moonshot[84] 및 Fast-Vid2Vid[85])은 좋은 비디오 프롬프트가 구체적이고 유연할 필요가 있음을 보여준다. 이는 모델이 특정 객체 및 시각적 주제의 묘사와 같이 특정 목표에 대한 명확한 방향을 수신하고 최종 산출물의 상상적 변화를 허용한다는 것을 보장한다. 예를 들어, 비디오 확장 작업에서, 프롬프트는 방향(시간상 전진 또는 후진) 및 확장문의 컨텍스트 또는 테마를 지정할 수 있다. 그림 17(a)에서 비디오 프롬프트는 소라에게 원래 시작점까지 이어지는 이벤트를 탐색하기 위해 비디오를 시간적으로 뒤로 연장하도록 지시한다. 비디오 프롬프트를 통해 비디오 대 비디오 편집을 수행할 때, 도 17(b)에 도시된 바와 같이, 모델은 비디오의 스타일을 변경하거나, 설정 또는 분위기를 변경하거나, 조명 또는 무드와 같은 미묘한 양태를 변경하는 것과 같은 원하는 변환을 명확하게 이해할 필요가 있다. 도 17(c)에서, 프롬프트는 소라에게 비디오에 걸쳐 상이한 장면들에서 객체들 사이의 부드러운 전환을 보장하면서 비디오들을 연결하도록 지시한다.\n' +
      '\n' +
      '#### 3.5.4 Discussion\n' +
      '\n' +
      '신속한 엔지니어링을 통해 사용자는 AI 모델이 의도에 맞는 콘텐츠를 생성하도록 안내할 수 있습니다. 예로서, 텍스트, 이미지 및 비디오 프롬프트의 결합된 사용은 소라가 시각적으로 강렬할 뿐만 아니라 사용자의 기대 및 의도에 잘 부합하는 콘텐츠를 생성할 수 있게 한다. 프롬프트 엔지니어링에 대한 기존 연구는 LLM 및 LVM에 대한 텍스트 및 이미지 프롬프트에 초점을 맞추었지만[86, 87, 88], 우리는 비디오 생성 모델에 대한 비디오 프롬프트에 대한 관심이 증가할 것으로 예상한다.\n' +
      '\n' +
      '### Trustworthiness\n' +
      '\n' +
      'ChatGPT[89], GPT4-V[90], 소라[3]와 같은 정교한 모델의 급속한 발전으로 이러한 모델의 기능은 현저한 향상을 보였다. 이러한 발전은 작업 효율성을 개선하고 기술 진보를 추진하는 데 상당한 기여를 했다. 그러나 이러한 발전은 가짜 뉴스의 생성[91, 92], 사생활 침해[93], 윤리적 딜레마[94, 95] 등 이러한 기술의 오용 가능성에 대한 우려도 제기한다. 따라서, 문제는\n' +
      '\n' +
      '도 16: 이 예는 소라의 텍스트-비디오 모델을 생성으로 안내하기 위한 이미지 프롬프트를 예시한다. 붉은 상자는 거대한 해일에 직면한 화려한 홀에서 다양한 디자인의 괴물, 구름 형성 철자 "SORA" 및 서퍼와 같은 각 장면의 주요 요소를 시각적으로 고정한다.\n' +
      '\n' +
      '대형 모델의 신뢰성은 학술 및 산업 분야 모두에서 광범위한 관심을 받아 현대 연구 논의의 초점으로 부상했다.\n' +
      '\n' +
      '###### 3.6.1 안전 우려 사항\n' +
      '\n' +
      '한 가지 주요 초점은 모델의 안전성, 특히 오용 및 소위 "탈옥" 공격에 대한 복원력이며, 여기서 사용자는 금지되거나 유해한 콘텐츠를 생성하기 위해 취약성을 이용하려고 시도한다[96, 97, 98, 99, 100, 101, 102, 103, 104, 105]. 예를 들어, 시스템 바이패스를 가능하게 하기 위해 그래디언트 기법에 기반한 새롭고 해석 가능한 적대적 공격 방법인 AutoDAN[103]이 도입된다. 최근 연구에서 연구자들은 LLM이 탈옥 공격에 저항하기 위해 고군분투하는 두 가지 이유를 탐구한다: 경쟁 목표와 일치하지 않는 일반화[106]. 텍스트 공격 외에도 시각적 탈옥은 멀티모달 모델(예: GPT-4V[90], 소라[3])의 안전성을 위협한다. 최근의 연구[107]는 추가적인 시각적 입력의 연속적이고 고차원적인 특성으로 인해 확장된 공격 표면을 나타내는 적대적 공격에 대해 약하기 때문에 대형 멀티모달 모델이 더 취약하다는 것을 발견했다.\n' +
      '\n' +
      '###### 3.6.2 기타 착취\n' +
      '\n' +
      '큰 규모의 학습 데이터셋과 대규모 기반 모델(예: ChatGPT[89] 및 Sora[3])의 학습 방법론으로 인해 환각과 같은 관련 문제가 널리 논의[108]됨에 따라 이러한 모델의 진실성이 향상될 필요가 있다. 이러한 맥락에서 환각은 설득력 있게 보일 수 있지만 근거가 없거나 거짓인 응답을 생성하는 모델의 경향을 의미한다[96]. 이 현상은 모델 산출물의 신뢰성과 신뢰성에 대한 중요한 질문을 제기하여 문제를 평가하고 해결하기 위한 포괄적인 접근 방식을 필요로 한다. 다양한 각도에서 환각의 문제를 해부하는 데 많은 연구가 집중되어 왔다. 여기에는 다양한 모델 및 시나리오[109, 96, 110, 111]에 걸쳐 환각의 범위와 특성을 평가하는 것을 목표로 하는 노력이 포함된다. 이러한 평가는 환각이 어떻게 그리고 왜 발생하는지에 대한 귀중한 통찰력을 제공하여 발병률을 완화하기 위한 전략을 개발할 수 있는 기반을 마련한다. 동시에, 중요한 연구는 이러한 큰 모델들[112, 113, 114]에서 환각을 줄이기 위한 방법들을 고안하고 구현하는 데 초점을 맞추고 있다.\n' +
      '\n' +
      '신뢰성의 또 다른 중요한 측면은 공정성과 편견이다. 모델 개발의 중요한 중요성\n' +
      '\n' +
      '도 17: 이러한 예들은 소라 모델들에 대한 비디오 프롬프트 기법들을 예시한다: (a) 비디오 확장, 여기서 모델은 원래 장면 뒤로 시퀀스를 외삽하고, (b) 세팅과 같은 특정 요소들이 텍스트 프롬프트에 따라 변환되는 비디오 편집, 및 (c) 두 개의 별개의 비디오 프롬프트들이 일관성 있는 내러티브를 생성하기 위해 매끄럽게 블렌딩되는 비디오 연결. 각각의 프로세스는 시각적 앵커에 의해 안내되고, 적색 박스로 표시되며, 생성된 비디오 콘텐츠에서 연속성 및 정밀도를 보장한다.\n' +
      '\n' +
      '사회적 편견을 영구화하거나 악화시키지 않는 것이 가장 중요한 관심사이다. 이러한 우선 순위는 이러한 모델 내에서 인코딩된 편향이 기존의 사회적 불평등을 강화하여 차별적인 결과를 초래할 수 있다는 인식에서 비롯된다. Gallegos et al. [115], Zhang et al. [116], Liang et al. [117], 및 Friedrich et al. [118]의 작업에 의해 입증된 바와 같이, 이 분야의 연구는 이러한 고유한 편향의 세심한 식별 및 정정에 전념한다. 목표는 인종, 성별 또는 기타 민감한 속성에 대한 편견 없이 모든 개인을 공평하게 대우하면서 공정하게 작동하는 모델을 배양하는 것이다. 이것은 데이터 세트에서 바이어스를 검출하고 완화할 뿐만 아니라 그러한 바이어스의 전파에 능동적으로 대응할 수 있는 알고리즘을 설계하는 것을 포함한다[119, 120].\n' +
      '\n' +
      '개인 정보 보호는 이러한 모델이 배치될 때 또 다른 기본 기둥으로 등장한다. 데이터 프라이버시 우려가 고조되는 시대에 사용자 데이터 보호에 대한 강조는 이보다 더 중요한 적이 없었다. 개인 데이터가 어떻게 처리되는지에 대한 대중의 인식과 우려는 대형 모델에 대한 보다 엄격한 평가를 촉발했다. 이러한 평가는 사용자 데이터를 보호하는 모델의 능력에 중점을 두어 개인 정보가 기밀로 유지되고 부주의하게 공개되지 않도록 한다. Mireshghallah et al. [121], Plant et al. [122], 및 Li et al. [123]에 의한 연구는 프라이버시를 보호하는 방법론 및 기술을 발전시키기 위한 노력을 예시한다.\n' +
      '\n' +
      '#### 3.6.3 Alignment\n' +
      '\n' +
      '이러한 문제를 해결하는 데 있어 대형 모델의 신뢰성을 보장하는 것은 연구자의 주요 관심사 중 하나가 되었다[124, 96, 99, 125]. 가장 중요한 기술 중 모델 정렬[125, 126]은 모델의 행동과 산출물이 인간 설계자의 의도 및 윤리적 기준과 일치하도록 보장하는 과정과 목표를 의미한다. 이것은 기술의 발전, 그것의 도덕적 책임, 그리고 사회적 가치에 관한 것이다. LLM 영역에서는 모델 정렬을 위해 RRLHF(Reinforcement Learning with Human Feedback) [127, 128]이 널리 적용되고 있다. 이 방법은 강화 학습(RL)과 직접적인 인간 피드백을 결합하여 모델이 작업을 이해하고 수행하는 데 있어 인간의 기대 및 표준에 더 잘 정렬할 수 있도록 한다.\n' +
      '\n' +
      '#### 3.6.4 Discussion\n' +
      '\n' +
      '소라(특히 기술 보고서)로부터 향후 작업에 대한 유익한 지침을 잠재적으로 제공하는 몇 가지 통찰력 있는 결과를 요약한다.\n' +
      '\n' +
      '(1)_통합 모델 및 외부 보안_: 모델들이 더욱 강력해짐에 따라, 특히 콘텐츠를 생성함에 있어서, 유해한 콘텐츠(혐오 발언[129] 및 거짓 정보[92, 91])를 생성하기 위해 오용되지 않도록 하는 것이 심각한 과제가 되었다. 모델 자체를 정렬하는 것 외에도 외부 보안 보호도 똑같이 중요하다. 여기에는 콘텐츠 필터링 및 검토 메커니즘, 사용 권한 및 액세스 제어, 데이터 개인 정보 보호, 투명성 및 설명 가능성 향상 등이 포함됩니다. 예를 들어, OpenAI는 이제 검출 분류기를 사용하여 주어진 비디오가 소라에 의해 생성되는지 여부를 말한다[130]. 더욱이, 잠재적으로 유해한 텍스트 입력을 검출하기 위해 텍스트 분류기가 배치된다[130].\n' +
      '\n' +
      '(2)_멀티모달 모델의 보안 과제_: 소라와 같은 텍스트-비디오 모델과 같은 멀티모달 모델은 다양한 유형의 콘텐츠(텍스트, 이미지, 비디오 등)를 이해하고 생성하는 능력으로 인해 보안에 추가적인 복잡성을 가져온다. 멀티모달 모델은 다양한 형태로 콘텐츠를 생산할 수 있어 오용 및 저작권 문제의 방식과 범위를 증가시킬 수 있다. 멀티모달 모델에 의해 생성된 콘텐츠가 더 복잡하고 다양하기 때문에, 전통적인 방식의 콘텐츠 검증 및 진정성은 더 이상 효과적이지 않을 수 있다. 이를 위해서는 이러한 모델에 의해 생성된 유해 콘텐츠를 식별하고 필터링할 수 있는 새로운 기술과 방법의 개발이 요구되어 규제 및 관리의 어려움이 가중된다.\n' +
      '\n' +
      '(3)_학제 간 협업의 필요성_: 모델의 안전성을 보장하는 것은 기술적인 문제일 뿐만 아니라 학제 간 협력이 필요하다. 이러한 과제를 해결하기 위해 법률[131]과 심리[132] 등 다양한 분야의 전문가들이 함께 협력하여 적절한 규범(예: 안전은 무엇이고 안전하지 않은 것은 무엇인가), 정책 및 기술 솔루션을 개발할 필요가 있다. 학제 간 협업의 필요성은 이러한 문제 해결의 복잡성을 크게 증가시킨다.\n' +
      '\n' +
      '## 4 Applications\n' +
      '\n' +
      '소라가 예시하는 영상 확산 모델이 최전방 기술로 부상하면서 다양한 연구 분야와 산업 전반에 걸친 채택이 빠르게 가속화되고 있다. 이 기술의 의미는 단순한 비디오 생성을 훨씬 넘어, 자동화된 콘텐츠 생성에서 복잡한 의사 결정 프로세스에 이르는 작업에 대한 변환 가능성을 제공한다. 이 섹션에서는 소라가 능력을 입증했을 뿐만 아니라 복잡한 문제를 해결하기 위한 접근 방식에 혁명을 일으켰던 주요 영역을 강조하면서 비디오 확산 모델의 현재 적용에 대한 포괄적인 조사를 조사한다. 우리는 실제 배치 시나리오에 대한 광범위한 관점을 제공하는 것을 목표로 한다(도 18 참조).\n' +
      '\n' +
      '### Movie\n' +
      '\n' +
      '전통적으로, 영화 대작을 만드는 것은 힘들고 비용이 많이 드는 과정이었고, 종종 수십 년의 노력, 최첨단 장비, 그리고 상당한 재정적 투자가 필요했다. 그러나 첨단 영상 생성 기술의 등장은 단순한 텍스트 입력으로부터 영화를 자율적으로 제작하는 꿈이 현실화되고 있는 영화 제작의 새로운 시대를 예고하고 있다. 연구자들은 비디오 생성 모델을 영화 제작으로 확장함으로써 영화 생성의 영역에 도전해 왔다. 영화 공장[133]은 확산 모델을 적용하여 챗-GPT[89]가 제작한 정교한 스크립트에서 영화 스타일의 비디오를 생성하는데 상당한 도약을 나타낸다. 후속 조치에서 MobileVidFactory[134]는 사용자가 제공하는 간단한 텍스트만으로 자동으로 수직 모바일 동영상을 생성할 수 있다. Vlogger[135]는 사용자가 1분 길이의 vlog를 구성하는 것을 가능하게 한다. 매혹적인 영화 콘텐츠를 쉽게 생성할 수 있는 소라의 능력으로 대표되는 이러한 발전은 영화 제작의 민주화에 중추적인 순간을 기록한다. 누구나 영화제작자가 될 수 있는 미래를 엿볼 수 있어 영화산업의 진입장벽을 대폭 낮추고, 전통적인 스토리텔링과 AI 중심의 창의성이 어우러진 영화제작에 참신한 차원을 도입한다. 이러한 기술의 의미는 단순화를 넘어 확장된다. 그들은 영화 제작의 풍경을 재구성하여 진화하는 시청자 선호도와 유통 채널에 직면하여 더 접근 가능하고 다재다능하게 만들 것을 약속한다.\n' +
      '\n' +
      '도 18: 소라의 응용예.\n' +
      '\n' +
      '### Education\n' +
      '\n' +
      '교육 콘텐츠의 풍경은 오랫동안 정적 자원에 의해 지배되어 왔으며, 그 가치에도 불구하고 오늘날 학생들의 다양한 요구와 학습 스타일에 부응하지 못하는 경우가 많다. 비디오 확산 모델은 교육 혁명의 최전선에 서서 학습자의 참여와 이해를 크게 향상시키는 방식으로 교육 자료를 맞춤화하고 애니메이션화할 수 있는 전례 없는 기회를 제공한다. 이러한 첨단 기술을 통해 교육자는 텍스트 설명 또는 커리큘럼 개요를 개별 학습자의 특정 스타일 및 관심사에 맞춘 역동적이고 매력적인 비디오 콘텐츠로 변환할 수 있다[136, 137, 138, 139]. 더욱이, 이미지-대-비디오 편집 기술[140, 141, 142]은 정적 교육 자산을 상호작용 비디오로 변환하기 위한 혁신적인 방법을 제시함으로써, 다양한 학습 선호도를 지원하고 잠재적으로 학생 참여를 증가시킨다. 이러한 모델을 교육 콘텐츠 생성에 통합함으로써 교육자는 무수히 많은 주제에 대한 비디오를 제작할 수 있어 복잡한 개념을 보다 쉽게 접근할 수 있고 학생들에게 매혹시킬 수 있다. 교육 영역을 혁신하는 데 소라를 사용하는 것은 이러한 기술의 변형 가능성을 예시한다. 개인화되고 역동적인 교육 콘텐츠로의 이러한 변화는 교육의 새로운 시대를 예고한다.\n' +
      '\n' +
      '### Gaming\n' +
      '\n' +
      '게임 산업은 현실주의와 몰입의 경계를 푸는 방법을 끊임없이 모색하지만, 전통적인 게임 개발은 종종 사전 렌더링된 환경과 스크립팅된 이벤트의 한계와 씨름한다. 확산 모델에 의한 동적, 고충실도 비디오 콘텐츠 및 사실적인 사운드의 생성은 실시간으로 효과를 발휘하며, 기존의 제약을 극복할 것을 약속하며, 개발자들에게 플레이어 액션 및 게임 이벤트에 유기적으로 반응하는 진화하는 게임 환경을 만드는 도구를 제공한다[143, 144]. 여기에는 변화하는 기상 조건을 생성하거나 풍경을 변형하거나 완전히 새로운 설정을 즉시 만들어 게임 세계를 보다 몰입적이고 반응적으로 만드는 것이 포함될 수 있다. 일부 방법들[145, 146]은 또한 비디오 입력들로부터 현실적인 임팩트 사운드들을 합성하여, 게임 오디오 경험들을 향상시킨다. 게임 도메인 내에서 소라의 통합으로 플레이어를 사로잡고 참여시키는 비교할 수 없는 몰입형 경험이 생성될 수 있다. 게임이 어떻게 개발되고, 플레이되고, 경험되는가는 혁신될 것이며, 스토리텔링, 상호작용, 몰입에 대한 새로운 가능성을 열어줄 것이다.\n' +
      '\n' +
      '### Healthcare\n' +
      '\n' +
      '생성 능력에도 불구하고 비디오 확산 모델은 복잡한 비디오 시퀀스를 이해하고 생성하는 데 탁월하여 조기 세포 사멸[147], 피부 병변 진행[148], 불규칙한 인간 움직임[149]과 같은 신체 내 동적 이상을 식별하는 데 특히 적합하며, 이는 조기 질병 감지 및 개입 전략에 중요하다. 또한 MedSegDiff-V2[150] 및 [151]과 같은 모델은 트랜스포머의 힘을 활용하여 전례 없는 정밀도로 의료 영상을 분할함으로써 임상의가 향상된 정확도로 다양한 영상 촬영 양식에 걸쳐 관심 영역을 정확하게 찾아낼 수 있다. 소라를 임상 실습에 통합하면 진단 과정을 정교화할 뿐만 아니라 환자 관리를 개인화할 수 있어 정밀한 의료 영상 분석에 기반한 맞춤형 치료 계획을 제공할 수 있다. 그러나 이러한 기술 통합은 강력한 데이터 개인 정보 보호 조치의 필요성과 의료의 윤리적 고려 사항을 다루는 것을 포함하여 자체적인 일련의 문제와 함께 제공됩니다.\n' +
      '\n' +
      '### Robotics\n' +
      '\n' +
      '비디오 확산 모델은 이제 로봇 공학에서 중요한 역할을 하며, 로봇이 향상된 인식[152, 153] 및 의사 결정[154, 155, 156]을 위해 복잡한 비디오 시퀀스를 생성하고 해석할 수 있는 새로운 시대를 보여준다. 이러한 모델은 로봇에서 새로운 기능을 잠금 해제하여 환경과 상호 작용하고 전례 없는 복잡성과 정밀도로 작업을 실행할 수 있습니다. 웹 규모의 확산 모델을 로봇 공학에 도입[152]하면 로봇 시각과 이해도를 향상시키기 위해 대규모 모델을 활용할 수 있는 가능성을 보여준다. 잠재 확산 모델은 언어 지시 비디오 예측[157]에 사용되며 로봇이 비디오 형식으로 액션의 결과를 예측하여 작업을 이해하고 실행할 수 있도록 한다. 또한, 로봇 공학 연구를 위한 시뮬레이션 환경에 대한 의존도는 매우 사실적인 비디오 시퀀스를 생성할 수 있는 비디오 확산 모델에 의해 혁신적으로 해결되었다[158, 159]. 이를 통해 로봇에 대한 다양한 훈련 시나리오를 생성할 수 있어 실제 데이터의 부족으로 인해 부과되는 한계를 완화할 수 있다. 우리는 소라와 같은 기술을 로봇 공학 분야에 통합하는 것이 획기적인 발전의 가능성을 가지고 있다고 믿습니다. 소라의 힘을 활용함으로써 로봇 공학의 미래는 로봇이 환경을 원활하게 탐색하고 상호 작용할 수 있는 전례 없는 발전을 위한 준비가 되어 있다.\n' +
      '\n' +
      '## 5 Discussion\n' +
      '\n' +
      '소라는 인간의 복잡한 지시를 정확하게 이해하고 실행하는 놀라운 재능을 보여준다. 이 모델은 정교하게 조작된 설정 내에서 모두 설정된 다양한 캐릭터로 자세한 비디오를 만드는 데 탁월합니다. 소라의 특히 인상적인 특징은 일관되고 매력적인 스토리텔링을 보장하면서 최대 1분 길이의 비디오를 제작하는 능력이다. 이것은 소라의 확장된 시퀀스가 명확한 서사 흐름을 나타내고 처음부터 끝까지 시각적 일관성을 유지하기 때문에 더 짧은 비디오 조각에 초점을 맞춘 이전 시도보다 상당한 개선을 나타낸다. 또한, 소라는 복잡한 움직임과 상호 작용을 포착하는 더 긴 비디오 시퀀스를 생성하여 짧은 클립과 기본 이미지만 처리할 수 있는 이전 모델의 제한을 넘어 전진함으로써 자신을 구별한다. 이러한 발전은 AI로 구동되는 창의적인 도구의 주요 진보를 의미하며, 사용자는 이전에 달성할 수 없었던 세부 사항과 정교함의 수준으로 작성된 이야기를 생생한 비디오로 변환할 수 있다.\n' +
      '\n' +
      '### Limitations\n' +
      '\n' +
      '**물리적 현실주의의 도전.** 소라는 시뮬레이션 플랫폼으로서 복잡한 시나리오를 정확하게 묘사하는 데 있어 그 효과를 저해하는 다양한 한계를 나타낸다. 가장 중요한 것은 복잡한 장면 내에서 물리적 원리를 일관성 없이 다루는 것으로, 원인과 결과의 특정 사례를 정확하게 복사하는 데 실패로 이어진다. 예를 들어, 쿠키의 일부를 소비하는 것은 해당 물린 자국을 초래하지 않을 수 있으며, 이는 시스템이 때때로 물리적 타당성에서 벗어나는 것을 나타낸다. 이 문제는 동작의 시뮬레이션으로 확장되며, 여기서 소라는 물체의 부자연스러운 변형이나 의자와 같은 단단한 구조의 잘못된 시뮬레이션과 같은 현실적인 물리적 모델링에 도전하는 움직임을 생성하여 비현실적인 물리적 상호 작용을 유도한다. 도전은 객체와 캐릭터 간의 복잡한 상호 작용을 시뮬레이션할 때 더욱 증가하며 때로는 유머러스한 쪽으로 기울어지는 결과를 생성한다.\n' +
      '\n' +
      '**공간 및 시간 복잡성.** 소라는 주어진 프롬프트 내에서 객체 및 문자의 배치 또는 배열과 관련된 지시를 때때로 오해하여 방향에 대한 혼란(예를 들어, 좌측에서 우측으로 혼동)을 초래한다. 또한, 특히 지정된 카메라 움직임 또는 시퀀스를 고수하는 것과 관련하여 이벤트의 시간적 정확도를 유지하는 데 어려움을 겪는다. 이는 장면들의 의도된 시간적 흐름으로부터 벗어나는 결과를 초래할 수 있다. 많은 캐릭터나 요소가 포함된 복잡한 시나리오에서 소라는 관련 없는 동물이나 사람을 삽입하는 경향이 있다. 이러한 추가는 계획된 서사나 시각적 레이아웃에서 벗어나 원래 구상된 장면의 구성과 분위기를 크게 변화시킬 수 있다. 이 문제는 특정 장면이나 내러티브를 정확하게 재현하는 모델의 능력에 영향을 미칠 뿐만 아니라 사용자의 기대와 생성된 출력의 일관성에 밀접하게 일치하는 콘텐츠를 생성하는 데 있어 신뢰성에 영향을 미친다.\n' +
      '\n' +
      '**인간-컴퓨터 상호작용(HCI)에서의 한계.** 소라는 비디오 생성 도메인에서 잠재력을 보여주면서도 HCI에서 상당한 한계에 직면한다. 이러한 제한들은 특히 생성된 콘텐츠에 대한 상세한 수정들 또는 최적화들을 할 때 사용자-시스템 상호작용들의 일관성 및 효율성에서 주로 명백하다. 예를 들어, 사용자는 액션 세부사항 및 장면 전환과 같은 비디오 내의 특정 요소의 제시를 정확하게 지정하거나 조정하는 것이 어렵다는 것을 발견할 수 있다. 또한, 복잡한 언어 명령어를 이해하거나 미묘한 의미적 차이를 포착하는 소라의 한계는 사용자의 기대나 요구를 완전히 충족시키지 못하는 비디오 콘텐츠를 초래할 수 있다. 이러한 단점은 영상 편집 및 향상에서 소라의 잠재력을 제한하며, 사용자 경험의 전반적인 만족도에도 영향을 미친다.\n' +
      '\n' +
      '**사용 제한.**사용 제한과 관련하여 오픈AI는 아직 소라에 대한 공개 접근에 대한 특정 출시 날짜를 설정하지 않았으며 광범위한 배포 전에 안전과 준비에 대한 신중한 접근법을 강조했다. 이는 보안, 프라이버시 보호 및 콘텐츠 검토와 같은 영역에서의 추가적인 개선 및 테스트가 여전히 소라에게 필요할 수 있음을 나타낸다. 더욱이, 현재 소라는 최대 1분 길이의 비디오만 생성할 수 있으며, 공개된 사례에 따르면 대부분의 생성된 비디오는 불과 수십 초의 길이이다. 이러한 제한은 상세한 교육 비디오 또는 심층 스토리텔링과 같이 더 긴 콘텐츠 디스플레이를 요구하는 애플리케이션에서의 사용을 제한한다. 이러한 제한은 콘텐츠 제작에 있어 소라의 유연성을 감소시킨다.\n' +
      '\n' +
      '### Opportunities\n' +
      '\n' +
      '** 아카데미.** (1) 오픈AI에 의한 소라의 도입은 확산 및 변압기 기술을 모두 활용하여 텍스트-비디오 모델의 탐사를 더 깊이 탐구하도록 광범위한 AI 커뮤니티를 장려하는 전략적 전환을 나타낸다. 이 이니셔티브는 콘텐츠 생성, 스토리텔링 및 정보 공유에 혁명을 약속하는 프론티어인 텍스트 기술에서 직접 고도로 정교하고 미묘한 비디오 콘텐츠를 생성할 수 있는 잠재력으로 초점을 재조정하는 것을 목표로 한다. (2) 전통적인 크기 조정이나 작부 방법과는 달리 소라를 고유 크기로 훈련시키는 혁신적인 접근 방식은 학계에 획기적인 영감을 준다. 수정되지 않은 데이터 세트를 활용하는 이점을 강조하여 새로운 경로를 열어 더 발전된 생성 모델을 생성한다.\n' +
      '\n' +
      '**Industry.** (1) 소라의 현재 능력은 비디오 시뮬레이션 기술의 발전을 위한 유망한 경로를 신호하여 물리적 및 디지털 영역 모두에서 사실성을 크게 향상시킬 가능성을 강조한다. 텍스트적 묘사를 통해 현실성이 높은 환경을 조성할 수 있는 소라의 전망은 콘텐츠 창작의 유망한 미래를 제시한다. 이 잠재력은 혁신적인 게임 개발로 확장되어 전례 없는 용이성과 정확성으로 몰입형 생성 세계를 제작할 수 있는 미래를 엿볼 수 있습니다. (2) 기업은 소라를 활용하여 시장 변화에 신속하게 적응하는 광고 동영상을 제작하고 맞춤형 마케팅 콘텐츠를 제작할 수 있다. 이는 제작비를 절감할 뿐만 아니라 광고의 매력성과 효과성도 높여준다. 텍스트 설명만으로 매우 사실적인 비디오 콘텐츠를 생성하는 소라의 능력은 브랜드가 청중과 참여하는 방식에 혁명을 일으킬 수 있으며, 전례 없는 방식으로 제품이나 서비스의 본질을 포착하는 몰입적이고 설득력 있는 비디오를 생성할 수 있다.\n' +
      '\n' +
      '**Society.** (1) 전통적인 영화 제작을 대체하기 위해 텍스트-비디오 기술을 활용할 것이라는 전망은 여전히 멀지만, 소라와 유사한 플랫폼은 소셜 미디어에서 콘텐츠 생성을 위한 혁신적인 잠재력을 가지고 있다. 현재 비디오 길이의 제약은 이러한 도구가 고품질 비디오 생산을 모든 사람이 액세스할 수 있게 하는 데 미칠 수 있는 영향을 감소시키지 않으며, 개인은 고가의 장비가 필요 없이 강력한 콘텐츠를 생산할 수 있게 한다. 그것은 틱톡과 릴스와 같은 플랫폼에서 콘텐츠 제작자에게 권한을 부여함으로써 새로운 창의성과 참여의 시대를 가져오기 위한 중요한 변화를 나타낸다. (2) 시나리오 작가와 크리에이티브 전문가는 소라를 사용하여 쓰기 스크립트를 비디오로 변환하고, 그들의 크리에이티브 개념을 더 잘 보여주고 공유하는 것을 돕고, 단편 영화와 애니메이션을 제작하는데도 도움을 줄 수 있다. 대본에서 상세하고 생생한 비디오를 만드는 능력은 영화 제작과 애니메이션의 사전 제작 과정을 근본적으로 변화시킬 수 있으며, 미래의 이야기꾼들이 어떻게 내러티브를 던지고 발전시키고 다듬을 수 있는지 엿볼 수 있다. 이 기술은 아이디어를 실시간으로 시각화하고 평가할 수 있는 보다 역동적이고 상호작용적인 형태의 스크립트 개발의 가능성을 열어 창의성과 협업을 위한 강력한 도구를 제공한다. (3) 기자 및 언론 기관도 소라를 활용하여 뉴스 보도나 설명 동영상을 빠르게 생성할 수 있어 뉴스 내용을 더욱 생생하고 매력적으로 만들 수 있다. 이는 뉴스 보도의 보도와 청중 참여를 크게 증가시킬 수 있다. 소라는 사실적인 환경과 시나리오를 시뮬레이션할 수 있는 도구를 제공함으로써 시각적 스토리텔링을 위한 강력한 솔루션을 제공하여 기자들이 이전에 제작하기 어렵거나 비용이 많이 들었던 매력적인 비디오를 통해 복잡한 이야기를 전달할 수 있도록 한다. 요약하자면, 소라의 마케팅, 저널리즘, 엔터테인먼트 전반에 걸친 콘텐츠 창작에 혁명을 일으킬 잠재력은 어마어마하다.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '개발자와 연구자가 소라의 역량과 관련 작품을 연구할 수 있도록 소라에 대한 종합적인 검토를 제시한다. 검토는 기존 문헌을 기반으로 출판된 기술 보고서 및 역공학에 대한 조사를 기반으로 한다. 소라의 API가 사용 가능하고 소라에 대한 추가 세부 정보가 공개되면 논문을 계속 업데이트할 것입니다. 이 리뷰 논문이 오픈소스 연구 커뮤니티에 귀중한 자원을 증명하고, AIGC 시대에 비디오 자동 생성을 민주화하기 위해 가까운 미래에 커뮤니티가 소라의 오픈소스 버전을 공동으로 개발할 수 있는 기반을 마련하기를 바란다. 이 목표를 달성하기 위해 모든 전선에서 논의, 제안 및 협력을 초대합니다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* [1] OpenAI, "Chatgpt: Get instant answers, find creative inspiration, learn something new.." [https://openai.com/chatgpt](https://openai.com/chatgpt), 2022.\n' +
      '* [2] OpenAI, "Gpt-4 technical report," 2023.\n' +
      '* [3] OpenAI, "Sora: Creating video from text." [https://openai.com/soora](https://openai.com/soora), 2024.\n' +
      '* [4] W. Peebles and S. Xie, "Scalable diffusion models with transformers," in _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 4195-4205, 2023.\n' +
      '* [5] A. A. Efros and T. K. Leung, "Texture synthesis by non-parametric sampling," in _Proceedings of the seventh IEEE international conference on computer vision_, vol. 2, pp. 1033-1038, IEEE, 1999.\n' +
      '* [6] P. S. Heckbert, "Survey of texture mapping," _IEEE computer graphics and applications_, vol. 6, no. 11, pp. 56-67, 1986.\n' +
      '* [7] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, "Generative adversarial networks," _arXiv_, 2014.\n' +
      '* [8] D. P. Kingma and M. Welling, "Auto-encoding variational bayes," _arXiv preprint arXiv:1312.6114_, 2013.\n' +
      '* [9] L. Dinh, D. Krueger, and Y. Bengio, "Nice: Non-linear independent components estimation," _arXiv preprint arXiv:1410.8516_, 2014.\n' +
      '* [10] Y. Song and S. Ermon, "Generative modeling by estimating gradients of the data distribution," _Advances in Neural Information Processing Systems_, vol. 32, 2019.\n' +
      '* [11] Y. Cao, S. Li, Y. Liu, Z. Yan, Y. Dai, P. S. Yu, and L. Sun, "A comprehensive survey of ai-generated content (aigc): A history of generative ai from gan to chatgpt," _arXiv preprint arXiv:2303.04226_, 2023.\n' +
      '* [12] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, and I. Polosukhin, "Attention is all you need," in _Advances in Neural Information Processing Systems_ (I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, eds.), vol. 30, Curran Associates, Inc., 2017.\n' +
      '\n' +
      '* [13] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, "Bert: Pre-training of deep bidirectional transformers for language understanding," _arXiv preprint arXiv:1810.04805_, 2018.\n' +
      '* [14] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, _et al._, "Improving language understanding by generative pre-training," 2018.\n' +
      '* [15] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, _et al._, "An image is worth 16x16 words: Transformers for image recognition at scale," _arXiv preprint arXiv:2010.11929_, 2020.\n' +
      '* [16] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, "Swin transformer: Hierarchical vision transformer using shifted windows," in _Proceedings of the IEEE/CVF international conference on computer vision_, pp. 10012-10022, 2021.\n' +
      '* [17] O. Ronneberger, P. Fischer, and T. Brox, "U-net: Convolutional networks for biomedical image segmentation," in _Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18_, pp. 234-241, Springer, 2015.\n' +
      '* [18] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, "Learning transferable visual models from natural language supervision," 2021.\n' +
      '* [19] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, "High-resolution image synthesis with latent diffusion models," in _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pp. 10684-10695, 2022.\n' +
      '* [20] M. AI, "Midjourney: Text to image with ai art generator." [https://www.midjourneyai.ai/en](https://www.midjourneyai.ai/en), 2023.\n' +
      '* [21] J. Betker, G. Goh, L. Jing, T. Brooks, J. Wang, L. Li, L. Ouyang, J. Zhuang, J. Lee, Y. Guo, _et al._, "Improving image generation with better captions," _Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf_, vol. 2, p. 3, 2023.\n' +
      '* [22] P. AI, "Pika is the idea-to-video platform that sets your creativity in motion.." [https://pika.art/home](https://pika.art/home), 2023.\n' +
      '* [23] R. AI, "Gen-2: Gen-2: The next step forward for generative ai." [https://research.runwayml.com/gen2](https://research.runwayml.com/gen2), 2023.\n' +
      '* [24] X. Zhai, A. Kolesnikov, N. Houlsby, and L. Beyer, "Scaling vision transformers," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 12104-12113, 2022.\n' +
      '* [25] M. Dehghani, J. Djolonga, B. Mustafa, P. Padlewski, J. Heek, J. Gilmer, A. P. Steiner, M. Caron, R. Geirhos, I. Alabdulmohsin, _et al._, "Scaling vision transformers to 22 billion parameters," in _International Conference on Machine Learning_, pp. 7480-7512, PMLR, 2023.\n' +
      '* [26] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, _et al._, "Learning transferable visual models from natural language supervision," in _International conference on machine learning_, pp. 8748-8763, PMLR, 2021.\n' +
      '* [27] A. Blattmann, T. Dockhorn, S. Kulal, D. Mendelevitch, M. Kilian, D. Lorenz, Y. Levi, Z. English, V. Voleti, A. Letts, _et al._, "Stable video diffusion: Scaling latent video diffusion models to large datasets," _arXiv preprint arXiv:2311.15127_, 2023.\n' +
      '\n' +
      '* [28] U. Singer, A. Polyak, T. Hayes, X. Yin, J. An, S. Zhang, Q. Hu, H. Yang, O. Ashual, O. Gafni, D. Parikh, S. Gupta, and Y. Taigman, "Make-a-video: Text-to-video generation without text-video data," 2022.\n' +
      '* [29] J. Ho, W. Chan, C. Saharia, J. Whang, R. Gao, A. Gritsenko, D. P. Kingma, B. Poole, M. Norouzi, D. J. Fleet, _et al._, "Imagen video: High definition video generation with diffusion models," _arXiv preprint arXiv:2210.02303_, 2022.\n' +
      '* [30] R. Sutton, "The bitter lesson." [http://www.incomplete](http://www.incomplete) ideas.net/IncIdeas/BitterLesson.html, March 2019. Accessed: Your Access Date Here.\n' +
      '* [31] S. Xie, "Take on sora technical report." [https://twitter.com/sainingxie/status/1758433676105310543](https://twitter.com/sainingxie/status/1758433676105310543), 2024.\n' +
      '* [32] A. Van Den Oord, O. Vinyals, _et al._, "Neural discrete representation learning," _Advances in neural information processing systems_, vol. 30, 2017.\n' +
      '* [33] K. He, X. Chen, S. Xie, Y. Li, P. Dollar, and R. Girshick, "Masked autoencoders are scalable vision learners," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 16000-16009, 2022.\n' +
      '* [34] S. Ge, S. Nah, G. Liu, T. Poon, A. Tao, B. Catanzaro, D. Jacobs, J.-B. Huang, M.-Y. Liu, and Y. Balaji, "Preserve your own correlation: A noise prior for video diffusion models," in _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 22930-22941, 2023.\n' +
      '* [35] A. Sauer, D. Lorenz, A. Blattmann, and R. Rombach, "Adversarial diffusion distillation," _arXiv preprint arXiv:2311.17042_, 2023.\n' +
      '* [36] A. Blattmann, R. Rombach, H. Ling, T. Dockhorn, S. W. Kim, S. Fidler, and K. Kreis, "Align your latents: High-resolution video synthesis with latent diffusion models," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 22563-22575, 2023.\n' +
      '* [37] M. Ryoo, A. Piergiovanni, A. Arnab, M. Dehghani, and A. Angelova, "Tokenlearner: Adaptive space-time tokenization for videos," _Advances in Neural Information Processing Systems_, vol. 34, pp. 12786-12797, 2021.\n' +
      '* [38] A. Arnab, M. Dehghani, G. Heigold, C. Sun, M. Lucic, and C. Schmid, "Vivit: A video vision transformer," _arXiv preprint arXiv:2103.15691_, 2021.\n' +
      '* [39] L. Beyer, P. Izmailov, A. Kolesnikov, M. Caron, S. Kornblith, X. Zhai, M. Minderer, M. Tschannen, I. Alabdulmohsin, and F. Pavetic, "Flexivit: One model for all patch sizes," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 14496-14506, 2023.\n' +
      '* [40] M. Dehghani, B. Mustafa, J. Djolonga, J. Heek, M. Minderer, M. Caron, A. Steiner, J. Puigcerver, R. Geirhos, I. M. Alabdulmohsin, _et al._, "Patch n\'pack: Navit, a vision transformer for any aspect ratio and resolution," _Advances in Neural Information Processing Systems_, vol. 36, 2024.\n' +
      '* [41] M. M. Krell, M. Kosec, S. P. Perez, and A. Fitzgibbon, "Efficient sequence packing without cross-contamination: Accelerating large language models without impacting performance," _arXiv preprint arXiv:2107.02027_, 2021.\n' +
      '* [42] H. Yin, A. Vahdat, J. M. Alvarez, A. Mallya, J. Kautz, and P. Molchanov, "A-vit: Adaptive tokens for efficient vision transformer," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 10809-10818, 2022.\n' +
      '\n' +
      '* [43] D. Bolya, C.-Y. Fu, X. Dai, P. Zhang, C. Feichtenhofer, and J. Hoffman, "Token merging: Your vit but faster," in _The Eleventh International Conference on Learning Representations_, 2022.\n' +
      '* [44] M. Fayyaz, S. A. Koohpayegani, F. R. Jafari, S. Sengupta, H. R. V. Joze, E. Sommerlade, H. Pirsiavash, and J. Gall, "Adaptive token sampling for efficient vision transformers," in _European Conference on Computer Vision_, pp. 396-414, Springer, 2022.\n' +
      '* [45] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, "Attention is all you need," _Advances in neural information processing systems_, vol. 30, 2017.\n' +
      '* [46] G. Bertasius, H. Wang, and L. Torresani, "Is space-time attention all you need for video understanding?," in _ICML_, vol. 2, p. 4, 2021.\n' +
      '* [47] L. Yu, J. Lezama, N. B. Gundavarapu, L. Versari, K. Sohn, D. Minnen, Y. Cheng, A. Gupta, X. Gu, A. G. Hauptmann, _et al._, "Language model beats diffusion-tokenizer is key to visual generation," _arXiv preprint arXiv:2310.05737_, 2023.\n' +
      '* [48] N. Shazeer, "Fast transformer decoding: One write-head is all you need," 2019.\n' +
      '* [49] J. Ainslie, J. Lee-Thorp, M. de Jong, Y. Zemlyanskiy, F. Lebron, and S. Sanghai, "Gqa: Training generalized multi-query transformer models from multi-head checkpoints," _arXiv preprint arXiv:2305.13245_, 2023.\n' +
      '* [50] A. Gu and T. Dao, "Mamba: Linear-time sequence modeling with selective state spaces," _arXiv preprint arXiv:2312.00752_, 2023.\n' +
      '* [51] J. Sohl-Dickstein, E. A. Weiss, N. Maheswaranathan, and S. Ganguli, "Deep unsupervised learning using nonequilibrium thermodynamics," _arXiv preprint arXiv:1503.03585_, 2015.\n' +
      '* [52] J. Ho, A. Jain, and P. Abbeel, "Denoising diffusion probabilistic models," _Advances in Neural Information Processing Systems_, vol. 33, pp. 6840-6851, 2020.\n' +
      '* [53] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole, "Score-based generative modeling through stochastic differential equations," _arXiv preprint arXiv:2011.13456_, 2020.\n' +
      '* [54] F. Bao, S. Nie, K. Xue, Y. Cao, C. Li, H. Su, and J. Zhu, "All are worth words: A vit backbone for diffusion models," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, 2023.\n' +
      '* [55] S. Gao, P. Zhou, M.-M. Cheng, and S. Yan, "Masked diffusion transformer is a strong image synthesizer," _arXiv preprint arXiv:2303.14389_, 2023.\n' +
      '* [56] A. Hatamizadeh, J. Song, G. Liu, J. Kautz, and A. Vahdat, "Diffit: Diffusion vision transformers for image generation," _arXiv preprint arXiv:2312.02139_, 2023.\n' +
      '* [57] J. Ho and T. Salimans, "Classifier-free diffusion guidance," _arXiv preprint arXiv:2207.12598_, 2022.\n' +
      '* [58] T. Salimans and J. Ho, "Progressive distillation for fast sampling of diffusion models," _arXiv preprint arXiv:2202.00512_, 2022.\n' +
      '* [59] J. Ho, C. Saharia, W. Chan, D. J. Fleet, M. Norouzi, and T. Salimans, "Cascaded diffusion models for high fidelity image generation," _The Journal of Machine Learning Research_, vol. 23, no. 1, pp. 2249-2281, 2022.\n' +
      '\n' +
      '* [60] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, "High-resolution image synthesis with latent diffusion models," 2021.\n' +
      '* [61] D. Podell, Z. English, K. Lacey, A. Blattmann, T. Dockhorn, J. Muller, J. Penna, and R. Rombach, "Sdxl: Improving latent diffusion models for high-resolution image synthesis," _arXiv preprint arXiv:2307.01952_, 2023.\n' +
      '* [62] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, _et al._, "Language models are few-shot learners," _arXiv_, 2020.\n' +
      '* [63] K. Zhou, J. Yang, C. C. Loy, and Z. Liu, "Conditional prompt learning for vision-language models," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 16816-16825, 2022.\n' +
      '* [64] V. Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler, T. L. Scao, A. Raja, _et al._, "Multitask prompted training enables zero-shot task generalization," _arXiv preprint arXiv:2110.08207_, 2021.\n' +
      '* [65] J. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le, "Finetuned language models are zero-shot learners," _arXiv preprint arXiv:2109.01652_, 2021.\n' +
      '* [66] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, _et al._, "Training language models to follow instructions with human feedback," _Advances in Neural Information Processing Systems_, vol. 35, pp. 27730-27744, 2022.\n' +
      '* [67] C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. Le, Y.-H. Sung, Z. Li, and T. Duerig, "Scaling up visual and vision-language representation learning with noisy text supervision," in _International conference on machine learning_, pp. 4904-4916, PMLR, 2021.\n' +
      '* [68] J. Yu, Z. Wang, V. Vasudevan, L. Yeung, M. Seyedhosseini, and Y. Wu, "Coca: Contrastive captioners are image-text foundation models," _arXiv preprint arXiv:2205.01917_, 2022.\n' +
      '* [69] S. Yan, T. Zhu, Z. Wang, Y. Cao, M. Zhang, S. Ghosh, Y. Wu, and J. Yu, "Video-text modeling with zero-shot transfer from contrastive captioners," _arXiv preprint arXiv:2212.04979_, 2022.\n' +
      '* [70] H. Xu, Q. Ye, M. Yan, Y. Shi, J. Ye, Y. Xu, C. Li, B. Bi, Q. Qian, W. Wang, _et al._, "mplug-2: A modularized multi-modal foundation model across text, image and video," _arXiv preprint arXiv:2302.00402_, 2023.\n' +
      '* [71] J. Wang, Z. Yang, X. Hu, L. Li, K. Lin, Z. Gan, Z. Liu, C. Liu, and L. Wang, "Git: A generative image-to-text transformer for vision and language," _arXiv preprint arXiv:2205.14100_, 2022.\n' +
      '* [72] A. Yang, A. Miech, J. Sivic, I. Laptev, and C. Schmid, "Zero-shot video question answering via frozen bidirectional language models," _Advances in Neural Information Processing Systems_, vol. 35, pp. 124-141, 2022.\n' +
      '* Large Language Models for Natural Language Processings_, RANLP, INCOMA Ltd., Shoumen, BULGARIA, 2023.\n' +
      '* [74] B. Chen, Z. Zhang, N. Langrene, and S. Zhu, "Unleashing the potential of prompt engineering in large language models: a comprehensive review," _arXiv preprint arXiv:2310.14735_, 2023.\n' +
      '\n' +
      '* [75] S. Pitis, M. R. Zhang, A. Wang, and J. Ba, "Boosted prompt ensembles for large language models," 2023.\n' +
      '* [76] Y. Hao, Z. Chi, L. Dong, and F. Wei, "Optimizing prompts for text-to-image generation," 2023.\n' +
      '* [77] S. Huang, B. Gong, Y. Pan, J. Jiang, Y. Lv, Y. Li, and D. Wang, "Vop: Text-video co-operative prompt tuning for cross-modal retrieval," 2023.\n' +
      '* [78] J. Z. Wu, Y. Ge, X. Wang, W. Lei, Y. Gu, Y. Shi, W. Hsu, Y. Shan, X. Qie, and M. Z. Shou, "Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation," 2023.\n' +
      '* [79] T. Luddecke and A. Ecker, "Image segmentation using text and image prompts," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pp. 7086-7096, June 2022.\n' +
      '* [80] X. Chen, Y. Wang, L. Zhang, S. Zhuang, X. Ma, J. Yu, Y. Wang, D. Lin, Y. Qiao, and Z. Liu, "Seine: Short-to-long video diffusion model for generative transition and prediction," 2023.\n' +
      '* [81] H. Chen, Y. Zhang, X. Cun, M. Xia, X. Wang, C. Weng, and Y. Shan, "Videocrafter2: Overcoming data limitations for high-quality video diffusion models," 2024.\n' +
      '* [82] T.-C. Wang, M.-Y. Liu, J.-Y. Zhu, G. Liu, A. Tao, J. Kautz, and B. Catanzaro, "Video-to-video synthesis," 2018.\n' +
      '* [83] T.-C. Wang, M.-Y. Liu, A. Tao, G. Liu, J. Kautz, and B. Catanzaro, "Few-shot video-to-video synthesis," 2019.\n' +
      '* [84] D. J. Zhang, D. Li, H. Le, M. Z. Shou, C. Xiong, and D. Sahoo, "Moonshot: Towards controllable video generation and editing with multimodal conditions," 2024.\n' +
      '* [85] L. Zhuo, G. Wang, S. Li, W. Wu, and Z. Liu, "Fast-vid2vid: Spatial-temporal compression for video-to-video synthesis," 2022.\n' +
      '* [86] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing," 2021.\n' +
      '* [87] B. Lester, R. Al-Rfou, and N. Constant, "The power of scale for parameter-efficient prompt tuning," in _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pp. 3045-3059, 2021.\n' +
      '* [88] M. Jia, L. Tang, B.-C. Chen, C. Cardie, S. Belongie, B. Hariharan, and S.-N. Lim, "Visual prompt tuning," in _European Conference on Computer Vision_, pp. 709-727, Springer, 2022.\n' +
      '* [89] OpenAI, "Introducing chatgpt," 2023.\n' +
      '* [90] OpenAI, "Gpt-4v(ision) system card," 2023.\n' +
      '* [91] Y. Huang and L. Sun, "Harnessing the power of chatgpt in fake news: An in-depth exploration in generation, detection and explanation," 2023.\n' +
      '* [92] C. Chen and K. Shu, "Can llm-generated misinformation be detected?," 2023.\n' +
      '* [93] Z. Liu, Y. Huang, X. Yu, L. Zhang, Z. Wu, C. Cao, H. Dai, L. Zhao, Y. Li, P. Shu, F. Zeng, L. Sun, W. Liu, D. Shen, Q. Li, T. Liu, D. Zhu, and X. Li, "Deid-gpt: Zero-shot medical text de-identification by gpt-4," 2023.\n' +
      '\n' +
      '* [94] J. Yao, X. Yi, X. Wang, Y. Gong, and X. Xie, "Value fulcra: Mapping large language models to the multidimensional spectrum of basic human values," 2023.\n' +
      '* [95] Y. Huang, Q. Zhang, P. S. Y, and L. Sun, "Trustgpt: A benchmark for trustworthy and responsible large language models," 2023.\n' +
      '* [96] L. Sun, Y. Huang, H. Wang, S. Wu, Q. Zhang, C. Gao, Y. Huang, W. Lyu, Y. Zhang, X. Li, Z. Liu, Y. Liu, Y. Wang, Z. Zhang, B. Kailkhura, C. Xiong, C. Xiao, C. Li, E. Xing, F. Huang, H. Liu, H. Ji, H. Wang, H. Zhang, H. Yao, M. Kellis, M. Zitnik, M. Jiang, M. Bansal, J. Zou, J. Pei, J. Liu, J. Gao, J. Han, J. Zhao, J. Tang, J. Wang, J. Mitchell, K. Shu, K. Xu, K.-W. Chang, L. He, L. Huang, M. Backes, N. Z. Gong, P. S. Yu, P.-Y. Chen, Q. Gu, R. Xu, R. Ying, S. Ji, S. Jana, T. Chen, T. Liu, T. Zhou, W. Wang, X. Li, X. Zhang, X. Wang, X. Xie, X. Chen, X. Wang, Y. Liu, Y. Ye, Y. Cao, Y. Chen, and Y. Zhao, "Trustllm: Trustworthiness in large language models," 2024.\n' +
      '* [97] M. Mazeika, L. Phan, X. Yin, A. Zou, Z. Wang, N. Mu, E. Sakhaee, N. Li, S. Basart, B. Li, D. Forsyth, and D. Hendrycks, "Harmbench: A standardized evaluation framework for automated red teaming and robust refusal," 2024.\n' +
      '* [98] Y. Wang, H. Li, X. Han, P. Nakov, and T. Baldwin, "Do-not-answer: A dataset for evaluating safeguards in llms," 2023.\n' +
      '* [99] B. Wang, W. Chen, H. Pei, C. Xie, M. Kang, C. Zhang, C. Xu, Z. Xiong, R. Dutta, R. Schaeffer, _et al._, "Decodingtrust: A comprehensive assessment of trustworthiness in gpt models," _arXiv preprint arXiv:2306.11698_, 2023.\n' +
      '* [100] Z. Zhang, L. Lei, L. Wu, R. Sun, Y. Huang, C. Long, X. Liu, X. Lei, J. Tang, and M. Huang, "Safetybench: Evaluating the safety of large language models with multiple choice questions," 2023.\n' +
      '* [101] X. Shen, Z. Chen, M. Backes, Y. Shen, and Y. Zhang, "" do anything now": Characterizing and evaluating in-the-wild jailbreak prompts on large language models," _arXiv preprint arXiv:2308.03825_, 2023.\n' +
      '* [102] X. Liu, N. Xu, M. Chen, and C. Xiao, "Autodan: Generating stealthy jailbreak prompts on aligned large language models," _arXiv preprint arXiv:2310.04451_, 2023.\n' +
      '* [103] S. Zhu, R. Zhang, B. An, G. Wu, J. Barrow, Z. Wang, F. Huang, A. Nenkova, and T. Sun, "Autodan: Interpretable gradient-based adversarial attacks on large language models," 2023.\n' +
      '* [104] A. Zhou, B. Li, and H. Wang, "Robust prompt optimization for defending language models against jailbreaking attacks," _arXiv preprint arXiv:2401.17263_, 2024.\n' +
      '* [105] X. Guo, F. Yu, H. Zhang, L. Qin, and B. Hu, "Cold-attack: Jailbreaking llms with stealthiness and controllability," 2024.\n' +
      '* [106] A. Wei, N. Haghtalab, and J. Steinhardt, "Jailbroken: How does llm safety training fail?," _arXiv preprint arXiv:2307.02483_, 2023.\n' +
      '* [107] Z. Niu, H. Ren, X. Gao, G. Hua, and R. Jin, "Jailbreaking attack against multimodal large language model," 2024.\n' +
      '* [108] H. Liu, W. Xue, Y. Chen, D. Chen, X. Zhao, K. Wang, L. Hou, R. Li, and W. Peng, "A survey on hallucination in large vision-language models," 2024.\n' +
      '\n' +
      '* [109] T. Guan, F. Liu, X. Wu, R. Xian, Z. Li, X. Liu, X. Wang, L. Chen, F. Huang, Y. Yacoob, D. Manocha, and T. Zhou, "Hallusionbench: An advanced diagnostic suite for entangled language hallucination & visual illusion in large vision-language models," 2023.\n' +
      '* [110] Y. Li, Y. Du, K. Zhou, J. Wang, W. X. Zhao, and J.-R. Wen, "Evaluating object hallucination in large vision-language models," 2023.\n' +
      '* [111] Y. Huang, J. Shi, Y. Li, C. Fan, S. Wu, Q. Zhang, Y. Liu, P. Zhou, Y. Wan, N. Z. Gong, _et al._, "Metaool benchmark for large language models: Deciding whether to use tools and which to use," _arXiv preprint arXiv:2310.03128_, 2023.\n' +
      '* [112] F. Liu, K. Lin, L. Li, J. Wang, Y. Yacoob, and L. Wang, "Mitigating hallucination in large multi-modal models via robust instruction tuning," 2023.\n' +
      '* [113] L. Wang, J. He, S. Li, N. Liu, and E.-P. Lim, "Mitigating fine-grained hallucination by fine-tuning large vision-language models with caption rewrites," in _International Conference on Multimedia Modeling_, pp. 32-45, Springer, 2024.\n' +
      '* [114] Y. Zhou, C. Cui, J. Yoon, L. Zhang, Z. Deng, C. Finn, M. Bansal, and H. Yao, "Analyzing and mitigating object hallucination in large vision-language models," _arXiv preprint arXiv:2310.00754_, 2023.\n' +
      '* [115] I. O. Gallegos, R. A. Rossi, J. Barrow, M. M. Tanjim, S. Kim, F. Dernoncourt, T. Yu, R. Zhang, and N. K. Ahmed, "Bias and fairness in large language models: A survey," _arXiv preprint arXiv:2309.00770_, 2023.\n' +
      '* [116] J. Zhang, K. Bao, Y. Zhang, W. Wang, F. Feng, and X. He, "Is chatgpt fair for recommendation? evaluating fairness in large language model recommendation," _arXiv preprint arXiv:2305.07609_, 2023.\n' +
      '* [117] Y. Liang, L. Cheng, A. Payani, and K. Shu, "Beyond detection: Unveiling fairness vulnerabilities in abusive language models," 2023.\n' +
      '* [118] F. Friedrich, P. Schramowski, M. Brack, L. Struppek, D. Hintersdorf, S. Luccioni, and K. Kersting, "Fair diffusion: Instructing text-to-image generation models on fairness," _arXiv preprint arXiv:2302.10893_, 2023.\n' +
      '* [119] R. Liu, C. Jia, J. Wei, G. Xu, L. Wang, and S. Vosoughi, "Mitigating political bias in language models through reinforced calibration," _Proceedings of the AAAI Conference on Artificial Intelligence_, vol. 35, pp. 14857-14866, May 2021.\n' +
      '* [120] R. K. Mahabadi, Y. Belinkov, and J. Henderson, "End-to-end bias mitigation by modelling biases in corpora," 2020.\n' +
      '* [121] N. Mireshghallah, H. Kim, X. Zhou, Y. Tsvetkov, M. Sap, R. Shokri, and Y. Choi, "Can llms keep a secret? testing privacy implications of language models via contextual integrity theory," _arXiv preprint arXiv:2310.17884_, 2023.\n' +
      '* [122] R. Plant, V. Giuffrida, and D. Gkatzia, "You are what you write: Preserving privacy in the era of large language models," _arXiv preprint arXiv:2204.09391_, 2022.\n' +
      '* [123] H. Li, Y. Chen, J. Luo, Y. Kang, X. Zhang, Q. Hu, C. Chan, and Y. Song, "Privacy in large language models: Attacks, defenses and future directions," 2023.\n' +
      '\n' +
      '* [124] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, E. Brynjolfsson, S. Buch, D. Card, R. Castellon, N. Chatterji, A. Chen, K. Creel, J. Q. Davis, D. Demszky, C. Donahue, M. Doumbouya, E. Durmus, S. Ermon, J. Etchemendy, K. Ethayarajh, L. Fei-Fei, C. Finn, T. Gale, L. Gillespie, K. Goel, N. Goodman, S. Grossman, N. Guha, T. Hashimoto, P. Henderson, J. Hewitt, D. E. Ho, J. Hong, K. Hsu, J. Huang, T. Icard, S. Jain, D. Jurafsky, P. Kalluri, S. Karamcheti, G. Keeling, F. Khani, O. Khattab, P. W. Koh, M. Krass, R. Krishna, R. Kuditipudi, A. Kumar, F. Ladhak, M. Lee, T. Lee, J. Leskovec, I. Levent, X. L. Li, X. Li, T. Ma, A. Malik, C. D. Manning, S. Mirchandani, E. Mitchell, Z. Munyikwa, S. Nair, A. Narayan, D. Narayanan, B. Newman, A. Nie, J. C. Niebles, H. Nilforoshan, J. Nyarko, G. Ogut, L. Orr, I. Papadimitriou, J. S. Park, C. Piech, E. Portelance, C. Potts, A. Raghunathan, R. Reich, H. Ren, F. Rong, Y. Roohani, C. Ruiz, J. Ryan, C. Re, D. Sadigh, S. Sagawa, K. Santhanam, A. Shih, K. Srinivasan, A. Tamkin, R. Taori, A. W. Thomas, F. Tramer, R. E. Wang, W. Wang, B. Wu, J. Wu, Y. Wu, S. M. Xie, M. Yasunaga, J. You, M. Zaharia, M. Zhang, T. Zhang, X. Zhang, Y. Zhang, L. Zheng, K. Zhou, and P. Liang, "On the opportunities and risks of foundation models," 2022.\n' +
      '* [125] T. Shen, R. Jin, Y. Huang, C. Liu, W. Dong, Z. Guo, X. Wu, Y. Liu, and D. Xiong, "Large language model alignment: A survey," _arXiv preprint arXiv:2309.15025_, 2023.\n' +
      '* [126] X. Liu, X. Lei, S. Wang, Y. Huang, Z. Feng, B. Wen, J. Cheng, P. Ke, Y. Xu, W. L. Tam, X. Zhang, L. Sun, H. Wang, J. Zhang, M. Huang, Y. Dong, and J. Tang, "Alignbench: Benchmarking chinese alignment of large language models," 2023.\n' +
      '* [127] P. Christiano, J. Leike, T. B. Brown, M. Martic, S. Legg, and D. Amodei, "Deep reinforcement learning from human preferences," 2023.\n' +
      '* [128] T. Yu, Y. Yao, H. Zhang, T. He, Y. Han, G. Cui, J. Hu, Z. Liu, H.-T. Zheng, M. Sun, and T.-S. Chua, "Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback," 2023.\n' +
      '* [129] M. S. Jahan and M. Oussalah, "A systematic review of hate speech automatic detection using natural language processing.," _Neurocomputing_, p. 126232, 2023.\n' +
      '* [130] OpenAI, "Sora safety." [https://openai.com/sora#safety](https://openai.com/sora#safety), 2024.\n' +
      '* [131] Z. Fei, X. Shen, D. Zhu, F. Zhou, Z. Han, S. Zhang, K. Chen, Z. Shen, and J. Ge, "Lawbench: Benchmarking legal knowledge of large language models," _arXiv preprint arXiv:2309.16289_, 2023.\n' +
      '* [132] Y. Li, Y. Huang, Y. Lin, S. Wu, Y. Wan, and L. Sun, "I think, therefore i am: Benchmarking awareness of large language models using awarebench," 2024.\n' +
      '* [133] J. Zhu, H. Yang, H. He, W. Wang, Z. Tuo, W.-H. Cheng, L. Gao, J. Song, and J. Fu, "Moviefactory: Automatic movie creation from text using large generative models for language and images," _arXiv preprint arXiv:2306.07257_, 2023.\n' +
      '* [134] J. Zhu, H. Yang, W. Wang, H. He, Z. Tuo, Y. Yu, W.-H. Cheng, L. Gao, J. Song, J. Fu, _et al._, "Mobilevidfactory: Automatic diffusion-based social media video generation for mobile devices from text," in _Proceedings of the 31st ACM International Conference on Multimedia_, pp. 9371-9373, 2023.\n' +
      '* [135] S. Zhuang, K. Li, X. Chen, Y. Wang, Z. Liu, Y. Qiao, and Y. Wang, "Vlogger: Make your dream a vlog," _arXiv preprint arXiv:2401.09414_, 2024.\n' +
      '* [136] R. Feng, W. Weng, Y. Wang, Y. Yuan, J. Bao, C. Luo, Z. Chen, and B. Guo, "Ccedit: Creative and controllable video editing via diffusion models," _arXiv preprint arXiv:2309.16496_, 2023.\n' +
      '\n' +
      '* [137] J. Xing, M. Xia, Y. Liu, Y. Zhang, Y. Zhang, H. Liu, H. Chen, X. Cun, X. Wang, _et al._, "Make-your-video: Customized video generation using textual and structural guidance," _arXiv preprint arXiv:2306.00943_, 2023.\n' +
      '* [138] Y. Guo, C. Yang, A. Rao, Y. Wang, Y. Qiao, D. Lin, and B. Dai, "Animatediff: Animate your personalized text-to-image diffusion models without specific tuning," _arXiv preprint arXiv:2307.04725_, 2023.\n' +
      '* [139] Y. He, M. Xia, H. Chen, X. Cun, Y. Gong, J. Xing, Y. Zhang, X. Wang, C. Weng, Y. Shan, _et al._, "Animate-a-story: Storytelling with retrieval-augmented video generation," _arXiv preprint arXiv:2307.06940_, 2023.\n' +
      '* [140] H. Ni, C. Shi, K. Li, S. X. Huang, and M. R. Min, "Conditional image-to-video generation with latent flow diffusion models," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 18444-18455, 2023.\n' +
      '* [141] L. Hu, X. Gao, P. Zhang, K. Sun, B. Zhang, and L. Bo, "Animate anyone: Consistent and controllable image-to-video synthesis for character animation," _arXiv preprint arXiv:2311.17117_, 2023.\n' +
      '* [142] Y. Hu, C. Luo, and Z. Chen, "Make it move: controllable image-to-video generation with text descriptions," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 18219-18228, 2022.\n' +
      '* [143] K. Mei and V. Patel, "Vidm: Video implicit diffusion models," in _Proceedings of the AAAI Conference on Artificial Intelligence_, vol. 37, pp. 9117-9125, 2023.\n' +
      '* [144] S. Yu, K. Sohn, S. Kim, and J. Shin, "Video probabilistic diffusion models in projected latent space," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 18456-18466, 2023.\n' +
      '* [145] K. Su, K. Qian, E. Shlizerman, A. Torralba, and C. Gan, "Physics-driven diffusion models for impact sound synthesis from videos," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 9749-9759, 2023.\n' +
      '* [146] S. Li, W. Dong, Y. Zhang, F. Tang, C. Ma, O. Deussen, T.-Y. Lee, and C. Xu, "Dance-to-music generation with encoder-based textual inversion of diffusion models," _arXiv preprint arXiv:2401.17800_, 2024.\n' +
      '* [147] A. Awasthi, J. Nizam, S. Zare, S. Ahmad, M. J. Montalvo, N. Varadarajan, B. Roysam, and H. V. Nguyen, "Video diffusion models for the apoptosis forcasting," _bioRxiv_, pp. 2023-11, 2023.\n' +
      '* [148] A. Bozorgpour, Y. Sadegheih, A. Kazerouni, R. Azad, and D. Merhof, "Dermosegdiff: A boundary-aware segmentation diffusion model for skin lesion delineation," in _International Workshop on PRedictive Intelligence In MEdicine_, pp. 146-158, Springer, 2023.\n' +
      '* [149] A. Flaborea, L. Collorone, G. M. D. di Melendugno, S. D\'Arrigo, B. Prenkaj, and F. Galasso, "Multimodal motion conditioned diffusion model for skeleton-based video anomaly detection," in _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 10318-10329, 2023.\n' +
      '* [150] J. Wu, R. Fu, H. Fang, Y. Zhang, and Y. Xu, "Medsegdiff-v2: Diffusion based medical image segmentation with transformer," _arXiv preprint arXiv:2301.11798_, 2023.\n' +
      '\n' +
      '* [151] G. J. Chowdary and Z. Yin, "Diffusion transformer u-net for medical image segmentation," in _International Conference on Medical Image Computing and Computer-Assisted Intervention_, pp. 622-631, Springer, 2023.\n' +
      '* [152] I. Kapelyukh, V. Vosylius, and E. Johns, "Dall-e-bot: Introducing web-scale diffusion models to robotics," _IEEE Robotics and Automation Letters_, 2023.\n' +
      '* [153] W. Liu, T. Hermans, S. Chernova, and C. Paxton, "Structdiffusion: Object-centric diffusion for semantic rearrangement of novel objects," in _Workshop on Language and Robotics at CoRL 2022_, 2022.\n' +
      '* [154] M. Janner, Y. Du, J. B. Tenenbaum, and S. Levine, "Planning with diffusion for flexible behavior synthesis," _arXiv preprint arXiv:2205.09991_, 2022.\n' +
      '* [155] A. Ajay, Y. Du, A. Gupta, J. Tenenbaum, T. Jaakkola, and P. Agrawal, "Is conditional generative modeling all you need for decision-making?," _arXiv preprint arXiv:2211.15657_, 2022.\n' +
      '* [156] J. Carvalho, A. T. Le, M. Baierl, D. Koert, and J. Peters, "Motion planning diffusion: Learning and planning of robot motions with diffusion models," in _2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_, pp. 1916-1923, IEEE, 2023.\n' +
      '* [157] X. Gu, C. Wen, J. Song, and Y. Gao, "Seer: Language instructed video prediction with latent diffusion models," _arXiv preprint arXiv:2303.14897_, 2023.\n' +
      '* [158] Z. Chen, S. Kiami, A. Gupta, and V. Kumar, "Genaug: Retargeting behaviors to unseen situations via generative augmentation," _arXiv preprint arXiv:2302.06671_, 2023.\n' +
      '* [159] Z. Mandi, H. Bharadhwaj, V. Moens, S. Song, A. Rajeswaran, and V. Kumar, "Cacti: A framework for scalable multi-task multi-scene visual imitation learning," _arXiv preprint arXiv:2212.05711_, 2022.\n' +
      '* [160] T. Chen, L. Li, S. Saxena, G. Hinton, and D. J. Fleet, "A generalist framework for panoptic segmentation of images and videos," in _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 909-919, 2023.\n' +
      '* [161] W. Harvey, S. Naderiparizi, V. Masrani, C. Weilbach, and F. Wood, "Flexible diffusion modeling of long videos," _Advances in Neural Information Processing Systems_, vol. 35, pp. 27953-27965, 2022.\n' +
      '* [162] A. Gupta, S. Tian, Y. Zhang, J. Wu, R. Martin-Martin, and L. Fei-Fei, "Maskvit: Masked visual pre-training for video prediction," _arXiv preprint arXiv:2206.11894_, 2022.\n' +
      '* [163] W. Hong, M. Ding, W. Zheng, X. Liu, and J. Tang, "Cogvideo: Large-scale pretraining for text-to-video generation via transformers," _arXiv preprint arXiv:2205.15868_, 2022.\n' +
      '* [164] U. Singer, A. Polyak, T. Hayes, X. Yin, J. An, S. Zhang, Q. Hu, H. Yang, O. Ashual, O. Gafni, _et al._, "Make-a-video: Text-to-video generation without text-video data," _arXiv preprint arXiv:2209.14792_, 2022.\n' +
      '* [165] D. Zhou, W. Wang, H. Yan, W. Lv, Y. Zhu, and J. Feng, "Magicvideo: Efficient video generation with latent diffusion models," _arXiv preprint arXiv:2211.11018_, 2022.\n' +
      '* [166] S. Ge, T. Hayes, H. Yang, X. Yin, G. Pang, D. Jacobs, J.-B. Huang, and D. Parikh, "Long video generation with time-agnostic vqgan and time-sensitive transformer," in _European Conference on Computer Vision_, pp. 102-118, Springer, 2022.\n' +
      '\n' +
      '* [167] R. Villegas, M. Babaeizadeh, P.-J. Kindermans, H. Moraldo, H. Zhang, M. T. Saffar, S. Castro, J. Kunze, and D. Erhan, "Phenaki: Variable length video generation from open domain textual description," _arXiv preprint arXiv:2210.02399_, 2022.\n' +
      '* [168] P. Esser, J. Chiu, P. Atighehchian, J. Granskog, and A. Germanidis, "Structure and content-guided video synthesis with diffusion models," in _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 7346-7356, 2023.\n' +
      '* [169] L. Khachatryan, A. Movsisyan, V. Tadevosyan, R. Henschel, Z. Wang, S. Navasardyan, and H. Shi, "Text2video-zero: Text-to-image diffusion models are zero-shot video generators," _arXiv preprint arXiv:2303.13439_, 2023.\n' +
      '* [170] Z. Luo, D. Chen, Y. Zhang, Y. Huang, L. Wang, Y. Shen, D. Zhao, J. Zhou, and T. Tan, "Videofusion: Decomposed diffusion models for high-quality video generation," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 10209-10218, 2023.\n' +
      '* [171] A. Jabri, D. Fleet, and T. Chen, "Scalable adaptive computation for iterative generation," _arXiv preprint arXiv:2212.11972_, 2022.\n' +
      '* [172] L. Lian, B. Shi, A. Yala, T. Darrell, and B. Li, "Llm-grounded video diffusion models," _arXiv preprint arXiv:2309.17444_, 2023.\n' +
      '* [173] E. Molad, E. Horwitz, D. Valevski, A. R. Acha, Y. Matias, Y. Pritch, Y. Leviathan, and Y. Hoshen, "Dreamix: Video diffusion models are general video editors," _arXiv preprint arXiv:2302.01329_, 2023.\n' +
      '* [174] J. H. Liew, H. Yan, J. Zhang, Z. Xu, and J. Feng, "Magicedit: High-fidelity and temporally coherent video editing," _arXiv preprint arXiv:2308.14749_, 2023.\n' +
      '* [175] W. Chen, J. Wu, P. Xie, H. Wu, J. Li, X. Xia, X. Xiao, and L. Lin, "Control-a-video: Controllable text-to-video generation with diffusion models," _arXiv preprint arXiv:2305.13840_, 2023.\n' +
      '* [176] W. Chai, X. Guo, G. Wang, and Y. Lu, "Stablevideo: Text-driven consistency-aware diffusion video editing," in _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 23040-23050, 2023.\n' +
      '* [177] S. Yang, Y. Zhou, Z. Liu, and C. C. Loy, "Rerender a video: Zero-shot text-guided video-to-video translation," _arXiv preprint arXiv:2306.07954_, 2023.\n' +
      '* [178] D. Ceylan, C.-H. P. Huang, and N. J. Mitra, "Pix2video: Video editing using image diffusion," in _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 23206-23217, 2023.\n' +
      '* [179] B. Qin, J. Li, S. Tang, T.-S. Chua, and Y. Zhuang, "Instructvid2vid: Controllable video editing with natural language instructions," _arXiv preprint arXiv:2305.12328_, 2023.\n' +
      '* [180] D. Liu, Q. Li, A.-D. Dinh, T. Jiang, M. Shah, and C. Xu, "Diffusion action segmentation," in _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 10139-10149, 2023.\n' +
      '* [181] R. Feng, Y. Gao, T. H. E. Tse, X. Ma, and H. J. Chang, "Diffpose: Spatiotemporal diffusion model for video-based human pose estimation," in _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 14861-14872, 2023.\n' +
      '\n' +
      '* [182] L. Yu, Y. Cheng, K. Sohn, J. Lezama, H. Zhang, H. Chang, A. G. Hauptmann, M.-H. Yang, Y. Hao, I. Essa, _et al._, "Magvit: Masked generative video transformer," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 10459-10469, 2023.\n' +
      '* [183] Z. Li, R. Tucker, N. Snavely, and A. Holynski, "Generative image dynamics," _arXiv preprint arXiv:2309.07906_, 2023.\n' +
      '* ai text-to-video model." [https://easywithai.com/ai-video-generators/zeroscope/](https://easywithai.com/ai-video-generators/zeroscope/), 2023.\n' +
      '* [185] R. Girdhar, M. Singh, A. Brown, Q. Duval, S. Azadi, S. S. Rambhatla, A. Shah, X. Yin, D. Parikh, and I. Misra, "Emu video: Factorizing text-to-video generation by explicit image conditioning," _arXiv preprint arXiv:2311.10709_, 2023.\n' +
      '* [186] Y. Zeng, G. Wei, J. Zheng, J. Zou, Y. Wei, Y. Zhang, and H. Li, "Make pixels dance: High-dynamic video generation," _arXiv preprint arXiv:2311.10982_, 2023.\n' +
      '* [187] A. Gupta, L. Yu, K. Sohn, X. Gu, M. Hahn, L. Fei-Fei, I. Essa, L. Jiang, and J. Lezama, "Photorealistic video generation with diffusion models," _arXiv preprint arXiv:2312.06662_, 2023.\n' +
      '* [188] B. Wu, C.-Y. Chuang, X. Wang, Y. Jia, K. Krishnakumar, T. Xiao, F. Liang, L. Yu, and P. Vajda, "Fairy: Fast parallelized instruction-guided video-to-video synthesis," _arXiv preprint arXiv:2312.13834_, 2023.\n' +
      '* [189] D. Kondratyuk, L. Yu, X. Gu, J. Lezama, J. Huang, R. Hornung, H. Adam, H. Akbari, Y. Alon, V. Birodkar, _et al._, "Videopoet: A large language model for zero-shot video generation," _arXiv preprint arXiv:2312.14125_, 2023.\n' +
      '* [190] J. Wu, X. Li, C. Si, S. Zhou, J. Yang, J. Zhang, Y. Li, K. Chen, Y. Tong, Z. Liu, _et al._, "Towards language-driven video inpainting via multimodal large language models," _arXiv preprint arXiv:2401.10226_, 2024.\n' +
      '* [191] O. Bar-Tal, H. Chefer, O. Tov, C. Herrmann, R. Paiss, S. Zada, A. Ephrat, J. Hur, Y. Li, T. Michaeli, _et al._, "Lumiere: A space-time diffusion model for video generation," _arXiv preprint arXiv:2401.12945_, 2024.\n' +
      '\n' +
      'Related Works\n' +
      '\n' +
      '우리는 표 1에서 비디오 생성 작업에 대한 몇 가지 관련 작업을 보여준다.\n' +
      '\n' +
      '## Appendix A\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c l l l} \\hline \\hline\n' +
      '**Model name** & **Year** & **Backbone** & **Task** & **Group** \\\\ \\hline Imagen Video[29] & 2022 & Diffusion & Generation & Google \\\\ Pix2Seq-D[160] & 2022 & Diffusion & Segmentation & Google Deepmind \\\\ FDM[161] & 2022 & Diffusion & Prediction & UBC \\\\ MaskViT[162] & 2022 & Masked Vision Models & Prediction & Stanford, Salesforce \\\\ CogVideo[163] & 2022 & Auto-regressive & Generation & THU \\\\ Make-a-video[164] & 2022 & Diffusion & Generation & Meta \\\\ MagicVideo[165] & 2022 & Diffusion & Generation & ByteDance \\\\ TATS[166] & 2022 & Auto-regressive & Generation & University of Maryland, Meta \\\\ Phenaski[167] & 2022 & Masked Vision Models & Generation & Google Brain \\\\ Gen-11[168] & 2023 & Diffusion & Generation, Editing & RunwayML \\\\ LFDM[140] & 2023 & Diffusion & Generation & PSU, UCSD \\\\ Text2video-Zero[169] & 2023 & Diffusion & Generation & Picsart \\\\ Video Fusion[170] & 2023 & Diffusion & Generation & USAC, Alibaba \\\\ PYoCo[34] & 2023 & Diffusion & Generation & Nvidia \\\\ Video LDM[36] & 2023 & Diffusion & Generation & University of Maryland, Nvidia \\\\ RIN[171] & 2023 & Diffusion & Generation & Google Brain \\\\ LVD[172] & 2023 & Diffusion & Generation & UCB \\\\ Dreami[173] & 2023 & Diffusion & Editing & Google \\\\ MagicEdit[174] & 2023 & Diffusion & Editing & ByteDance \\\\ Control-A-Video[175] & 2023 & Diffusion & Editing & Sun Yat-Sen University \\\\ StableVideo[176] & 2023 & Diffusion & Editing & ZJU, MSRA \\\\ Tune-A-Video[78] & 2023 & Diffusion & Editing & NUS \\\\ Rerender-A-Video[177] & 2023 & Diffusion & Editing & NTU \\\\ Pix2Video[178] & 2023 & Diffusion & Editing & Adobe, UCL \\\\ InstrumentVid2Yid[179] & 2023 & Diffusion & Editing & ZJU \\\\ DiffAct[180] & 2023 & Diffusion & Action Detection & University of Sydney \\\\ DiPFose[181] & 2023 & Diffusion & Pose Estimation & Jilin University \\\\ MAGVIT[182] & 2023 & Masked Vision Models & Generation & Google \\\\ AnimateDiff[138] & 2023 & Diffusion & Generation & CUHK \\\\ MAGVIT V2[47] & 2023 & Masked Vision Models & Generation & Google \\\\ Generative Dynamics[183] & 2023 & Diffusion & Generation & Google \\\\ VideoCrafter[81] & 2023 & Diffusion & Generation & Tencent \\\\ Zeroscope[184] & 2023 & - & Generation & EasyWithAI \\\\ ModelScope & 2023 & - & Generation & Damo \\\\ Gen-2[23] & 2023 & - & Generation & RunwayML \\\\ Pika[22] & 2023 & - & Generation & Pika Labs \\\\ Emu Video[185] & 2023 & Diffusion & Generation & Meta \\\\ PixelDance[186] & 2023 & Diffusion & Generation & ByteDance \\\\ Stable Video Diffusion[27] & 2023 & Diffusion & Generation & Stability AI \\\\ W.A.L.T[187] & 2023 & Diffusion & Generation & Stanford, Google \\\\ Fairy[188] & 2023 & Diffusion & Generation, Editing & Meta \\\\ VideoPoet[189] & 2023 & Auto-regressive & Generation, Editing & Google \\\\ LGVI[190] & 2024 & Diffusion & Editing & PKU, NTU \\\\ Lumiere[191] & 2024 & Diffusion & Generation & Google \\\\ Sora[3] & 2024 & Diffusion & Generation, Editing & OpenAI \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 비디오 생성의 요약.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>