date: "2024-06-10"
author: Junlin Wang
title: Mixture-of-Agents Enhances Large Language Model Capabilities
thumbnail: https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.04692.png
link: https://huggingface.co/papers/2406.04692
summary: Researchers propose a new approach to improve the performance of large language models by combining multiple models in a layered architecture. The new method, called Mixture-of-Agents (MoA), achieves state-of-the-art performance on various tasks, including AlpacaEval 2.0, MT-Bench, and FLASK, outperforming GPT-4 Omni....
opinion: placeholder
tags:
    - ML
