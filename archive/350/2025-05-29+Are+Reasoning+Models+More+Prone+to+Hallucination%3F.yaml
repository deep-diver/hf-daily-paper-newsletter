date: "2025-05-29"
author: Zijun Yao
title: Are Reasoning Models More Prone to Hallucination?
thumbnail: ""
link: https://huggingface.co/papers/2505.23646
summary: This study investigates hallucination in large reasoning models, finding that post-training methods like supervised fine-tuning and verifiable reward RL reduce hallucination, while other methods may introduce nuanced hallucinations. The research also identifies two cognitive behaviors affecting factuality and explores the link between model uncertainty and hallucination....
opinion: placeholder
tags:
    - ML
