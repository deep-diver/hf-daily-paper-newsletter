date: "2025-05-28"
author: Ruichen Chen
title: 'Re-ttention: Ultra Sparse Visual Generation via Attention Statistical   Reshape'
thumbnail: ""
link: https://huggingface.co/papers/2505.22918
summary: The study presents Re-ttention, a method that significantly reduces computational complexity in visual generation models like Diffusion Transformers by utilizing high sparse attention and temporal redundancy. Re-ttention maintains visual quality at ultra-high sparsity levels, requiring only 3.1% of tokens during inference, and achieves substantial latency reduction on GPUs with minimal overhead cost....
opinion: placeholder
tags:
    - ML
