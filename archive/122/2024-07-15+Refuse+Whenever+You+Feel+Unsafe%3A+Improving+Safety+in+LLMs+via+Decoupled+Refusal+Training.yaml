date: "2024-07-15"
author: Youliang Yuan
title: 'Refuse Whenever You Feel Unsafe: Improving Safety in LLMs via Decoupled Refusal Training'
thumbnail: ""
link: https://huggingface.co/papers/2407.09121
summary: This study introduces a novel approach, Decoupled Refusal Training (DeRTa), to improve the safety of Large Language Models (LLMs) by training them to recognize and avoid unsafe content, and equipping them with the ability to transition from potential harm to safety refusal consistently throughout the harmful response sequence. Empirical evaluation demonstrates that DeRTa improves model safety without compromising performance, and surpasses well-known models such as GPT-4 in defending against att...
opinion: placeholder
tags:
    - ML
