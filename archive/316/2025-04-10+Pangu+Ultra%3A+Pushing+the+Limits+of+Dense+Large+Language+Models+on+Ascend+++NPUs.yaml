date: "2025-04-10"
author: Yichun Yin
title: 'Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend   NPUs'
thumbnail: ""
link: https://huggingface.co/papers/2504.07866
summary: The paper describes Pangu Ultra, a 135 billion parameter dense Transformer LLM trained on Ascend NPUs. It introduces depth-scaled sandwich normalization to stabilize training, pre-trains on 13.2 trillion tokens, and efficiently utilizes 8,192 Ascend NPUs for large-scale training, outperforming Llama 405B and Mistral Large 2, and achieving competitive results with DeepSeek-R1....
opinion: placeholder
tags:
    - ML
