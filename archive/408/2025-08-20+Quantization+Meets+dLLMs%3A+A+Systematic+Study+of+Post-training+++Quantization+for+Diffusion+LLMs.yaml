date: "2025-08-20"
author: Haokun Lin
title: 'Quantization Meets dLLMs: A Systematic Study of Post-training   Quantization for Diffusion LLMs'
thumbnail: ""
link: https://huggingface.co/papers/2508.14896
summary: This study explores compressing Diffusion LLMs, which are large language models that require a lot of resources, making them hard to use on devices like phones or laptops. The researchers found a problem with these models where some values are very large, making it hard to reduce the model size without losing important information. They then applied different methods to fix this issue and tested how well each method worked across various tasks and model types, providing useful insights for futur...
opinion: placeholder
tags:
    - ML
