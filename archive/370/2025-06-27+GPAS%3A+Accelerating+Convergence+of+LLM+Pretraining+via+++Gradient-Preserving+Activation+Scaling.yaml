date: "2025-06-27"
author: Tianhao Chen
title: 'GPAS: Accelerating Convergence of LLM Pretraining via   Gradient-Preserving Activation Scaling'
thumbnail: ""
link: https://huggingface.co/papers/2506.22049
summary: The study presents a method called Gradient-Preserving Activation Scaling (GPAS) to boost the performance of large language models by controlling activation variance without altering gradients. GPAS effectively scales down activations, maintains information, and prevents gradient vanishing, leading to consistent performance improvements across various model sizes and architectures....
opinion: placeholder
tags:
    - ML
