date: "2025-05-27"
author: Yehui Tang
title: 'Pangu Pro MoE: Mixture of Grouped Experts for Efficient Sparsity'
thumbnail: ""
link: https://huggingface.co/papers/2505.21411
summary: The paper presents a new architecture called Mixture of Grouped Experts (MoGE) that groups experts and balances their workload better than traditional Mixture of Experts (MoE) for improved efficiency. MoGE ensures a balanced computational load across devices, enhancing throughput, particularly for inference. The authors also introduce Pangu Pro MoE, a sparse model based on MoGE with 72 billion total parameters, optimized for Ascend NPUs, which demonstrates better expert load balancing, more effi...
opinion: placeholder
tags:
    - ML
