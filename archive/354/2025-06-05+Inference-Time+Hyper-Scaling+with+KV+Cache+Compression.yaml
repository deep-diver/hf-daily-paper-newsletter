date: "2025-06-05"
author: Adrian Łańcucki
title: Inference-Time Hyper-Scaling with KV Cache Compression
thumbnail: ""
link: https://huggingface.co/papers/2506.05345
summary: This study presents a new method called Dynamic Memory Sparsification (DMS) that compresses the KV cache in Transformer LLMs, enabling more tokens to be generated within the same compute budget. The result is improved accuracy for similar inference runtime and memory load, as demonstrated on various LLM families, including a significant boost in performance for the Qwen-R1 32B model....
opinion: placeholder
tags:
    - ML
