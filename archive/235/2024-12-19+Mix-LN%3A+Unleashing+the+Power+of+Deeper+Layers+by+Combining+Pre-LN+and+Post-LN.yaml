date: "2024-12-19"
author: Pengxiang Li
title: 'Mix-LN: Unleashing the Power of Deeper Layers by Combining Pre-LN and Post-LN'
thumbnail: ""
link: https://huggingface.co/papers/2412.13795
summary: This paper proposes a new normalization technique called Mix-LN that combines Pre-LN and Post-LN to improve the effectiveness of deeper layers in Large Language Models (LLMs). Mix-LN applies Post-LN to earlier layers and Pre-LN to deeper layers, resulting in more balanced and healthier gradient norms across the network, and enhancing the overall quality of LLM pre-training. Models pre-trained with Mix-LN also perform better during supervised fine-tuning and reinforcement learning from human feed...
opinion: placeholder
tags:
    - ML
