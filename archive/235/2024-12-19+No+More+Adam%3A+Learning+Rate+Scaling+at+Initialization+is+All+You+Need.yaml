date: "2024-12-19"
author: Minghao Xu
title: 'No More Adam: Learning Rate Scaling at Initialization is All You Need'
thumbnail: ""
link: https://huggingface.co/papers/2412.11768
summary: This paper presents SGD-SaI, an enhancement to SGD with momentum that adjusts learning rates based on gradient signal-to-noise ratios. It matches or outperforms AdamW in training various Transformer-based tasks and is more memory efficient. SGD-SaI is robust to hyperparameter variations and can be used for diverse applications like LoRA fine-tuning and diffusion models....
opinion: placeholder
tags:
    - ML
