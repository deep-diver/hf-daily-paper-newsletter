date: "2024-08-29"
author: Fangxun Shu
title: 'LLaVA-MoD: Making LLaVA Tiny via MoE Knowledge Distillation'
thumbnail: ""
link: https://huggingface.co/papers/2408.15881
summary: LLaVA-MoD is a new method that makes small language models better by learning from bigger ones. It uses a type of network called Mixture of Experts and a process called Direct Preference Optimization to help the small models understand and choose better responses. This method uses less computing power and has fewer parameters, making it more efficient. In tests, a small model using LLaVA-MoD was better than a bigger model and used much less data and parameters during training....
opinion: placeholder
tags:
    - ML
