date: "2024-08-29"
author: Shengyuan Zhang
title: Distribution Backtracking Builds A Faster Convergence Trajectory for One-step Diffusion Distillation
thumbnail: ""
link: https://huggingface.co/papers/2408.15991
summary: DisBack is a method for training student generators to mimic the convergence trajectory of teacher models in the score distillation process. It consists of Degradation Recording and Distribution Backtracking stages. DisBack achieves faster and better convergence than existing distillation methods and can be generalized to improve other distillation methods....
opinion: placeholder
tags:
    - ML
