date: "2025-03-10"
author: Jongwoo Ko
title: 'DistiLLM-2: A Contrastive Approach Boosts the Distillation of LLMs'
thumbnail: ""
link: https://huggingface.co/papers/2503.07067
summary: The paper proposes DistiLLM-2, a contrastive approach for language model distillation that improves performance by aligning teacher and student models across different data types. Experiments show that this method builds high-performing student models for tasks like instruction-following and code generation, and supports applications like preference alignment and vision-language extensions....
opinion: placeholder
tags:
    - ML
