date: "2025-03-07"
author: Justin Chih-Yao Chen
title: 'Symbolic Mixture-of-Experts: Adaptive Skill-based Routing for   Heterogeneous Reasoning'
thumbnail: ""
link: https://huggingface.co/papers/2503.05641
summary: The proposed Symbolic-MoE framework allows for adaptive instance-level mixing of pre-trained LLM experts by emphasizing skills. It improves performance by dynamically selecting the most relevant set of expert LLMs for diverse reasoning tasks based on their strengths, and then synthesizing the outputs into a final high-quality response. The system performs better than other strong LLMs like GPT4o-mini, as well as multi-agent approaches, with an average improvement of 8.15% over the best multi-age...
opinion: placeholder
tags:
    - ML
