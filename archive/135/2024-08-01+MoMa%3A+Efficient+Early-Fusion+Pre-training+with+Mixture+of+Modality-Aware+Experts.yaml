date: "2024-08-01"
author: Xi Victoria Lin
title: 'MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware Experts'
thumbnail: ""
link: https://huggingface.co/papers/2407.21770
summary: MoMa is a new way to train machines to understand both text and images at the same time. It splits the work between different 'experts' and saves a lot of energy and time compared to other methods....
opinion: placeholder
tags:
    - ML
