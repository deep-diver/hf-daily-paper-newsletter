date: "2025-10-12"
author: Zichun Yu
title: 'RePro: Training Language Models to Faithfully Recycle the Web for   Pretraining'
thumbnail: ""
link: https://huggingface.co/papers/2510.10681
summary: The researchers present a new method called RePro that uses a small language model to rephrase pretraining data in a faithful and effective way, improving the performance of large language models by 4.7%-14.0% on various tasks, and making the most of limited high-quality pretraining data....
opinion: placeholder
tags:
    - ML
