author: Berivan Isik
date: '2024-02-07'
link: https://huggingface.co/papers/2402.04177
opinion: placeholder
summary: This paper studies the scaling behavior of large language models (LLMs) in
  a transfer learning setting and finds that the choice of pretraining data and its
  size greatly influence downstream performance (translation quality). It also suggests
  that sufficient alignment between pretraining and downstream data is necessary for
  good performance....
tags:
- Natural Language Processing
- Transfer Learning
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/0hOM6hyH6onuLhmYmp6yt.png
title: Scaling Laws for Downstream Task Performance of Large Language Models
translated_paths:
  INT: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.04177/paper.en.html
  KR: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.04177/paper.ko.html
