date: "2024-04-01"
author: Opher Lieber
title: 'Jamba: A Hybrid Transformer-Mamba Language Model'
thumbnail: https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.19887.png
link: https://huggingface.co/papers/2403.19887
summary: The paper introduces Jamba, a new large language model that combines the best features of Transformer and Mamba architectures. This hybrid model is efficient, powerful, and flexible, offering state-of-the-art performance on language model benchmarks and long-context evaluations. Jamba has a small memory footprint and high throughput. Its structure allows for resource- and objective-specific configurations. The model's implementation will be made publicly available under a permissive license....
opinion: placeholder
tags:
    - Deep Learning
    - Natural Language Processing
