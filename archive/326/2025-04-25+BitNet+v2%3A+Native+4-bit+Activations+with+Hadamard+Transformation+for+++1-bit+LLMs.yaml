date: "2025-04-25"
author: Hongyu Wang
title: 'BitNet v2: Native 4-bit Activations with Hadamard Transformation for   1-bit LLMs'
thumbnail: ""
link: https://huggingface.co/papers/2504.18415
summary: This study presents BitNet v2, a new framework that allows for native 4-bit activation quantization in 1-bit Large Language Models (LLMs). The framework uses a module called H-BitLinear, which applies an online Hadamard transformation to smooth activation distributions and reduce memory and computational costs for LLMs, without significant performance loss....
opinion: placeholder
tags:
    - ML
