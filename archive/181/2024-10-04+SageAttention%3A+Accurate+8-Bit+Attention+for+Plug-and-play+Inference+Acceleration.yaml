date: "2024-10-04"
author: Jintao Zhang
title: 'SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration'
thumbnail: ""
link: https://huggingface.co/papers/2410.02367
summary: SageAttention is a new method for quantizing attention in transformer models, which can speed up inference and improve accuracy. It outperforms other methods like FlashAttention2 and xformers by about 2-3 times in speed, and also achieves better accuracy than FlashAttention3. It works well for different types of models, including those used for large language processing, image generation, and video generation, without affecting the overall performance of the models....
opinion: placeholder
tags:
    - ML
