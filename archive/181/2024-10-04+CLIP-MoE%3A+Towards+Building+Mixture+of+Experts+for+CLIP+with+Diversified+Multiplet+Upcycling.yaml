date: "2024-10-04"
author: Jihai Zhang
title: 'CLIP-MoE: Towards Building Mixture of Experts for CLIP with Diversified Multiplet Upcycling'
thumbnail: ""
link: https://huggingface.co/papers/2409.19291
summary: In recent years, Contrastive Language-Image Pre-training (CLIP) has become a cornerstone in multimodal intelligence. However, recent studies have identified that the information loss in the CLIP encoding process is substantial, and CLIP tends to capture only coarse-grained features from the input. This deficiency significantly limits the ability of a single CLIP model to handle images rich in visual detail. In this work, we propose a simple yet effective model-agnostic strategy, Diversified Mult...
opinion: placeholder
tags:
    - ML
