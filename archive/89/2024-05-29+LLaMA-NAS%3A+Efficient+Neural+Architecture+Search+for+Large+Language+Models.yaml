date: "2024-05-29"
author: Anthony Sarah
title: 'LLaMA-NAS: Efficient Neural Architecture Search for Large Language Models'
thumbnail: https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.18377.png
link: https://huggingface.co/papers/2405.18377
summary: The paper proposes a method to find smaller, less computationally complex network architectures for large language models (LLMs) using one-shot NAS and genetic algorithms. This method reduces model size by 1.5x and speeds up throughput by 1.3x for certain tasks with negligible drop in accuracy, and is more effective and efficient than certain pruning or sparsification techniques. Quantization can further decrease the size and complexity of the networks found by this method....
opinion: placeholder
tags:
    - Supervised Learning
    - Optimization and Learning Algorithms
    - Deep Learning
    - Natural Language Processing
