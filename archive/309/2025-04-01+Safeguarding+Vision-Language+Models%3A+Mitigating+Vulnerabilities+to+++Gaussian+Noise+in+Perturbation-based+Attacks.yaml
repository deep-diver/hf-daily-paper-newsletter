date: "2025-04-01"
author: Jiawei Wang
title: 'Safeguarding Vision-Language Models: Mitigating Vulnerabilities to   Gaussian Noise in Perturbation-based Attacks'
thumbnail: ""
link: https://huggingface.co/papers/2504.01308
summary: This study identifies a weakness in Vision-Language Models (VLMs) - their vulnerability to simple perturbations like Gaussian noise due to lack of noise-augmented training, and proposes Robust-VLGuard, a multimodal safety dataset with aligned/misaligned image-text pairs, combined with noise-augmented fine-tuning to counter this issue. For more complex attacks, it introduces DiffPure-VLM, which uses diffusion models to convert adversarial perturbations into Gaussian-like noise, which can be defen...
opinion: placeholder
tags:
    - ML
