date: "2025-02-12"
author: Kiran Kamble
title: 'Expect the Unexpected: FailSafe Long Context QA for Finance'
thumbnail: ""
link: https://huggingface.co/papers/2502.06329
summary: 'The research introduces a new benchmark called FailSafeQA to test the robustness and context-awareness of Large Language Models (LLMs) in finance, focusing on two case studies: Query Failure and Context Failure. The benchmark uses the LLM-as-a-Judge methodology and fine-grained rating criteria, revealing that while some models excel at mitigating input perturbations, they must balance robust answering with avoiding hallucination. The dataset is available at: https://huggingface.co/datasets/Write...'
opinion: placeholder
tags:
    - ML
