date: "2025-05-12"
author: Xiaomi LLM-Core Team
title: 'MiMo: Unlocking the Reasoning Potential of Language Model -- From   Pretraining to Posttraining'
thumbnail: ""
link: https://huggingface.co/papers/2505.07608
summary: The researchers developed a large language model called MiMo-7B, specifically for reasoning tasks, by optimizing its pre-training and post-training stages. They enhanced the data pipeline, used a three-stage data mixing strategy, and trained the model on 25 trillion tokens with a new objective for better performance and speed. In post-training, they created a dataset of 130K problems for reinforcement learning, which led to the final model, MiMo-7B-RL, outperforming even larger models on various...
opinion: placeholder
tags:
    - ML
