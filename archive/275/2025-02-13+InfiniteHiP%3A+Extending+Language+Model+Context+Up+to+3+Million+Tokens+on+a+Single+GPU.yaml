date: "2025-02-13"
author: Heejun Lee
title: 'InfiniteHiP: Extending Language Model Context Up to 3 Million Tokens on a Single GPU'
thumbnail: ""
link: https://huggingface.co/papers/2502.08910
summary: The InfiniteHiP framework is proposed to handle long context lengths in large language models more efficiently, by using a token pruning algorithm and RoPE adjustments. It allows processing up to 3 million tokens on a single GPU, achieving an 18.95x speedup in attention decoding for a 1 million token context....
opinion: placeholder
tags:
    - ML
