date: "2024-02-13"
author: Jakub Krajewski
title: Scaling Laws for Fine-Grained Mixture of Experts
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/tfoNPxO6_eP5zZ76Lti4Z.png
link: https://huggingface.co/papers/2402.07871
summary: This paper analyzes the scaling properties of Mixture of Experts (MoE) models for Large Language Models, introducing a new hyperparameter 'granularity' to control expert size and establishing scaling laws considering training tokens, model size, and granularity. Findings show MoE consistently outperforms dense Transformers, with efficiency gap widening with scaling, and challenges common expert size practice....
opinion: placeholder
tags:
    - Supervised Learning
    - Natural Language Processing
    - Deep Learning
