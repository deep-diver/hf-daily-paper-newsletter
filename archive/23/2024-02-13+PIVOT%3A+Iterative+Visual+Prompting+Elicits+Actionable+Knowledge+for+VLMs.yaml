author: Soroush Nasiriany
date: '2024-02-13'
link: https://huggingface.co/papers/2402.07872
opinion: placeholder
summary: This paper proposes a novel approach called PIVOT that enables Vision Language
  Models to handle spatial tasks such as robotic control and navigation without fine-tuning,
  by casting tasks as iterative visual question answering. PIVOT is investigated on
  various real-world and simulated spatial tasks, showing promising results such as
  zero-shot control of robotic systems without any training data....
tags:
- Deep Learning
- Computer Vision
- Robotics and Control
thumbnail: https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2402.07872.png
title: 'PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs'
translated_paths:
  INT: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.07872/paper.en.html
  KR: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.07872/paper.ko.html
