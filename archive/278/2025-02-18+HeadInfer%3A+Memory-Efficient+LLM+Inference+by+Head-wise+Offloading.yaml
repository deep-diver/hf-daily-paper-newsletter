date: "2025-02-18"
author: Cheng Luo
title: 'HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading'
thumbnail: ""
link: https://huggingface.co/papers/2502.12574
summary: The proposed method HEADINFER reduces the memory footprint of large language models during inference by employing a fine-grained, head-wise offloading strategy to maintain only selective attention heads KV cache on the GPU, while dynamically computing attention output, achieving a 92% reduction compared to the baseline inference....
opinion: placeholder
tags:
    - ML
