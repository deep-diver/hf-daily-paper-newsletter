date: "2024-10-29"
author: Sangmin Bae
title: 'Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA'
thumbnail: ""
link: https://huggingface.co/papers/2410.20672
summary: This paper introduces a method to share parameters across layers in transformers, reducing their size and cost without significantly impacting performance. The paper also proposes a new inference paradigm called Continuous Depth-wise Batching, which could lead to significant gains in inference throughput....
opinion: placeholder
tags:
    - ML
