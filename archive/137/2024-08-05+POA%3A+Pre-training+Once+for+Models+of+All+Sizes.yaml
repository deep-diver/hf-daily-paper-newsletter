date: "2024-08-05"
author: Yingying Zhang
title: 'POA: Pre-training Once for Models of All Sizes'
thumbnail: ""
link: https://huggingface.co/papers/2408.01031
summary: We propose a novel tri-branch self-supervised training framework, POA, to pre-train once for models of all sizes. Our approach introduces an innovative elastic student branch into a modern self-distillation paradigm, allowing the extraction of pre-trained models of diverse sizes for downstream tasks. Extensive experiments demonstrate the effectiveness and advantages of our POA, achieving state-of-the-art performance using ViT, Swin Transformer, and ResNet backbones. The code is available at http...
opinion: placeholder
tags:
    - ML
