date: "2024-10-16"
author: Hojoon Lee
title: 'SimBa: Simplicity Bias for Scaling Up Parameters in Deep Reinforcement Learning'
thumbnail: ""
link: https://huggingface.co/papers/2410.09754
summary: 'SimBa is a new architecture for deep reinforcement learning that helps models to generalize better by using a simplicity bias. It has three components: observation normalization, residual feedforward block, and layer normalization. Using SimBa can improve the sample efficiency of various deep RL algorithms and outperform other methods with high computational efficiency....'
opinion: placeholder
tags:
    - ML
