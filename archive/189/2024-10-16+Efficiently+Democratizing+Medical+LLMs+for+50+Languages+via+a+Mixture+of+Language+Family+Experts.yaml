date: "2024-10-16"
author: Guorui Zheng
title: Efficiently Democratizing Medical LLMs for 50 Languages via a Mixture of Language Family Experts
thumbnail: ""
link: https://huggingface.co/papers/2410.10626
summary: This paper proposes a method to adapt medical Large Language Models to local languages by constructing a high-quality medical dataset and using a Mixture of Experts (MoE) modularity to leverage the generalization capability of multilingual LLMs. The proposed method employs language-specific experts and cross-lingual routing, and it reveals a Spread Out in the End information flow mechanism. The Post-MoE architecture is developed based on this insight, and it enhances the generalization of multil...
opinion: placeholder
tags:
    - ML
