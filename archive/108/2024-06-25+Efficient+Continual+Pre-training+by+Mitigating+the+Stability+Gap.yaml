date: "2024-06-25"
author: Yiduo Guo
title: Efficient Continual Pre-training by Mitigating the Stability Gap
thumbnail: ""
link: https://huggingface.co/papers/2406.14833
summary: This paper presents strategies to improve the performance of Large Language Models during continual pre-training, leading to faster recovery, better domain performance, and reduced forgetting. Experiments on Llama-family models show improved performance on medical and general tasks, and the proposed strategies are applied to the Llama-3-8B model, resulting in the Llama-3-Physician model, which outperforms current open-source models in medical tasks and compares favorably with GPT-4 on several me...
opinion: placeholder
tags:
    - ML
