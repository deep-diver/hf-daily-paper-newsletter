date: "2024-06-25"
author: Xiaochen Li
title: Preference Tuning For Toxicity Mitigation Generalizes Across Languages
thumbnail: ""
link: https://huggingface.co/papers/2406.16235
summary: This paper explores the zero-shot cross-lingual generalization of preference tuning in detoxifying multilingual Large Language Models (LLMs). The training method, called Direct Preference Optimization (DPO), significantly reduces toxicity in multilingual open-ended generations across 17 different languages. The method also works for other multilingual LLMs, and the cross-lingual generalization of DPO is explained by the dual multilinguality property of MLP layers in LLMs. Additionally, bilingual...
opinion: placeholder
tags:
    - ML
