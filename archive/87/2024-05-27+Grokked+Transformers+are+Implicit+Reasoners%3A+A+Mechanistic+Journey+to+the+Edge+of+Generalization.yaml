date: "2024-05-27"
author: Boshi Wang
title: 'Grokked Transformers are Implicit Reasoners: A Mechanistic Journey to the Edge of Generalization'
thumbnail: https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.15071.png
link: https://huggingface.co/papers/2405.15071
summary: Transformers can implicitly reason through extended training, but their generalization varies depending on the reasoning type. The study reveals the mechanism behind grokking and its connection to systematicity, suggesting improvements to the transformer architecture and the power of parametric memory for complex reasoning....
opinion: placeholder
tags:
    - Supervised Learning
    - Deep Learning
    - Optimization and Learning Algorithms
    - Explainable AI and Interpretability
