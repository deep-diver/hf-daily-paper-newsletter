date: "2024-05-27"
author: Ce Ge
title: 'Data Mixing Made Efficient: A Bivariate Scaling Law for Language Model Pretraining'
thumbnail: https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.14908.png
link: https://huggingface.co/papers/2405.14908
summary: This paper proposes a unified scaling law called BiMix to study the bivariate scaling behaviors of data quantity and mixing proportions in language model pretraining. It finds that entropy-driven training-free data mixtures can achieve comparable or even better performance than more resource-intensive methods, providing insights for cost-effective language modeling....
opinion: placeholder
tags:
    - Natural Language Processing
