date: "2025-06-10"
author: Yizhao Gao
title: 'SeerAttention-R: Sparse Attention Adaptation for Long Reasoning'
thumbnail: ""
link: https://huggingface.co/papers/2506.08889
summary: The researchers present a new sparse attention framework called SeerAttention-R, designed for long reasoning tasks. This framework can be easily integrated into existing pretrained models without altering their original parameters, and it achieves near-lossless reasoning accuracy with large sparse attention block sizes, up to 9x faster than FlashAttention-3 on H100 GPU at 90% sparsity....
opinion: placeholder
tags:
    - ML
