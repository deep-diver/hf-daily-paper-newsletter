date: "2024-07-25"
author: Jiaheng Liu
title: 'DDK: Distilling Domain Knowledge for Efficient Large Language Models'
thumbnail: ""
link: https://huggingface.co/papers/2407.16154
summary: This paper proposes DDK, a new framework for LLM distillation that adjusts the distillation dataset based on the domain performance differences between the teacher and student models. DDK improves the performance of student models and outperforms existing knowledge distillation methods....
opinion: placeholder
tags:
    - ML
