date: "2024-07-16"
author: Haiwen Diao
title: 'SHERL: Synthesizing High Accuracy and Efficient Memory for Resource-Limited Transfer Learning'
thumbnail: ""
link: https://huggingface.co/papers/2407.07523
summary: SHERL is a new method for transfer learning that reduces the amount of memory needed for fine-tuning pre-trained models without sacrificing accuracy. It does this by using a two-step process to make the models more flexible and powerful for new tasks. SHERL performs as well or better than other methods that reduce the number of trainable parameters, while using less memory during fine-tuning. The code for SHERL is available online....
opinion: placeholder
tags:
    - ML
