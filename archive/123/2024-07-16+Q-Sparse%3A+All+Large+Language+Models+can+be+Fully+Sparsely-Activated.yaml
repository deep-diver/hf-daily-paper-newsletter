date: "2024-07-16"
author: Hongyu Wang
title: 'Q-Sparse: All Large Language Models can be Fully Sparsely-Activated'
thumbnail: ""
link: https://huggingface.co/papers/2407.10969
summary: This paper presents Q-Sparse, a method for training sparsely-activated large language models. It allows for full sparsity of activations, leading to more efficient inference. The method involves top-K sparsification and the straight-through-estimator. Q-Sparse can achieve results similar to baseline LLMs while being more efficient. It works for different settings and both full-precision and 1-bit LLMs. The combination of Q-Sparse and BitNet b1.58 provides a path to revolutionize the efficiency o...
opinion: placeholder
tags:
    - ML
