date: "2024-10-11"
author: Zheyang Xiong
title: 'Everything Everywhere All at Once: LLMs can In-Context Learn Multiple Tasks in Superposition'
thumbnail: ""
link: https://huggingface.co/papers/2410.05603
summary: Large language models (LLMs) can perform multiple tasks simultaneously during inference, a phenomenon called 'task superposition'. This capability emerges even when trained to learn one task at a time and is well within the expressive power of transformers. Larger models can solve more tasks in parallel and better calibrate their output distribution. Our findings offer insights into the latent capabilities of LLMs and raise questions about the mechanisms enabling simultaneous task execution....
opinion: placeholder
tags:
    - ML
