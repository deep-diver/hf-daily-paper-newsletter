date: "2024-04-16"
author: Alexey Gorbatovski
title: Learn Your Reference Model for Real Good Alignment
thumbnail: https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.09656.png
link: https://huggingface.co/papers/2404.09656
summary: The authors propose a new method called Trust Region DPO (TR-DPO) that updates the reference policy during training to improve the quality of models across several parameters. They demonstrate the effectiveness of TR-DPO against Direct Preference Optimization (DPO) on the Anthropic HH and TLDR datasets and show that TR-DPO outperforms DPO by up to 19%....
opinion: placeholder
tags:
    - Reinforcement Learning
