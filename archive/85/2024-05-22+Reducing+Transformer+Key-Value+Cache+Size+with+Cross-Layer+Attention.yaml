date: "2024-05-22"
author: William Brandon
title: Reducing Transformer Key-Value Cache Size with Cross-Layer Attention
thumbnail: https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.12981.png
link: https://huggingface.co/papers/2405.12981
summary: Cross-Layer Attention (CLA) is a new attention design that shares key and value heads between adjacent layers, reducing the size of the key-value cache by 2x while maintaining accuracy. This enables longer sequence lengths and larger batch sizes during inference, providing a Pareto improvement over traditional Multi-Query Attention (MQA)....
opinion: placeholder
tags:
    - Deep Learning
    - Optimization and Learning Algorithms
    - Natural Language Processing
