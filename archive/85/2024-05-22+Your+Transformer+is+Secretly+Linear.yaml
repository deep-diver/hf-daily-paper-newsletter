date: "2024-05-22"
author: Anton Razzhigaev
title: Your Transformer is Secretly Linear
thumbnail: https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.12250.png
link: https://huggingface.co/papers/2405.12250
summary: This paper discovers that transformer decoders, including popular models like GPT, LLaMA, OPT, and BLOOM, have a hidden linear characteristic. The study reveals a near-perfect linear relationship between sequential layers, which is affected by the residual component's output norm. Removing or approximating the most linear blocks does not significantly impact performance, and a new regularization method improves benchmark results while reducing linearity....
opinion: placeholder
tags:
    - Deep Learning
    - Natural Language Processing
