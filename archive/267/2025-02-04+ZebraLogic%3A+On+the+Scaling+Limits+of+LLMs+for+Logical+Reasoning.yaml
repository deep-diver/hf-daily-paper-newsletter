date: "2025-02-04"
author: Bill Yuchen Lin
title: 'ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning'
thumbnail: ""
link: https://huggingface.co/papers/2502.01100
summary: The paper presents ZebraLogic, a new evaluation framework for assessing the logical reasoning abilities of large language models (LLMs) in complex, non-monotonic reasoning. ZebraLogic generates puzzles with controllable complexity and tests models like Llama and DeepSeek-R1, revealing a 'curse of complexity' where accuracy drops as problem complexity grows. The study also explores strategies like Best-of-N sampling to improve logical reasoning....
opinion: placeholder
tags:
    - ML
