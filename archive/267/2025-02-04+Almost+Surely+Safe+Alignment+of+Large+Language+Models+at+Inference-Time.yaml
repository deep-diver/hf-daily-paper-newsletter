date: "2025-02-04"
author: Xiaotong Ji
title: Almost Surely Safe Alignment of Large Language Models at Inference-Time
thumbnail: ""
link: https://huggingface.co/papers/2502.01208
summary: This research presents a method to make large language models (LLMs) generate safe responses during inference, almost surely. This is achieved by treating safe response generation as a constrained Markov decision process within the LLM's latent space and introducing a safety state to track safety constraints, ensuring formal safety guarantees....
opinion: placeholder
tags:
    - ML
