date: "2025-08-21"
author: Xiaojuan Tang
title: 'TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated   Prefill \& Decode Inference'
thumbnail: ""
link: https://huggingface.co/papers/2508.15881
summary: The authors present Tensor-Parallel Latent Attention (TPLA), a method that divides the latent representation and input dimensions among devices to enhance efficiency in tensor parallelism, while preserving the benefits of a compressed cache. TPLA is compatible with existing models and reduces the per-device cache, resulting in faster performance without sacrificing accuracy....
opinion: placeholder
tags:
    - ML
