date: "2024-02-08"
author: Abhimanyu Rajeshkumar Bambhaniya
title: Progressive Gradient Flow for Robust N:M Sparsity Training in Transformers
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/r6FHVJ8gCAZ9tvuIAAnso.png
link: https://huggingface.co/papers/2402.04744
summary: This paper presents a new approach, called Progressive Gradient Flow, to train transformers with high N:M structured sparsity. The method improves model quality by up to 5% in language models and 2% in vision models at high sparsity levels, and reduces the training compute cost at iso-training FLOPs, resulting in up to a 2% improvement in accuracy....
opinion: placeholder
tags:
    - Optimization and Learning Algorithms
    - Deep Learning
    - Natural Language Processing
    - Computer Vision
