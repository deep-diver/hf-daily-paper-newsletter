date: "2024-02-08"
author: Michael Zhang
title: 'The Hedgehog & the Porcupine: Expressive Linear Attentions with Softmax Mimicry'
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/uClxXWYMEayIgucmXY5mN.png
link: https://huggingface.co/papers/2402.04347
summary: The paper proposes a new type of attention called Hedgehog, which improves upon prior linear attentions by mimicking the properties of standard softmax attention while maintaining the computational efficiency of linear attentions. Hedgehog is able to achieve near-state-of-the-art performance on several NLP tasks and enables efficient conversion of large models into linear versions....
opinion: placeholder
tags:
    - Supervised Learning
    - Natural Language Processing
    - Deep Learning
