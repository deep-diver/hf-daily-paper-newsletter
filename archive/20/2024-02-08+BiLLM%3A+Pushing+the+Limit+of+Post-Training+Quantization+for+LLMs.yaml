date: "2024-02-08"
author: Wei Huang
title: 'BiLLM: Pushing the Limit of Post-Training Quantization for LLMs'
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/SVbBf7vhT-Dh266xo-B2J.png
link: https://huggingface.co/papers/2402.04291
summary: The paper presents BiLLM, a groundbreaking 1-bit post-training quantization scheme tailored for pretrained large language models (LLMs). It significantly reduces the number of model weights to 1 bit while maintaining high accuracy inference and outperforms state-of-the-art quantization methods for LLMs....
opinion: placeholder
tags:
    - Deep Learning
    - Optimization and Learning Algorithms
    - Natural Language Processing
