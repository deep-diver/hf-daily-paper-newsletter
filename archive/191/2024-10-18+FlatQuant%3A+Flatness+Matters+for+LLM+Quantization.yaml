date: "2024-10-18"
author: Yuxuan Sun
title: 'FlatQuant: Flatness Matters for LLM Quantization'
thumbnail: ""
link: https://huggingface.co/papers/2410.09426
summary: FlatQuant is a new post-training quantization approach that enhances the flatness of weights and activations in large language models (LLMs). It identifies optimal affine transformations tailored to each linear layer and reduces runtime overhead. Extensive experiments show that FlatQuant sets up a new state-of-the-art quantization benchmark, reducing inference latency and achieving less than 1% accuracy drop for W4A4 quantization on the LLaMA-3-70B model....
opinion: placeholder
tags:
    - ML
