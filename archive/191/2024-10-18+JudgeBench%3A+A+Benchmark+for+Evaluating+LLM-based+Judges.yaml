date: "2024-10-18"
author: Sijun Tan
title: 'JudgeBench: A Benchmark for Evaluating LLM-based Judges'
thumbnail: ""
link: https://huggingface.co/papers/2410.12784
summary: The abstract introduces JudgeBench, a benchmark for evaluating LLM-based judges, which are used to assess and improve models. The benchmark focuses on challenging tasks where human preference is not a good indicator of correctness. The authors propose a novel evaluation framework and show that JudgeBench is a greater challenge than previous benchmarks, with many strong models performing just slightly better than random guessing....
opinion: placeholder
tags:
    - ML
