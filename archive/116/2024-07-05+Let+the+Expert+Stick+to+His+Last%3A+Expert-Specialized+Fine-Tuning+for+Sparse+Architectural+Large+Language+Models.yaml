date: "2024-07-05"
author: Zihan Wang
title: 'Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models'
thumbnail: ""
link: https://huggingface.co/papers/2407.01906
summary: This paper explores how to fine-tune large language models with a specific architecture called Mixture-of-Experts (MoE), which can improve the model's performance and efficiency. It proposes a new method called Expert-Specialized Fine-Tuning (ESFT) that focuses on tuning the most relevant experts for a specific task, while freezing the others. The paper also analyzes how the MoE architecture affects the expert-specialized fine-tuning process, finding that finer-grained experts can improve both t...
opinion: placeholder
tags:
    - ML
