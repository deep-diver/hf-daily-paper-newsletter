date: "2024-06-14"
author: Andrew Jesson
title: Estimating the Hallucination Rate of Generative AI
thumbnail: ""
link: https://huggingface.co/papers/2406.07457
summary: This paper introduces a new method for estimating the probability of generative AI models, like large language models, producing incorrect or untrue predictions (hallucinations) when given a dataset and asked to make a prediction. The method involves generating queries and responses from the model and evaluating its response log probability. The paper also presents empirical evaluations of the method on synthetic regression and natural language ICL tasks....
opinion: placeholder
tags:
    - ML
