author: Eric Zelikman
date: '2024-03-15'
link: https://huggingface.co/papers/2403.09629
opinion: placeholder
summary: The paper proposes Quiet-STaR, a method for language models to learn to reason
  by generating rationales for each token in text. This improves the models' ability
  to predict difficult-to-predict tokens and answer difficult questions, without fine-tuning
  on specific tasks....
tags:
- Deep Learning
- Natural Language Processing
thumbnail: https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.09629.png
title: 'Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking'
translated_paths:
  INT: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2403.09629/paper.en.html
  KR: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2403.09629/paper.ko.html
