date: "2025-03-22"
author: Zeyu Liu
title: 'CODA: Repurposing Continuous VAEs for Discrete Tokenization'
thumbnail: ""
link: https://huggingface.co/papers/2503.17760
summary: CODA is a framework that adapts continuous VAEs into discrete tokenizers by decoupling compression and discretization, resulting in stable and efficient training while maintaining high visual fidelity. It achieves 100% codebook utilization and low rFID for 8 times and 16 times compression on ImageNet 256x256 benchmark with 6 times less training budget than VQGAN....
opinion: placeholder
tags:
    - ML
