date: "2025-03-24"
author: Edoardo Debenedetti
title: Defeating Prompt Injections by Design
thumbnail: ""
link: https://huggingface.co/papers/2503.18813
summary: The proposed CaMeL system is a defense mechanism protecting Large Language Models (LLMs) against prompt injection attacks. By separating the control and data flows from the trusted query, it prevents untrusted data from affecting the program flow and uses a capability concept to prevent unauthorized data exfiltration....
opinion: placeholder
tags:
    - ML
