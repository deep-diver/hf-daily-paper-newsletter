date: "2025-01-13"
author: Yifan Zhang
title: Tensor Product Attention Is All You Need
thumbnail: ""
link: https://huggingface.co/papers/2501.06425
summary: This paper introduces Tensor Product Attention (TPA), a new attention mechanism that uses tensor decompositions to represent queries, keys, and values compactly, reducing memory overhead during inference. TPA is integrated with RoPE and used to create the Tensor ProducT ATTenTion Transformer (T6) model architecture. T6 outperforms standard Transformer baselines across various language modeling tasks and allows processing longer sequences with fixed resource constraints....
opinion: placeholder
tags:
    - ML
