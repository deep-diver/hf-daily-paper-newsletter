date: "2025-07-22"
author: Helena Casademunt
title: Steering Out-of-Distribution Generalization with Concept Ablation   Fine-Tuning
thumbnail: ""
link: https://huggingface.co/papers/2507.16795
summary: The authors present a new method called Concept Ablation Fine-Tuning (CAFT) that helps control the generalization of large language models (LLMs) during fine-tuning, without altering the training data or using data from the target distribution. By identifying and eliminating undesired concepts in the LLM's latent space, CAFT reduces misaligned responses by 10 times in various fine-tuning tasks, including emergent misalignment, without hurting performance on the training data....
opinion: placeholder
tags:
    - ML
