date: "2024-10-30"
author: Hanshi Sun
title: 'ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference'
thumbnail: ""
link: https://huggingface.co/papers/2410.21465
summary: ShadowKV is a system that reduces the memory footprint and improves the throughput of long-context large language models by storing the key cache and offloading the value cache, and using an accurate KV selection strategy. It supports larger batch sizes and boosts throughput by up to 3.04 times on an A100 GPU without sacrificing accuracy....
opinion: placeholder
tags:
    - ML
