date: "2025-09-08"
author: Praneet Suresh
title: 'From Noise to Narrative: Tracing the Origins of Hallucinations in   Transformers'
thumbnail: ""
link: https://huggingface.co/papers/2509.06938
summary: This study explores why and when AI models like transformers make up information (hallucinate) by using sparse autoencoders to understand their concept representation. They found that as input information becomes more disorganized, transformers are more likely to create coherent but irrelevant concepts, leading to hallucinations. The research has implications for AI safety, aligning models with human values, and understanding potential adversarial attacks....
opinion: placeholder
tags:
    - ML
