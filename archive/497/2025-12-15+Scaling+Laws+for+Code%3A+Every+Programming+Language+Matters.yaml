date: "2025-12-15"
author: Jian Yang
title: 'Scaling Laws for Code: Every Programming Language Matters'
thumbnail: ""
link: https://huggingface.co/papers/2512.13472
summary: This study investigates how different programming languages affect the performance of code large language models during pre-training, revealing that interpretated languages benefit more from scaled-up models and data compared to compiled languages. The research also finds that multilingual pre-training and a specific pre-training strategy improve cross-lingual abilities, leading to a new scaling law for optimally allocating training tokens across languages....
opinion: placeholder
tags:
    - ML
