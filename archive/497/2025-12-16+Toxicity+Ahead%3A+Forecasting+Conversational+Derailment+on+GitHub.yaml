date: "2025-12-16"
author: Mia Mohammad Imran
title: 'Toxicity Ahead: Forecasting Conversational Derailment on GitHub'
thumbnail: ""
link: https://huggingface.co/papers/2512.15031
summary: Researchers collected data from GitHub discussions to understand how toxic conversations develop and created a new system using large language models to predict when a conversation might turn toxic. This system, which uses a two-step prompting process, was able to accurately identify toxic conversations in various tests, providing a more scalable approach for moderators to prevent such interactions in open-source software communities....
opinion: placeholder
tags:
    - ML
