author: Aonan Zhang
date: '2024-03-18'
link: https://huggingface.co/papers/2403.09919
opinion: placeholder
summary: This paper proposes a new method for fast speculative decoding in large language
  models by combining the strengths of two existing techniques. The method uses a
  single, lightweight draft head with a recurrent dependency design, which allows
  for efficient filtering of undesired candidates. Empirical results show the effectiveness
  of the method on several popular language models....
tags:
- Natural Language Processing
- Deep Learning
- Optimization and Learning Algorithms
thumbnail: https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.09919.png
title: Recurrent Drafter for Fast Speculative Decoding in Large Language Models
translated_paths:
  INT: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2403.09919/paper.en.html
  KR: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2403.09919/paper.ko.html
