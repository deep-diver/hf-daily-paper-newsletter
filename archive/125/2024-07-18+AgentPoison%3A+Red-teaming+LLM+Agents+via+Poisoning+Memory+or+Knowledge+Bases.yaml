date: "2024-07-18"
author: Zhaorun Chen
title: 'AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases'
thumbnail: ""
link: https://huggingface.co/papers/2407.12784
summary: AgentPoison is a new method for attacking language models by adding bad information to their memory or knowledge base. This makes the models more likely to give wrong answers when they see a special trigger word, while still working correctly most of the time....
opinion: placeholder
tags:
    - ML
