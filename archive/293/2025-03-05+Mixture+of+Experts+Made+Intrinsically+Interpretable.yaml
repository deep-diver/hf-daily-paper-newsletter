date: "2025-03-05"
author: Xingyi Yang
title: Mixture of Experts Made Intrinsically Interpretable
thumbnail: ""
link: https://huggingface.co/papers/2503.07639
summary: The paper introduces MoE-X, a language model using Mixture-of-Experts architecture designed to be intrinsically interpretable by aligning with interpretability objectives through sparse activations and expert selection. MoE-X achieves performance comparable to dense models and surpasses interpretability of SAE-based approaches....
opinion: placeholder
tags:
    - ML
