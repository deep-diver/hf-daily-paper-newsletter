date: "2025-09-28"
author: Zeqing Wang
title: 'SparseD: Sparse Attention for Diffusion Language Models'
thumbnail: ""
link: https://huggingface.co/papers/2509.24014
summary: The authors present SparseD, a new method for reducing inference latency in diffusion language models (DLMs). SparseD addresses the unique sparsity behaviors in DLMs by reusing head-specific sparse patterns and using full attention in early steps, resulting in efficient and practical DLM deployment for long-context applications....
opinion: placeholder
tags:
    - ML
