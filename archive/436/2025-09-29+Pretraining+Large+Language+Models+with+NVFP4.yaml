date: "2025-09-29"
author: NVIDIA
title: Pretraining Large Language Models with NVFP4
thumbnail: ""
link: https://huggingface.co/papers/2509.25149
summary: The study presents a new method for efficiently training large language models using NVFP4 format, which improves computational speed and resource utilization. The approach involves techniques like Random Hadamard transforms, two-dimensional quantization, and selective high-precision layers, resulting in a model with comparable performance to an 8-bit floating point baseline, while using significantly less compute and energy....
opinion: placeholder
tags:
    - ML
