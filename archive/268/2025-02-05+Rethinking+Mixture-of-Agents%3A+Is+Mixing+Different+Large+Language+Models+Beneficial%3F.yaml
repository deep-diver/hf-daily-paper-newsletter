date: "2025-02-05"
author: Wenzhe Li
title: 'Rethinking Mixture-of-Agents: Is Mixing Different Large Language Models Beneficial?'
thumbnail: ""
link: https://huggingface.co/papers/2502.00674
summary: This study explores the benefits of mixing different Large Language Models (LLMs) and introduces 'Self-MoA', an ensemble method that aggregates outputs only from the top-performing LLM. Experiments reveal that Self-MoA outperforms standard MoA in many scenarios, including a 6.6% improvement on the AlpacaEval 2.0 benchmark and an average of 3.8% improvement across other benchmarks. The research also looks into the sensitivity of MoA performance towards the quality of outputs....
opinion: placeholder
tags:
    - ML
