date: "2025-02-18"
author: Jingyang Yuan
title: 'Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention'
thumbnail: ""
link: https://huggingface.co/papers/2502.11089
summary: The paper introduces NSA, a hardware-aligned and natively trainable sparse attention mechanism designed for long-context modeling. NSA combines coarse-grained token compression and fine-grained token selection using a dynamic hierarchical sparse strategy and achieves substantial speedups through arithmetic intensity-balanced algorithm design, enabling end-to-end training for long-context tasks and instruction-based reasoning....
opinion: placeholder
tags:
    - ML
