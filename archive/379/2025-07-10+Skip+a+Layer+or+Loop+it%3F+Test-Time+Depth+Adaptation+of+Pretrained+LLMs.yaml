date: "2025-07-10"
author: Ziyue Li
title: Skip a Layer or Loop it? Test-Time Depth Adaptation of Pretrained LLMs
thumbnail: ""
link: https://huggingface.co/papers/2507.07996
summary: The study explores whether pretrained neural networks can adjust their structure for various inputs without additional training. By treating each layer of a large language model as a separate component, the researchers created a more efficient and customizable model for each test sample, which they call chain-of-layers (CoLa). They used a Monte Carlo Tree Search protocol to find the best CoLa for each sample from math and commonsense reasoning benchmarks, resulting in more flexible and dynamic a...
opinion: placeholder
tags:
    - ML
