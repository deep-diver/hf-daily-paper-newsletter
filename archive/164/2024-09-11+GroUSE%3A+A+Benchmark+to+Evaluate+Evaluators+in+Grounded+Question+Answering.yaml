date: "2024-09-11"
author: Sacha Muller
title: 'GroUSE: A Benchmark to Evaluate Evaluators in Grounded Question Answering'
thumbnail: ""
link: https://huggingface.co/papers/2409.06595
summary: A new benchmark called GroUSE is introduced to evaluate the performance of grounded question answering systems. The benchmark reveals that existing evaluation frameworks often overlook important failures modes, even when using GPT-4 as a judge. To improve this, a novel pipeline is proposed and it is found that finetuning Llama-3 on GPT-4's reasoning traces can significantly boost its evaluation capabilities....
opinion: placeholder
tags:
    - ML
