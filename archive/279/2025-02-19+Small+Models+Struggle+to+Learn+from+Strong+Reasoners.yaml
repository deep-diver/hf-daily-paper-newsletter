date: "2025-02-19"
author: Yuetai Li
title: Small Models Struggle to Learn from Strong Reasoners
thumbnail: ""
link: https://huggingface.co/papers/2502.12143
summary: The research discusses the Small Model Learnability Gap, where smaller language models (leq3B parameters) don't improve from long reasoning or distillation from larger models. Instead, they benefit from shorter, simpler reasoning chains that match their learning capacity. To tackle this, the study presents Mix Distillation, a strategy that combines different reasoning chain lengths or models. Experiments show that Mix Distillation enhances smaller model reasoning performance compared to training...
opinion: placeholder
tags:
    - ML
