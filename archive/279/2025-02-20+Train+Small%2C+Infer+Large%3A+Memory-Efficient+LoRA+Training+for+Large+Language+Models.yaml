date: "2025-02-20"
author: Jun Zhang
title: 'Train Small, Infer Large: Memory-Efficient LoRA Training for Large Language Models'
thumbnail: ""
link: https://huggingface.co/papers/2502.13533
summary: The authors present LoRAM, a memory-efficient training scheme for LoRA, which addresses the high memory footprint of LoRA by training on a pruned model and recovering the results for inference on the original model. The method, QLoRAM, achieves a 15.81x (16.95x) reduction in parameter storage cost for LLaMA-3.1-70B (LLaMA-2-70B) and outperforms both the original model and LoRA-trained LLaMA-3.1-8B (LLaMA-2-13B)....
opinion: placeholder
tags:
    - ML
