author: Zhengbao Jiang
date: '2024-02-21'
link: https://huggingface.co/papers/2402.12847
opinion: placeholder
summary: The paper proposes a new method called pre-instruction-tuning (PIT) that
  improves the ability of large language models (LLMs) to learn new information by
  first training them on question-answer pairs before continuing to train on complex
  documents. This method outperforms the standard instruction-tuning method by 17.8%....
tags:
- Natural Language Processing
- Deep Learning
- Supervised Learning
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/8mfynPVJrF-DhLkflr77T.png
title: Instruction-tuned Language Models are Better Knowledge Learners
translated_paths:
  INT: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.12847/paper.en.html
  KR: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.12847/paper.ko.html
