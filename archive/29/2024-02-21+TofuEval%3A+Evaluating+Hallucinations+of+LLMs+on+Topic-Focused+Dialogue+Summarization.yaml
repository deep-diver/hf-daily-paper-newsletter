author: Liyan Tang
date: '2024-02-21'
link: https://huggingface.co/papers/2402.13249
opinion: placeholder
summary: This study evaluates the factual consistency of LLMs in topic-focused dialogue
  summarization and finds that existing LLMs hallucinate significant amounts of factual
  errors, regardless of the model's size. The study also shows that LLMs perform poorly
  as binary factual evaluators and that non-LLM based metrics can capture all error
  types better than LLM-based evaluators....
tags:
- Natural Language Processing
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/PBfJ7ChEbHACyakpBwPSd.png
title: 'TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue Summarization'
translated_paths:
  INT: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.13249/paper.en.html
  KR: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.13249/paper.ko.html
