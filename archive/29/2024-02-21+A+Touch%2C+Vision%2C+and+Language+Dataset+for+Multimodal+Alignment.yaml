author: Letian Fu
date: '2024-02-21'
link: https://huggingface.co/papers/2402.13232
opinion: placeholder
summary: This paper introduces a new dataset of 44K in-the-wild vision-touch pairs
  with English language labels annotated by humans and textual pseudo-labels from
  GPT-4V. They use this dataset to train a touch-vision-language model for text generation,
  which improves touch-vision-language alignment and visual-tactile understanding
  over existing models....
tags:
- Computer Vision
- Natural Language Processing
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/AfQoeNvvWP_Z5vGDmmU43.png
title: A Touch, Vision, and Language Dataset for Multimodal Alignment
translated_paths:
  INT: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.13232/paper.en.html
  KR: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.13232/paper.ko.html
