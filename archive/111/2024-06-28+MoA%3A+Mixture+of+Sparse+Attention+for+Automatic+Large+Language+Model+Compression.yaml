date: "2024-06-28"
author: Tianyu Fu
title: 'MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression'
thumbnail: ""
link: https://huggingface.co/papers/2406.14909
summary: MoA is a method that customizes sparse attention configurations for different attention heads and layers in Large Language Models (LLMs). It improves accuracy and throughput while reducing memory usage, outperforming uniform sparse attention methods by 1.5-7.1 times and achieving a 5.5-6.7 times boost in decode throughput on a single GPU....
opinion: placeholder
tags:
    - ML
