date: "2025-09-25"
author: Junu Kim
title: 'Behind RoPE: How Does Causal Mask Encode Positional Information?'
thumbnail: ""
link: https://huggingface.co/papers/2509.21042
summary: The study reveals that the causal mask in Transformer decoders, in addition to RoPE, impacts attention scores and introduces positional information. The research shows that the causal mask favors nearby query-key pairs, which is similar to common positional encodings, and this effect is also present in modern large language models....
opinion: placeholder
tags:
    - ML
