date: "2025-05-20"
author: Haoran Zhao
title: Let LLMs Break Free from Overthinking via Self-Braking Tuning
thumbnail: ""
link: https://huggingface.co/papers/2505.14604
summary: The study presents a new framework called Self-Braking Tuning that helps large reasoning models reduce redundant thinking and computational overhead by allowing the models to control their own reasoning process, without relying on external interventions. The framework uses overthinking identification metrics and a braking prompt mechanism to help models learn when to stop thinking, resulting in up to 60% token consumption reduction while maintaining accuracy on mathematical benchmarks....
opinion: placeholder
tags:
    - ML
