date: "2024-06-12"
author: Tianwen Wei
title: 'Skywork-MoE: A Deep Dive into Training Techniques for Mixture-of-Experts Language Models'
thumbnail: ""
link: https://huggingface.co/papers/2406.06563
summary: In this technical report, we introduce the training methodologies implemented in the development of Skywork-MoE, a high-performance mixture-of-experts (MoE) large language model (LLM) with 146 billion parameters and 16 experts. It is initialized from the pre-existing dense checkpoints of our Skywork-13B model. We explore the comparative effectiveness of upcycling versus training from scratch initializations. Our findings suggest that the choice between these two approaches should consider both t...
opinion: placeholder
tags:
    - ML
