date: "2024-10-28"
author: Shuhao Gu
title: 'Infinity-MM: Scaling Multimodal Performance with Large-Scale and High-Quality Instruction Data'
thumbnail: ""
link: https://huggingface.co/papers/2410.18558
summary: This paper introduces Infinity-MM, a large-scale multimodal instruction dataset with 40 million samples, and Aquila-VL-2B, a 2-billion-parameter VLM trained using this data, achieving SOTA performance for models of similar scale. This shows that increasing instruction data and generating synthetic data can improve open-source model performance....
opinion: placeholder
tags:
    - ML
