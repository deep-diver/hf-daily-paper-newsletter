date: "2024-10-28"
author: Yujian Liu
title: Fictitious Synthetic Data Can Improve LLM Factuality via Prerequisite Learning
thumbnail: ""
link: https://huggingface.co/papers/2410.19290
summary: This paper proposes a new fine-tuning strategy called Prereq-Tune to improve the factuality of Large Language Models (LLMs) by separating the learning of skills and knowledge. The strategy introduces an additional learning stage to learn necessary knowledge, allowing subsequent fine-tuning to focus on task skills. Prereq-Tune can also be combined with fictitious synthetic data to enhance the grounding of LLM outputs to their internal knowledge. Experiments show that Prereq-Tune outperforms exist...
opinion: placeholder
tags:
    - ML
