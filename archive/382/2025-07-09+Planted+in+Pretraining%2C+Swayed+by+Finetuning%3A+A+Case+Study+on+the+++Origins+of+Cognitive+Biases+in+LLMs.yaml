date: "2025-07-09"
author: Itay Itzhak
title: 'Planted in Pretraining, Swayed by Finetuning: A Case Study on the   Origins of Cognitive Biases in LLMs'
thumbnail: ""
link: https://huggingface.co/papers/2507.07186
summary: The study examines cognitive biases in language models and finds that these biases are primarily shaped by pretraining, rather than finetuning. The researchers used a two-step approach to analyze the impact of training randomness and dataset differences on bias, concluding that understanding biases in finetuned models requires considering their pretraining origins....
opinion: placeholder
tags:
    - ML
