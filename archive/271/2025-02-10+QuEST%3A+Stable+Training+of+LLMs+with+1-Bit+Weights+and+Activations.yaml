date: "2025-02-10"
author: Andrei Panferov
title: 'QuEST: Stable Training of LLMs with 1-Bit Weights and Activations'
thumbnail: ""
link: https://huggingface.co/papers/2502.05003
summary: The proposed QuEST method enables training large language models with weights and activations in 4-bits or less, providing better accuracy at lower model size compared to FP16, and even allows stable training with 1-bit weights and activations. It achieves this by improving the quantization of continuous distributions and introducing a new trust gradient estimator, and is extendable to sparse representations....
opinion: placeholder
tags:
    - ML
