date: "2025-02-10"
author: Zehua Pei
title: 'CMoE: Fast Carving of Mixture-of-Experts for Efficient LLM Inference'
thumbnail: ""
link: https://huggingface.co/papers/2502.04416
summary: The authors present a new method, CMoE, to efficiently create Mixture-of-Experts (MoE) models from dense ones, reducing inference time in large language models (LLMs) without requiring extensive training data and resources. CMoE works by grouping neurons into experts based on activation rates and creating a routing mechanism with a differentiable process and load balancing, which can be fine-tuned quickly to achieve high performance....
opinion: placeholder
tags:
    - ML
