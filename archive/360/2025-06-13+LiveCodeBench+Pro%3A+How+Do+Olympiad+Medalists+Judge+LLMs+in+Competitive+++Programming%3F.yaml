date: "2025-06-13"
author: Zihan Zheng
title: 'LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive   Programming?'
thumbnail: ""
link: https://huggingface.co/papers/2506.11928
summary: This study compares large language models (LLMs) to elite human programmers in competitive programming, using a new benchmark called LiveCodeBench Pro. The results show that while LLMs can perform well on implementation-heavy problems, they struggle with complex algorithmic reasoning and are outperformed by human experts in difficult tasks....
opinion: placeholder
tags:
    - ML
