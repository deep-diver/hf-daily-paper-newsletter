author: Dejiao Zhang
date: '2024-02-06'
link: https://huggingface.co/papers/2402.01935
opinion: placeholder
summary: This paper introduces a two-stage pretraining scheme that enhances the performance
  of code representation learning on downstream tasks. The model uses a mix of randomness
  and structure aspects of programming language to train encoders, and then uses contrastive
  learning with hard negatives and positives. The findings suggest that a customized
  token-level denoising scheme, hard negatives and positives, and bimodal contrastive
  learning contribute to the success of the model....
tags:
- Deep Learning
- Natural Language Processing
- Optimization and Learning Algorithms
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/UTrwanKFD7-DrbqYNWT5S.png
title: Code Representation Learning At Scale
translated_paths:
  INT: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.01935/paper.en.html
  KR: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.01935/paper.ko.html
