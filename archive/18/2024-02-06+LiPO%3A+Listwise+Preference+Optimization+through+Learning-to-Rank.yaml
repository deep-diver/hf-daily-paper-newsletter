author: Tianqi Liu
date: '2024-02-06'
link: https://huggingface.co/papers/2402.01878
opinion: placeholder
summary: "This paper presents Listwise Preference Optimization (LiPO), a framework\
  \ for aligning language models with human feedback by treating ranking as a listwise\
  \ problem. LiPO is an alternative to traditional Reinforcement Learning from Human\
  \ Feedback (RLHF) and its specific method LiPO-\u03BB performs better than DPO and\
  \ SLiC on two preference alignment tasks...."
tags:
- Supervised Learning
- Reinforcement Learning
- Natural Language Processing
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/dhu1o2CJpvV0x7hP83VB2.png
title: 'LiPO: Listwise Preference Optimization through Learning-to-Rank'
translated_paths:
  INT: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.01878/paper.en.html
  KR: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.01878/paper.ko.html
