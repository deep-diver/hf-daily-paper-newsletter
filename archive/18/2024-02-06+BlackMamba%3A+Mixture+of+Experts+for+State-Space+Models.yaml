author: Quentin Anthony
date: '2024-02-06'
link: https://huggingface.co/papers/2402.01771
opinion: placeholder
summary: This paper presents BlackMamba, a new model that combines the benefits of
  Mamba state-space models (fast and memory-efficient) with Mixture-of-Experts (cheap
  and fast inference). BlackMamba achieves a competitive performance in both language
  modeling and long sequence processing tasks, and is fully trained on a custom dataset
  with both 340M/1.5B and 630M/2.8B weights and checkpoints available open-source.
  Inference code can be found at <https://github.com/Zyphra/BlackMamba>....
tags:
- Supervised Learning
- Deep Learning
- Natural Language Processing
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/_hCUha2m5Pn1dGEZGL_Ze.png
title: 'BlackMamba: Mixture of Experts for State-Space Models'
translated_paths:
  INT: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.01771/paper.en.html
  KR: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.01771/paper.ko.html
