date: "2025-02-06"
author: Yunzhen Feng
title: 'PILAF: Optimal Human Preference Sampling for Reward Modeling'
thumbnail: ""
link: https://huggingface.co/papers/2502.04270
summary: The authors present PILAF, a new approach to sample responses for preference labeling in RLHF, which enhances alignment with the underlying oracle reward. PILAF is theoretically optimized and easy to implement, showing strong performance in iterative and online RLHF settings....
opinion: placeholder
tags:
    - ML
