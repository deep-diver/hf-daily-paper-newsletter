date: "2024-11-26"
author: Shaohan Huang
title: MH-MoE:Multi-Head Mixture-of-Experts
thumbnail: ""
link: https://huggingface.co/papers/2411.16205
summary: MH-MoE is a new model that uses multiple experts to work together and improve performance. It has the same amount of calculations and parameters as other models, but it works better. It also works well with large language models that use only 1 bit of information....
opinion: placeholder
tags:
    - ML
