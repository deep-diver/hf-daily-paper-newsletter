date: "2025-08-14"
author: Aditya Tomar
title: 'XQuant: Breaking the Memory Wall for LLM Inference with KV Cache   Rematerialization'
thumbnail: ""
link: https://huggingface.co/papers/2508.10395
summary: The study presents XQuant, a method that reduces memory consumption for language model inference by quantizing and caching input activations instead of using standard KV caching. This results in significant memory savings and improved accuracy compared to existing techniques, with up to 7.7 times memory savings and less than 0.1 perplexity degradation. Additionally, XQuant-CL is introduced for extreme compression, achieving up to 12.5 times memory savings with only 0.1 perplexity degradation....
opinion: placeholder
tags:
    - ML
