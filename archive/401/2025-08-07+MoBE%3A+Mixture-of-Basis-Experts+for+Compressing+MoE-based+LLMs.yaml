date: "2025-08-07"
author: Xiaodong Chen
title: 'MoBE: Mixture-of-Basis-Experts for Compressing MoE-based LLMs'
thumbnail: ""
link: https://huggingface.co/papers/2508.05257
summary: The paper presents a new method called MoBE that reduces the size of large language models with the Mixture-of-Experts architecture, using a special decomposition to minimize accuracy loss. Experiments show that MoBE significantly outperforms previous methods, reducing parameter counts by 24%-30% with only 1%-2% accuracy drop....
opinion: placeholder
tags:
    - ML
