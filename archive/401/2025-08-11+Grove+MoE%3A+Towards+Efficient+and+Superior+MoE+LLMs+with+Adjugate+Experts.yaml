date: "2025-08-11"
author: Haoyuan Wu
title: 'Grove MoE: Towards Efficient and Superior MoE LLMs with Adjugate Experts'
thumbnail: ""
link: https://huggingface.co/papers/2508.07785
summary: The authors present Grove MoE, a new architecture for large language models that improves computational efficiency by using experts of varying sizes and a dynamic activation mechanism, resulting in models that perform as well as state-of-the-art models with fewer parameters....
opinion: placeholder
tags:
    - ML
