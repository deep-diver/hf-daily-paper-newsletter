date: "2025-02-11"
author: Wenfang Sun
title: The Curse of Depth in Large Language Models
thumbnail: ""
link: https://huggingface.co/papers/2502.05795
summary: This work discusses the 'Curse of Depth' in Large Language Models (LLMs), where many layers are less effective due to the use of Pre-Layer Normalization (Pre-LN). The researchers propose a solution, LayerNorm Scaling, which reduces the output variance growth of deeper Transformer layers, improving their contribution to the training process. Experimental results show that this method significantly enhances LLM pre-training and fine-tuning performance....
opinion: placeholder
tags:
    - ML
