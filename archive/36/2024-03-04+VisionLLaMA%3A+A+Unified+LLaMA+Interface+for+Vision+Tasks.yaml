author: Xiangxiang Chu
date: '2024-03-04'
link: https://huggingface.co/papers/2403.00522
opinion: placeholder
summary: This paper introduces VisionLLaMA, a unified and generic modeling framework
  for solving most vision tasks, built on top of a transformer-based architecture.
  It has been shown to perform better than previous state-of-the-art vision transformers
  in a variety of tasks and can serve as a new baseline model for vision generation
  and understanding....
tags:
- Computer Vision
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/2YprH5pyX9dMHxImKGo4X.png
title: 'VisionLLaMA: A Unified LLaMA Interface for Vision Tasks'
translated_paths:
  INT: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2403.00522/paper.en.html
  KR: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2403.00522/paper.ko.html
