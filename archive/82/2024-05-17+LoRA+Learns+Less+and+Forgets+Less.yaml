date: "2024-05-17"
author: Dan Biderman
title: LoRA Learns Less and Forgets Less
thumbnail: https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.09673.png
link: https://huggingface.co/papers/2405.09673
summary: LoRA, a parameter-efficient finetuning method for large language models, underperforms full finetuning in most cases but provides stronger regularization and maintains better performance on tasks outside the target domain. Full finetuning learns perturbations with a rank 10-100X greater than typical LoRA configurations, possibly explaining the performance gap....
opinion: placeholder
tags:
    - Supervised Learning
    - Deep Learning
    - Natural Language Processing
