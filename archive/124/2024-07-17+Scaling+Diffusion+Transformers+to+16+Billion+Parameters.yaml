date: "2024-07-17"
author: Zhengcong Fei
title: Scaling Diffusion Transformers to 16 Billion Parameters
thumbnail: ""
link: https://huggingface.co/papers/2407.11633
summary: 'This paper introduces DiT-MoE, a sparse version of the diffusion Transformer that is scalable and competitive with dense networks while exhibiting highly optimized inference. It includes two simple designs: shared expert routing and expert-level balance loss, thereby capturing common knowledge and reducing redundancy among the different routed experts. When applied to conditional image generation, a deep analysis of experts specialization gains some interesting observations: (i) Expert selection...'
opinion: placeholder
tags:
    - ML
