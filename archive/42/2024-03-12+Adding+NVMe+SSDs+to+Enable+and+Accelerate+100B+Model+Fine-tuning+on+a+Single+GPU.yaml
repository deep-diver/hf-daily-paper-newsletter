author: Changyue Liao
date: '2024-03-12'
link: https://huggingface.co/papers/2403.06504
opinion: placeholder
summary: This paper presents a new training framework called Fuyou that enables efficient
  training of huge models on a low-end server with a low-end GPU and limited CPU memory
  capacity. The key idea is to add SSD-CPU communication as an optimization dimension
  and co-optimize computation and data swapping to maximize GPU utilization. The experimental
  results show that Fuyou can fine-tune a 175B GPT-3 model on a consumer GPU RTX 4090
  with high GPU utilization, while ZeRO-Infinity fails to do so. Additional...
tags:
- Optimization and Learning Algorithms
- Deep Learning
thumbnail: https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.06504.png
title: Adding NVMe SSDs to Enable and Accelerate 100B Model Fine-tuning on a Single
  GPU
translated_paths:
  INT: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2403.06504/paper.en.html
  KR: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2403.06504/paper.ko.html
