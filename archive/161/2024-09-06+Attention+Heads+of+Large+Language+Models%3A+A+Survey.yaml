date: "2024-09-06"
author: Zifan Zheng
title: 'Attention Heads of Large Language Models: A Survey'
thumbnail: ""
link: https://huggingface.co/papers/2409.03752
summary: This paper presents a survey of the internal mechanisms of Large Language Models (LLMs), focusing on attention heads. The authors propose a four-stage framework for human thought processes and use it to categorize the functions of specific attention heads. They also summarize the experimental methodologies and evaluate current research limitations and future directions....
opinion: placeholder
tags:
    - ML
