date: "2024-04-26"
author: Mostafa Elhoushi
title: 'Layer Skip: Enabling Early Exit Inference and Self-Speculative Decoding'
thumbnail: https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.16710.png
link: https://huggingface.co/papers/2404.16710
summary: This paper presents LayerSkip, a method to speed up inference of large language models. By applying layer dropout during training and using early exit loss, the method increases the accuracy of early exit without adding extra layers or modules to the model. The paper also introduces a novel self-speculative decoding solution that reduces memory footprint and benefits from shared compute and activations of the draft and verification stages. The method achieves up to 2.16x speedup on various tasks...
opinion: placeholder
tags:
    - Supervised Learning
    - Deep Learning
    - Natural Language Processing
