date: "2024-06-18"
author: Hoyeon Chang
title: How Do Large Language Models Acquire Factual Knowledge During Pretraining?
thumbnail: ""
link: https://huggingface.co/papers/2406.11813
summary: This paper investigates how large language models (LLMs) acquire factual knowledge during pretraining. The study reveals that pretraining on more data does not significantly improve knowledge acquisition, and there is a power-law relationship between training steps and forgetting of factual knowledge. Training with larger batch sizes can enhance model robustness against forgetting. The findings suggest that LLMs gradually increase the probability of factual knowledge during pretraining, but this...
opinion: placeholder
tags:
    - ML
