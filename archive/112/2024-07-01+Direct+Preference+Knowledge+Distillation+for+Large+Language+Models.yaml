date: "2024-07-01"
author: Yixing Li
title: Direct Preference Knowledge Distillation for Large Language Models
thumbnail: ""
link: https://huggingface.co/papers/2406.19774
summary: In the field of large language models (LLMs), Knowledge Distillation (KD) is a critical technique for transferring capabilities from teacher models to student models. However, existing KD methods face limitations and challenges in distillation of LLMs, including efficiency and insufficient measurement capabilities of traditional KL divergence. It is shown that LLMs can serve as an implicit reward function, which we define as a supplement to KL divergence. In this work, we propose Direct Preferen...
opinion: placeholder
tags:
    - ML
