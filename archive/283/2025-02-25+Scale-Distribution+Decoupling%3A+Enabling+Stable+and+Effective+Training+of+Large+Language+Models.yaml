date: "2025-02-25"
author: Ya Wang
title: 'Scale-Distribution Decoupling: Enabling Stable and Effective Training of Large Language Models'
thumbnail: ""
link: https://huggingface.co/papers/2502.15499
summary: The paper describes a new method, Scale-Distribution Decoupling (SDD), to stabilize the training of large language models by separating the scale and distribution of the weight matrix. This approach, which uses normalization and a learnable scaling vector, is shown to prevent gradient explosion and dissipation, leading to more stable training across different architectures and normalization configurations....
opinion: placeholder
tags:
    - ML
