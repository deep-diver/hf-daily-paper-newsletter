date: "2025-02-26"
author: Zhenheng Tang
title: The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?
thumbnail: ""
link: https://huggingface.co/papers/2502.17535
summary: Research on reducing LLM costs has mainly focused on maintaining performance through model and KV cache compression, often measured by perplexity or accuracy on commonsense QA and basic arithmetic reasoning. The authors propose the 'Lottery LLM Hypothesis,' which suggests a smaller LLM, aided by multi-step reasoning and external tools, can achieve the same performance as the original LLM, highlighting the need to preserve capabilities like retrieval-augmented generation, multi-step reasoning, ex...
opinion: placeholder
tags:
    - ML
