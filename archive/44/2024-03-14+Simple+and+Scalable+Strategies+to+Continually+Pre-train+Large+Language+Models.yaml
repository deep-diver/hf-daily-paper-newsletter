author: Adam Ibrahim
date: '2024-03-14'
link: https://huggingface.co/papers/2403.08763
opinion: placeholder
summary: A research paper presents simple and scalable strategies to continually pre-train
  large language models, achieving performance comparable to re-training from scratch,
  while saving significant compute. The strategies include learning rate re-warming,
  re-decaying, and replay of previous data, and are demonstrated at scale with different
  distribution shifts and model sizes....
tags:
- ML
thumbnail: https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.08763.png
title: Simple and Scalable Strategies to Continually Pre-train Large Language Models
translated_paths:
  INT: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2403.08763/paper.en.html
  KR: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2403.08763/paper.ko.html
