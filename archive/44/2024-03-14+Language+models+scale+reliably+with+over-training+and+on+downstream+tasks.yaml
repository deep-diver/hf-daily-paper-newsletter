date: "2024-03-14"
author: Samir Yitzhak Gadre
title: Language models scale reliably with over-training and on downstream tasks
thumbnail: https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.08540.png
link: https://huggingface.co/papers/2403.08540
summary: The paper addresses gaps in current scaling studies of language models by investigating scaling in the over-trained regime and relating language model perplexity to downstream task performance. They create a testbed of models with various parameters and tokens, fit scaling laws, and use a power law to predict performance. Their experiments are available at <https://github.com/mlfoundations/scaling>....
opinion: placeholder
tags:
    - ML
