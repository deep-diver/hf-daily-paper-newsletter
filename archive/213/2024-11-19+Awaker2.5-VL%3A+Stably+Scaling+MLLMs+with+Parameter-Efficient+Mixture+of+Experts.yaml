date: "2024-11-19"
author: Jinqiang Long
title: 'Awaker2.5-VL: Stably Scaling MLLMs with Parameter-Efficient Mixture of Experts'
thumbnail: ""
link: https://huggingface.co/papers/2411.10669
summary: The paper introduces Awaker2.5-VL, a Mixture of Experts architecture for Multimodal Large Language Models that improves performance on various tasks by using sparsely activated experts and a low-rank adaptation structure. The code and model weights are available on the project's GitHub page....
opinion: placeholder
tags:
    - ML
