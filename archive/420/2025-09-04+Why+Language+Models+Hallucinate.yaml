date: "2025-09-04"
author: Adam Tauman Kalai
title: Why Language Models Hallucinate
thumbnail: ""
link: https://huggingface.co/papers/2509.04664
summary: Large language models sometimes make up incorrect information, called 'hallucinations', because they are trained to guess instead of admitting uncertainty. This issue is made worse by evaluation methods that reward correct answers over uncertainty, but it can be fixed by changing how these models are scored and evaluated....
opinion: placeholder
tags:
    - ML
