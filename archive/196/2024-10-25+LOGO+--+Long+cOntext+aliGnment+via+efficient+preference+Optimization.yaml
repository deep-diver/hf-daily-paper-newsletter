date: "2024-10-25"
author: Zecheng Tang
title: LOGO -- Long cOntext aliGnment via efficient preference Optimization
thumbnail: ""
link: https://huggingface.co/papers/2410.18533
summary: This paper introduces LOGO, a training strategy that improves the generation capabilities of long-context models by optimizing their alignment and efficiency. By training with a smaller amount of data on a single GPU machine, LOGO allows the Llama-3-8B-Instruct-80K model to achieve comparable performance with GPT-4 in real-world long-context tasks while maintaining its original capabilities on other tasks, such as language modeling and MMLU. LOGO also extends the model's context window size and ...
opinion: placeholder
tags:
    - ML
