date: "2024-10-03"
author: Jeremy Bernstein
title: 'Old Optimizer, New Norm: An Anthology'
thumbnail: ""
link: https://huggingface.co/papers/2409.20325
summary: The paper proposes a new design space for training algorithms in deep learning, arguing that popular optimizers like Adam, Shampoo, and Prodigy can be understood as first-order methods without convexity assumptions. The authors suggest tailoring operator norms to different tensor roles within the neural network, potentially leading to more stable, scalable, and faster training....
opinion: placeholder
tags:
    - ML
