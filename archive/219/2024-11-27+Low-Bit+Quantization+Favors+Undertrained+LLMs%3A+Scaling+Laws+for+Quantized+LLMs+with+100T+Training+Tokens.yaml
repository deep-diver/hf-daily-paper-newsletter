date: "2024-11-27"
author: Xu Ouyang
title: 'Low-Bit Quantization Favors Undertrained LLMs: Scaling Laws for Quantized LLMs with 100T Training Tokens'
thumbnail: ""
link: https://huggingface.co/papers/2411.17691
summary: We find that low-bit quantization works better for undertrained large language models, and we derive scaling laws to predict how well different-sized models will perform with 100 trillion training tokens. Our findings suggest that low-bit quantization may not be desirable for future models trained with over 100 trillion tokens, and we release our quantized checkpoints to help future research....
opinion: placeholder
tags:
    - ML
