date: "2025-09-01"
author: Rushi Wang
title: 'Context Engineering for Trustworthiness: Rescorla Wagner Steering Under   Mixed and Inappropriate Contexts'
thumbnail: ""
link: https://huggingface.co/papers/2509.04500
summary: This study explores how Large Language Models (LLMs) handle mixed contexts containing both relevant and inappropriate content. They develop a testbed and use a neuroscience model to show that LLMs often incorporate less prevalent information, making them vulnerable to small amounts of inappropriate content. To fix this, they introduce RW-Steering, a method that improves LLM safety and response quality in real-world settings without needing extensive supervision....
opinion: placeholder
tags:
    - ML
