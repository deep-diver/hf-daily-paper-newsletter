date: "2025-01-14"
author: MiniMax
title: 'MiniMax-01: Scaling Foundation Models with Lightning Attention'
thumbnail: ""
link: https://huggingface.co/papers/2501.08313
summary: MiniMax-01 is a new series of foundation models that can process longer contexts than other models. It uses lightning attention and Mixture of Experts (MoE) to scale efficiently. The models have up to 456 billion parameters and can handle contexts spanning millions of tokens. They match the performance of state-of-the-art models like GPT-4o and Claude-3.5-Sonnet while offering 20-32 times longer context window. The models are available at https://github.com/MiniMax-AI....
opinion: placeholder
tags:
    - ML
