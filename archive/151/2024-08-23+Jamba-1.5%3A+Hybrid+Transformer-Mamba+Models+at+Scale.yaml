date: "2024-08-23"
author: Jamba Team
title: 'Jamba-1.5: Hybrid Transformer-Mamba Models at Scale'
thumbnail: ""
link: https://huggingface.co/papers/2408.12570
summary: Jamba-1.5 is a new large language model that uses a hybrid architecture to provide high throughput and low memory usage while maintaining the same or better quality as Transformer models. It has an effective context length of 256K tokens and is fine-tuned for conversational and instruction-following capabilities. Two model sizes are released, along with a novel quantization technique to support cost-effective inference. Jamba-1.5 achieves excellent results on various benchmarks and outperforms o...
opinion: placeholder
tags:
    - ML
