author: Silin Gao
date: '2024-02-01'
link: https://huggingface.co/papers/2401.17464
opinion: placeholder
summary: The paper presents a new method called Chain-of-Abstraction (CoA) that improves
  the usage of tools in multi-step reasoning for large language models (LLMs). CoA
  trains LLMs to first decode reasoning chains with abstract placeholders and then
  call domain tools to fill in specific knowledge, allowing for parallel decoding
  and tool calling. The method shows consistent improvement in mathematical reasoning
  and Wiki QA tasks, with faster inference speeds compared to previous baselines....
tags:
- Supervised Learning
- Deep Learning
- Natural Language Processing
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/4GMhSwUSbIcRXS7lpkjVu.png
title: Efficient Tool Use with Chain-of-Abstraction Reasoning
translated_paths:
  INT: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2401.17464/paper.en.html
  KR: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2401.17464/paper.ko.html
