date: "2025-02-03"
author: Ken M. Nakanishi
title: Scalable-Softmax Is Superior for Attention
thumbnail: ""
link: https://huggingface.co/papers/2501.19399
summary: The paper presents Scalable-Softmax (SSMax) to overcome the limitation of Transformer-based language models' attention distribution flattening as context size grows, causing reduced ability to prioritize key information. Using SSMax, models achieve faster loss reduction during pretraining and significantly improve performance in long contexts and key information retrieval, allowing the model to focus attention on key information even in long contexts....
opinion: placeholder
tags:
    - ML
