date: "2024-04-24"
author: Xun Wu
title: Multi-Head Mixture-of-Experts
thumbnail: https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.15045.png
link: https://huggingface.co/papers/2404.15045
summary: The paper introduces Multi-Head Mixture-of-Experts (MH-MoE) to address the issues of low expert activation and lack of fine-grained analytical capabilities in Sparse Mixtures of Experts (SMoE). MH-MoE uses a multi-head mechanism to split tokens into sub-tokens, which are processed by a diverse set of experts in parallel. This collectively attends to information from various representation spaces within different experts, enhancing expert activation and improving context understanding....
opinion: placeholder
tags:
    - Supervised Learning
    - Deep Learning
