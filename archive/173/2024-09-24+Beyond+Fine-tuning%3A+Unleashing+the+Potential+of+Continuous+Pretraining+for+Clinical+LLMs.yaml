date: "2024-09-24"
author: Cl√©ment Christophe
title: 'Beyond Fine-tuning: Unleashing the Potential of Continuous Pretraining for Clinical LLMs'
thumbnail: ""
link: https://huggingface.co/papers/2409.14988
summary: This study explores ways to improve Large Language Models (LLMs) for medical applications by using them for a long time (continuous pretraining), giving them specific instructions (instruct fine-tuning), and using special phrases to guide them (prompt engineering). The study finds that continuous pretraining helps, but giving instructions and using special phrases can make the models even better. A technique called NEFTune, which is meant to improve how the models create text, also helps the mod...
opinion: placeholder
tags:
    - ML
