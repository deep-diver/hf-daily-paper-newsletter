date: "2024-02-09"
author: Liang Wang
title: 'Multilingual E5 Text Embeddings: A Technical Report'
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/34hZOlCcUTJ8cOVKtQ9Ij.png
link: https://huggingface.co/papers/2402.05672
summary: This technical report talks about the training methodology and evaluation results of a new multilingual text embedding model called E5, which was released in mid-2023. E5 comes in different sizes (small / base / large) and its training procedure follows the English E5 model recipe, where it was first pre-trained on 1 billion multilingual text pairs and then fine-tuned on labeled datasets. They also introduced a new instruction-tuned embedding model, which performs as well as state-of-the-art Eng...
opinion: placeholder
tags:
    - Natural Language Processing
