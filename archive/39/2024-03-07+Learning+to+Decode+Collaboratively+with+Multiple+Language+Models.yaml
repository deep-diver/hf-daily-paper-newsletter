author: Shannon Zejiang Shen
date: '2024-03-07'
link: https://huggingface.co/papers/2403.03870
opinion: placeholder
summary: This paper introduces a method to teach multiple large language models to
  collaborate by interleaving their generations at the token level. By optimizing
  the marginal likelihood of a training set under their latent variable model, the
  base LLM learns when to generate and when to call on an assistant language model
  to generate, all without direct supervision. This collaboration improves performance
  on various tasks....
tags:
- Natural Language Processing
- Deep Learning
- Optimization and Learning Algorithms
thumbnail: https://github.com/deep-diver/hf-daily-paper-newsletter/blob/main/assets/2403.03870.gif?raw=true
title: Learning to Decode Collaboratively with Multiple Language Models
