author: Jiawei Zhao
date: '2024-03-07'
link: https://huggingface.co/papers/2403.03507
opinion: placeholder
summary: GaLore is a new training strategy for Large Language Models (LLMs) that reduces
  memory usage without sacrificing performance. It allows full-parameter learning
  and reduces memory usage by up to 82.5% while pre-training and fine-tuning. GaLore
  demonstrates the feasibility of pre-training a 7B model on consumer GPUs with 24GB
  memory....
tags:
- Deep Learning
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/tymX0hLyiKdpwCt7sNZ2h.png
title: 'GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection'
translated_paths:
  INT: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2403.03507/paper.en.html
  KR: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2403.03507/paper.ko.html
