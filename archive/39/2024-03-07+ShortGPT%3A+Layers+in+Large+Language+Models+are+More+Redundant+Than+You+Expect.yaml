author: Xin Men
date: '2024-03-07'
link: https://huggingface.co/papers/2403.03853
opinion: placeholder
summary: Researchers found that many layers in large language models are redundant
  and proposed a method called ShortGPT that removes these layers based on their Block
  Influence scores. This method significantly outperforms previous state-of-the-art
  methods in model pruning and can be used in conjunction with other methods to further
  reduce parameters and computation....
tags:
- Natural Language Processing
- Deep Learning
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/8My5G3ybxfrV5_fP1CE47.png
title: 'ShortGPT: Layers in Large Language Models are More Redundant Than You Expect'
translated_paths:
  INT: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2403.03853/paper.en.html
  KR: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2403.03853/paper.ko.html
