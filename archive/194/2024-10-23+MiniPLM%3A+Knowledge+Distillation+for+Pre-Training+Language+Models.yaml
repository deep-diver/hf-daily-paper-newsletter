date: "2024-10-23"
author: Yuxian Gu
title: 'MiniPLM: Knowledge Distillation for Pre-Training Language Models'
thumbnail: ""
link: https://huggingface.co/papers/2410.17215
summary: MiniPLM is a knowledge distillation framework that improves the performance of small language models during pre-training by refining training data with knowledge from large teacher models. It's efficient, flexible, and effective, and can be used across different model families....
opinion: placeholder
tags:
    - ML
