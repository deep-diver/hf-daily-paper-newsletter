date: "2025-02-12"
author: Jihoon Tack
title: LLM Pretraining with Continuous Concepts
thumbnail: ""
link: https://huggingface.co/papers/2502.08524
summary: The authors present CoCoMix, a new language model pretraining framework that combines next token prediction with continuous concepts learned from a sparse autoencoder. Experiments show CoCoMix is more efficient and performs better than traditional methods on various tasks, while also improving model interpretability and steerability....
opinion: placeholder
tags:
    - ML
