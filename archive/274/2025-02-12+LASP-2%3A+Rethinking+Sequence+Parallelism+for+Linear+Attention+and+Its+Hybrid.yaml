date: "2025-02-12"
author: Weigao Sun
title: 'LASP-2: Rethinking Sequence Parallelism for Linear Attention and Its Hybrid'
thumbnail: ""
link: https://huggingface.co/papers/2502.07563
summary: The paper proposes LASP-2, a method for enhancing communication and computation parallelism in training linear attention transformer models with very-long input sequences. Compared to previous work LASP, LASP-2 reorganizes the communication-computation workflow, requiring only one AllGather collective communication on intermediate memory states, which leads to significant improvements in both communication and computation parallelism. Additionally, LASP-2H is introduced as a hybrid version for m...
opinion: placeholder
tags:
    - ML
