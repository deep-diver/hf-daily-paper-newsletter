date: "2025-02-13"
author: Konstantin Kolomeitsev
title: 'LLM Modules: Knowledge Transfer from a Large to a Small Model using Enhanced Cross-Attention'
thumbnail: ""
link: https://huggingface.co/papers/2502.08213
summary: The authors propose an architecture of LLM Modules that transfers knowledge from a large model, Qwen2-1.5B, to a smaller one, GPT-Neo-125M, using an Enhanced Cross-Attention mechanism. After 15 epochs of training on the Bespoke-Stratos-17k dataset, the combined model generates responses of similar quality to those obtained by distillation....
opinion: placeholder
tags:
    - ML
