date: "2025-02-12"
author: Dan Busbridge
title: Distillation Scaling Laws
thumbnail: ""
link: https://huggingface.co/papers/2502.08606
summary: A law is presented to estimate distilled model performance with a compute budget and its allocation between student and teacher models. This law helps maximize student performance and offers compute optimal distillation recipes for different scenarios, including when a teacher exists or needs training, and when multiple students are distilled or a single student needs training with a new teacher....
opinion: placeholder
tags:
    - ML
