date: "2024-04-15"
author: Zichao Li
title: 'Scaling (Down) CLIP: A Comprehensive Analysis of Data, Architecture, and Training Strategies'
thumbnail: https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.08197.png
link: https://huggingface.co/papers/2404.08197
summary: This study investigates how scaling down the Contrastive Language-Image Pre-training (CLIP) model affects its performance, and provides guidance on choosing the right data, architecture, and training strategies for different applications and compute budgets. They find that high-quality, smaller datasets can outperform larger datasets with lower quality, and that the choice of training strategy depends on available compute resources. CLIP+Data Augmentation achieves comparable performance to CLIP ...
opinion: placeholder
tags:
    - Deep Learning
    - Natural Language Processing
    - Computer Vision
    - Optimization and Learning Algorithms
