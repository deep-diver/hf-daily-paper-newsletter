date: "2025-01-21"
author: Zihan Qiu
title: 'Demons in the Detail: On Implementing Load Balancing Loss for Training Specialized Mixture-of-Expert Models'
thumbnail: ""
link: https://huggingface.co/papers/2501.11873
summary: This paper proposes a new method for training Mixture-of-Experts (MoE) models called Load-balancing Loss (LBL) to improve expert specialization. The new method calculates LBL using a global-batch instead of a micro-batch, which leads to better performance gains in both pre-training perplexity and downstream tasks. The analysis shows that the global-batch LBL also improves the domain specialization of MoE experts....
opinion: placeholder
tags:
    - ML
