date: "2025-03-04"
author: Kunlun Zhu
title: 'MultiAgentBench: Evaluating the Collaboration and Competition of LLM agents'
thumbnail: ""
link: https://huggingface.co/papers/2503.01935
summary: To evaluate Large Language Models (LLMs) as multi-agent systems, a new benchmark called MultiAgentBench is proposed. It assesses LLMs across diverse scenarios, measuring task completion and collaboration/competition quality using milestone-based KPIs, and tests coordination protocols and strategies like group discussion and cognitive planning....
opinion: placeholder
tags:
    - ML
