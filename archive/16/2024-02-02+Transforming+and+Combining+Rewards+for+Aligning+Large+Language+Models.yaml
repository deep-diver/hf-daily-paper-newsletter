date: "2024-02-02"
author: Zihao Wang
title: Transforming and Combining Rewards for Aligning Large Language Models
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/_S-dQdA05eQLF3cbp7P8G.png
link: https://huggingface.co/papers/2402.00742
summary: This paper proposes a transformation of reward models for aligning language models with human preferences that emphasizes improving poorly-performing outputs and enables principled aggregation of rewards. Experiments show improvements in aligning language models to be both helpful and harmless using Reinforcement Learning from Human Feedback (RLHF)....
opinion: placeholder
tags:
    - Reinforcement Learning
    - Natural Language Processing
