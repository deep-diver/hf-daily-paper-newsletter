date: "2025-06-24"
author: Zeju Qiu
title: Orthogonal Finetuning Made Scalable
thumbnail: ""
link: https://huggingface.co/papers/2506.19847
summary: The study presents an improved version of Orthogonal Finetuning (OFT) called OFTv2, which is faster and uses less memory than the original OFT. The authors achieved this by changing the computation method and introducing a new efficient parameterization, allowing OFTv2 to train up to 10 times faster and use 3 times less GPU memory without sacrificing performance. Additionally, OFTv2 can finetune quantized foundation models and outperforms QLoRA in training stability, efficiency, and memory usage...
opinion: placeholder
tags:
    - ML
