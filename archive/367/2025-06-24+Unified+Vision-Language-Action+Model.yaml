date: "2025-06-24"
author: Yuqi Wang
title: Unified Vision-Language-Action Model
thumbnail: ""
link: https://huggingface.co/papers/2506.19850
summary: The study introduces UniVLA, a model that combines vision, language, and action signals to improve robotic manipulation, especially for long-term tasks, by learning from large-scale video data and capturing causal dynamics. UniVLA outperforms existing methods in various simulation benchmarks and shows promise in real-world applications like manipulation and autonomous driving....
opinion: placeholder
tags:
    - ML
