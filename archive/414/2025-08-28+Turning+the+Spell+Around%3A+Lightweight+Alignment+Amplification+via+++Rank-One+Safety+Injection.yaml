date: "2025-08-28"
author: Harethah Abu Shairah
title: 'Turning the Spell Around: Lightweight Alignment Amplification via   Rank-One Safety Injection'
thumbnail: ""
link: https://huggingface.co/papers/2508.20766
summary: This research presents a new method called Rank-One Safety Injection (ROSI) that enhances the safety of Large Language Models (LLMs) by steering their activations towards refusal-mediating subspaces, thus preventing harmful requests. ROSI is a simple and cost-effective technique that can be applied without fine-tuning, and it has been shown to improve safety refusal rates while maintaining the model's utility on various benchmarks....
opinion: placeholder
tags:
    - ML
