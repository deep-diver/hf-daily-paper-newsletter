date: "2024-09-25"
author: Yizhi Li
title: 'OmniBench: Towards The Future of Universal Omni-Language Models'
thumbnail: ""
link: https://huggingface.co/papers/2409.15272
summary: The paper introduces OmniBench, a new benchmark for evaluating models' ability to process and reason across visual, acoustic, and textual inputs simultaneously. They found that open-source models have limitations in instruction-following and reasoning capabilities within tri-modal contexts, and suggest that future research should focus on developing more robust tri-modal integration techniques and training strategies to enhance performance across diverse modalities....
opinion: placeholder
tags:
    - ML
