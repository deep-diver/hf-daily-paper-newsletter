author: Yiran Ding
date: '2024-02-22'
link: https://huggingface.co/papers/2402.13753
opinion: placeholder
summary: The paper presents LongRoPE, a method that extends the context window of
  pre-trained LLMs to an impressive 2048k tokens, with up to only 1k fine-tuning steps
  at within 256k training lengths, by exploiting non-uniformities in positional interpolation
  and using a progressive extension strategy. Experiments on LLaMA2 and Mistral demonstrate
  the effectiveness of the method. The models extended via LongRoPE retain the original
  architecture with minor modifications to the positional embedding and can ...
tags:
- Deep Learning
- Natural Language Processing
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/VO-TU3OZvAP3C-BSvFMeI.png
title: 'LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens'
translated_paths:
  INT: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.13753/paper.en.html
  KR: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.13753/paper.ko.html
