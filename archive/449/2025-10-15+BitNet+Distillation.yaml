date: "2025-10-15"
author: Xun Wu
title: BitNet Distillation
thumbnail: ""
link: https://huggingface.co/papers/2510.13998
summary: 'The authors propose a method called BitNet Distillation that reduces the size of large language models by converting them into 1.58-bit precision, resulting in faster and more memory-efficient models without significantly compromising performance. This is achieved through three techniques: SubLN module, multi-head attention distillation, and continual pre-training, which together enable up to 10x memory savings and 2.65x faster inference on CPUs compared to full-precision models....'
opinion: placeholder
tags:
    - ML
