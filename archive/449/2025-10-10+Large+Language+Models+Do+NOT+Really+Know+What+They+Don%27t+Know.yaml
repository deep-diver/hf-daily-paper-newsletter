date: "2025-10-10"
author: Chi Seng Cheang
title: Large Language Models Do NOT Really Know What They Don't Know
thumbnail: ""
link: https://huggingface.co/papers/2510.09033
summary: The study analyzes how large language models process factual queries and finds that they don't distinguish internal states for correct and hallucinated responses unless the hallucination is unrelated to subject knowledge. This reveals that LLMs don't encode truthfulness but only recall patterns, meaning they don't truly know what they don't know....
opinion: placeholder
tags:
    - ML
