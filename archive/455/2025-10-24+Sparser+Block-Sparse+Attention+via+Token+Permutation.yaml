date: "2025-10-24"
author: Xinghao Wang
title: Sparser Block-Sparse Attention via Token Permutation
thumbnail: ""
link: https://huggingface.co/papers/2510.21270
summary: This research presents Permuted Block-Sparse Attention (PBS-Attn), a new method that improves the efficiency of large language models by increasing block-level sparsity through attention permutation, outperforming existing block-sparse attention methods and achieving up to 2.75 times speedup in long-context prefilling....
opinion: placeholder
tags:
    - ML
