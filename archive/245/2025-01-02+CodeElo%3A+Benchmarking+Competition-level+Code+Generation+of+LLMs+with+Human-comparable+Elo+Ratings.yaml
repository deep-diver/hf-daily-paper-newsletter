date: "2025-01-02"
author: Shanghaoran Quan
title: 'CodeElo: Benchmarking Competition-level Code Generation of LLMs with Human-comparable Elo Ratings'
thumbnail: ""
link: https://huggingface.co/papers/2501.01257
summary: The paper introduces CodeElo, a benchmark for testing the coding abilities of large language models (LLMs) on competition-level problems. CodeElo uses the CodeForces platform and calculates Elo ratings for LLMs, which are comparable to human participants. The paper tests 30 open-source and 3 proprietary LLMs on CodeElo and finds that o1-mini and QwQ-32B-Preview perform best, while others struggle with even the easiest problems....
opinion: placeholder
tags:
    - ML
