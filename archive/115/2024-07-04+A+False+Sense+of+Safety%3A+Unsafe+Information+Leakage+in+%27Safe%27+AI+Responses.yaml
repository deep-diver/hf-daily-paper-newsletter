date: "2024-07-04"
author: David Glukhov
title: 'A False Sense of Safety: Unsafe Information Leakage in ''Safe'' AI Responses'
thumbnail: ""
link: https://huggingface.co/papers/2407.02551
summary: The paper discusses the vulnerability of Large Language Models to information leakage and proposes a new defense mechanism called inferential adversaries. This mechanism aims to bound the leakage of impermissible information and ensure safety, but it also reveals a safety-utility trade-off....
opinion: placeholder
tags:
    - ML
