author: Soham De
date: '2024-03-01'
link: https://huggingface.co/papers/2402.19427
opinion: placeholder
summary: This paper proposes Hawk, an RNN with gated linear recurrences, and Griffin,
  a hybrid model that mixes gated linear recurrences with local attention. Griffin
  matches the performance of Llama-2 despite being trained on 6 times fewer tokens
  and can extrapolate on long sequences. Griffin is also more efficient in hardware
  during inference than Transformers....
tags:
- Natural Language Processing
- Deep Learning
- Optimization and Learning Algorithms
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/5e68p09De9fBpLfPl_uUJ.png
title: 'Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient
  Language Models'
translated_paths:
  INT: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.19427/paper.en.html
  KR: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.19427/paper.ko.html
