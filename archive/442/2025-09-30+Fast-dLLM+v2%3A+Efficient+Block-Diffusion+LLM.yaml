date: "2025-09-30"
author: Chengyue Wu
title: 'Fast-dLLM v2: Efficient Block-Diffusion LLM'
thumbnail: ""
link: https://huggingface.co/papers/2509.26328
summary: The authors present Fast-dLLM v2, a block diffusion language model that adapts pretrained autoregressive models for parallel text generation with just 1B tokens of fine-tuning, compared to 580B tokens for full-attention diffusion LLMs like Dream. This model uses a novel training recipe, a hierarchical caching mechanism, and a parallel decoding pipeline to achieve up to 2.5x faster decoding than standard autoregressive models without sacrificing quality, making it a promising solution for practic...
opinion: placeholder
tags:
    - ML
