date: "2025-10-07"
author: Qingyu Yin
title: 'Refusal Falls off a Cliff: How Safety Alignment Fails in Reasoning?'
thumbnail: ""
link: https://huggingface.co/papers/2510.06036
summary: The study explores why reasoning models sometimes fail to follow safety guidelines. They discovered that these models can identify harmful prompts but suppress their refusal intentions before output. By identifying and adjusting a small number of attention heads, they improved the models' safety, using only a small portion of the training data....
opinion: placeholder
tags:
    - ML
