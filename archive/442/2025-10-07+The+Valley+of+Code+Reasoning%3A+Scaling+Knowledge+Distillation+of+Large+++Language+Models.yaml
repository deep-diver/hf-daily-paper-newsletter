date: "2025-10-07"
author: Muyu He
title: 'The Valley of Code Reasoning: Scaling Knowledge Distillation of Large   Language Models'
thumbnail: ""
link: https://huggingface.co/papers/2510.06101
summary: Researchers studied the impact of distillation data quantity on the performance of two small non-reasoning language models in coding tasks, discovering a 'valley of code reasoning' where performance initially drops then sharply rises with more data. They also found that smaller models benefit more from easier coding questions than harder ones in low and medium-low data regimes, and the correctness of training data outputs doesn't affect distillation outcomes....
opinion: placeholder
tags:
    - ML
