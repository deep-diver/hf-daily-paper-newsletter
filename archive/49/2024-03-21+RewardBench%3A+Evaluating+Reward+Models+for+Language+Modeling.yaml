date: "2024-03-21"
author: Nathan Lambert
title: 'RewardBench: Evaluating Reward Models for Language Modeling'
thumbnail: https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.13787.png
link: https://huggingface.co/papers/2403.13787
summary: RewardBench is a benchmark dataset and codebase for evaluating reward models used in Reinforcement Learning from Human Feedback (RLHF) to align pretrained models with human preferences. It provides a collection of prompt-win-lose trios to benchmark how different reward models perform on challenging, structured, and out-of-distribution queries, and evaluates these models trained with various methods and datasets. The goal is to enhance scientific understanding of reward models and the RLHF proces...
opinion: placeholder
tags:
    - Reinforcement Learning
    - Natural Language Processing
