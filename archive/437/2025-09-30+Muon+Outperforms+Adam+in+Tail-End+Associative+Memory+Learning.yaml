date: "2025-09-30"
author: Shuche Wang
title: Muon Outperforms Adam in Tail-End Associative Memory Learning
thumbnail: ""
link: https://huggingface.co/papers/2509.26030
summary: The Muon optimizer is faster than Adam in training Large Language Models, and this study shows that Muon is better at learning rare classes in heavy-tailed data due to its update rule, which leads to more balanced learning across classes compared to Adam....
opinion: placeholder
tags:
    - ML
