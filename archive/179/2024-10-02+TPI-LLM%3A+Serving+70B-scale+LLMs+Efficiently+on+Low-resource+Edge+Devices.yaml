date: "2024-10-02"
author: Zonghang Li
title: 'TPI-LLM: Serving 70B-scale LLMs Efficiently on Low-resource Edge Devices'
thumbnail: ""
link: https://huggingface.co/papers/2410.00531
summary: TPI-LLM is a tensor parallel inference system that efficiently serves 70B-scale LLMs on low-resource edge devices by using a sliding window memory scheduler and a star-based allreduce algorithm. It reduces time-to-first-token and token latency by over 80% compared to other systems, and cuts the peak memory footprint of Llama 2-70B by 90%, requiring only 3.1 GB of memory for 70B-scale models....
opinion: placeholder
tags:
    - ML
