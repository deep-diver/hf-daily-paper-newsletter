date: "2024-06-24"
author: Aman Singh Thakur
title: 'Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges'
thumbnail: ""
link: https://huggingface.co/papers/2406.12624
summary: This paper studies the performance of various large language models (LLMs) acting as judges for evaluating other LLMs. It uses TriviaQA as a benchmark and evaluates the judge models alongside human annotations. The study includes 9 judge models and 9 exam taker models. The paper finds that Llama-3 70B and GPT-4 Turbo have an excellent alignment with humans, but are outperformed by JudgeLM-7B and the lexical judge Contains in terms of ranking exam taker models. The paper also discusses the import...
opinion: placeholder
tags:
    - ML
