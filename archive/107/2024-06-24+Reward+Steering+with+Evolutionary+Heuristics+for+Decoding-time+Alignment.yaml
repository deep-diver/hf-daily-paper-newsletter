date: "2024-06-24"
author: Chia-Yu Hung
title: Reward Steering with Evolutionary Heuristics for Decoding-time Alignment
thumbnail: ""
link: https://huggingface.co/papers/2406.15193
summary: The paper proposes a new method for aligning language models with user preferences during inference, which decouples exploration and exploitation of reward and uses evolutionary heuristics to strike a balance between the two. This method is shown to outperform existing preference optimization and decode-time alignment approaches on two benchmarks, and its implementation will be made available online....
opinion: placeholder
tags:
    - ML
