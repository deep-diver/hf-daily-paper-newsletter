date: "2024-12-24"
author: Zhexin Zhang
title: 'Agent-SafetyBench: Evaluating the Safety of LLM Agents'
thumbnail: ""
link: https://huggingface.co/papers/2412.14470
summary: 'The paper introduces Agent-SafetyBench, a benchmark to evaluate the safety of large language models (LLMs) used as agents. It tests 16 popular LLM agents and finds that none of them has a safety score above 60%. The paper identifies two main safety issues: lack of robustness and lack of risk awareness. It suggests that defense prompts alone are not enough to address these issues, and more advanced strategies are needed....'
opinion: placeholder
tags:
    - ML
