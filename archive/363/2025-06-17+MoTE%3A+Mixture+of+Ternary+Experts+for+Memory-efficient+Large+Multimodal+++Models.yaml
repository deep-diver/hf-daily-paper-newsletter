date: "2025-06-17"
author: Hongyu Wang
title: 'MoTE: Mixture of Ternary Experts for Memory-efficient Large Multimodal   Models'
thumbnail: ""
link: https://huggingface.co/papers/2506.14435
summary: The authors present MoTE, a method for training efficient large multimodal models by using more low-precision experts instead of fewer high-precision ones. MoTE provides similar performance to full-precision models while consuming less memory, making it suitable for devices with limited memory....
opinion: placeholder
tags:
    - ML
