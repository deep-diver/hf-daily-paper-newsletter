date: "2025-06-17"
author: Zongxia Li
title: Semantically-Aware Rewards for Open-Ended R1 Training in Free-Form   Generation
thumbnail: ""
link: https://huggingface.co/papers/2506.15068
summary: The authors present PrefBERT, a new model for evaluating open-ended long-form generation, which offers more effective semantic reward feedback than traditional metrics. PrefBERT, trained on diverse long-form styles and Likert-rated quality, aligns well with human preferences and improves the training of policy models in generating better responses....
opinion: placeholder
tags:
    - ML
