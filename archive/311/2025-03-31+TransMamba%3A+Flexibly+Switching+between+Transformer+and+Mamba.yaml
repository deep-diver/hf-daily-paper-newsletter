date: "2025-03-31"
author: Yixing Li
title: 'TransMamba: Flexibly Switching between Transformer and Mamba'
thumbnail: ""
link: https://huggingface.co/papers/2503.24067
summary: The TransMamba framework combines the strengths of Transformer and Mamba models by using shared parameters and a memory converter to switch between attention and SSM mechanisms. This results in improved efficiency and performance in long-sequence processing, validated through extensive experiments....
opinion: placeholder
tags:
    - ML
