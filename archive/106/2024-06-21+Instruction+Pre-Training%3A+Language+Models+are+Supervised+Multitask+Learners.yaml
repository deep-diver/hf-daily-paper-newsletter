date: "2024-06-21"
author: Daixuan Cheng
title: 'Instruction Pre-Training: Language Models are Supervised Multitask Learners'
thumbnail: ""
link: https://huggingface.co/papers/2406.14491
summary: The paper proposes Instruction Pre-Training, a method for improving language models by training them on instruction-response pairs generated by an instruction synthesizer. The authors demonstrate the effectiveness of this method by synthesizing 200M instruction-response pairs and pre-training a language model on them, which leads to improved performance compared to other models....
opinion: placeholder
tags:
    - ML
