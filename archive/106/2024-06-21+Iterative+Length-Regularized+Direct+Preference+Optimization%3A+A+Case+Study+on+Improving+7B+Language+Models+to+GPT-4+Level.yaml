date: "2024-06-21"
author: Jie Liu
title: 'Iterative Length-Regularized Direct Preference Optimization: A Case Study on Improving 7B Language Models to GPT-4 Level'
thumbnail: ""
link: https://huggingface.co/papers/2406.11817
summary: iLR-DPO is a method that improves the performance of a 7B model to be as good as GPT-4, without increasing verbosity. This is done by penalizing response length and using online preferences labeled by a trained reward model. The 7B model achieves a 50.5% length-controlled win rate against GPT-4 Preview on AlpacaEval 2.0 and excels across standard benchmarks such as MT-Bench, Arena-Hard and OpenLLM Leaderboard, demonstrating the effectiveness of iterative DPO in aligning language models with huma...
opinion: placeholder
tags:
    - ML
