date: "2024-09-04"
author: Niklas Muennighoff
title: 'OLMoE: Open Mixture-of-Experts Language Models'
thumbnail: ""
link: https://huggingface.co/papers/2409.02060
summary: OLMoE is a new, open-source language model that uses a special method called Mixture-of-Experts (MoE) to be more efficient with its 7 billion parameters. It was trained on a huge amount of text and can do better than other models with even more parameters. The researchers also shared all their work, including the model, training data, and code, so others can learn from it....
opinion: placeholder
tags:
    - ML
