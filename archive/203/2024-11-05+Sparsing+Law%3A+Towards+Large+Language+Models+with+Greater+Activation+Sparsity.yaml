date: "2024-11-05"
author: Yuqi Luo
title: 'Sparsing Law: Towards Large Language Models with Greater Activation Sparsity'
thumbnail: ""
link: https://huggingface.co/papers/2411.02335
summary: This paper studies the quantitative scaling properties and influential factors of activation sparsity within decoder-only Transformer-based language models. It proposes a new activation sparsity metric and finds that ReLU is more efficient than SiLU and that activation patterns within LLMs are insensitive to the parameter scale....
opinion: placeholder
tags:
    - ML
