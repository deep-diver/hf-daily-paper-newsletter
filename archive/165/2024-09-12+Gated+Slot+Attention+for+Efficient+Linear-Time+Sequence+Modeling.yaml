date: "2024-09-12"
author: Yu Zhang
title: Gated Slot Attention for Efficient Linear-Time Sequence Modeling
thumbnail: ""
link: https://huggingface.co/papers/2409.07146
summary: Gated Slot Attention (GSA) is a new model that improves memory capacity and efficiency in sequence modeling tasks. It uses a gating mechanism inspired by Gated Linear Attention (GLA) and maintains compact recurrent state size. This design enhances both training and inference efficiency and is particularly beneficial in 'finetuning pretrained Transformers to RNNs' (T2R) settings, reducing the need for extensive training from scratch. Experiments show GSA's superior performance in scenarios requir...
opinion: placeholder
tags:
    - ML
