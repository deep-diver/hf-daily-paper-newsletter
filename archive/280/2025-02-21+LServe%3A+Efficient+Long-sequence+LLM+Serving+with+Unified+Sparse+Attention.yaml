date: "2025-02-21"
author: Shang Yang
title: 'LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention'
thumbnail: ""
link: https://huggingface.co/papers/2502.14866
summary: The authors present LServe, a system that enhances long-sequence LLM serving using hybrid sparse attention. This method combines several hardware-friendly sparsity patterns for prefilling and decoding attention into one framework, skipping computations for less important tokens in blocks. LServe accelerates LLM prefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, while maintaining long-context accuracy....
opinion: placeholder
tags:
    - ML
