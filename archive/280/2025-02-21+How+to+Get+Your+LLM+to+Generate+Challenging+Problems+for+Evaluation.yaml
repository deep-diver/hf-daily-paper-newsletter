date: "2025-02-21"
author: Arkil Patel
title: How to Get Your LLM to Generate Challenging Problems for Evaluation
thumbnail: ""
link: https://huggingface.co/papers/2502.14678
summary: This work presents CHASE, a framework to generate challenging problems for evaluating Large Language Models (LLMs) without human involvement. CHASE generates problems by decomposing the process into verifiable sub-tasks, and is used to create evaluation benchmarks in three domains, demonstrating effectiveness with 40-60% accuracy....
opinion: placeholder
tags:
    - ML
