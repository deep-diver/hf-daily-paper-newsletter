date: "2025-10-15"
author: Yuxiang Huang
title: 'NOSA: Native and Offloadable Sparse Attention'
thumbnail: ""
link: https://huggingface.co/papers/2510.13602
summary: The authors present NOSA, a framework for trainable sparse attention that reduces memory usage and improves decoding speed by offloading key-value pairs to the CPU, without altering the attention computation. NOSA's design, which introduces locality constraints, leads to a 2.3x improvement in decoding throughput compared to traditional methods, while maintaining performance....
opinion: placeholder
tags:
    - ML
