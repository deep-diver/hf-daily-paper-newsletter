date: "2025-09-29"
author: Huu Nguyen
title: 'MixtureVitae: Open Web-Scale Pretraining Dataset With High Quality   Instruction and Reasoning Data Built from Permissive-First Text Sources'
thumbnail: ""
link: https://huggingface.co/papers/2509.25531
summary: MixtureVitae is a new, open pretraining dataset that offers strong model performance while minimizing legal risks. It uses a mix of public-domain, permissively licensed, and low-risk texts, along with instruction, reasoning, and synthetic data. Experiments show that models trained on MixtureVitae perform well on various benchmarks, especially in math, code, and QA tasks, making it a practical and legally safe choice for training capable language models....
opinion: placeholder
tags:
    - ML
