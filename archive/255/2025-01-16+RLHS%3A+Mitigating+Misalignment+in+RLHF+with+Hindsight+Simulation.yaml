date: "2025-01-16"
author: Kaiqu Liang
title: 'RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation'
thumbnail: ""
link: https://huggingface.co/papers/2501.08617
summary: We propose a new method called RLHS that uses hindsight feedback to improve the alignment of generative AI systems with human values and reduce misalignment in RLHF. Our method involves simulating plausible consequences and then eliciting feedback to assess what behaviors were genuinely beneficial in hindsight. We apply RLHS to two widely-employed online and offline preference optimization methods and show empirically that misalignment is significantly reduced with both methods. Through an onlin...
opinion: placeholder
tags:
    - ML
