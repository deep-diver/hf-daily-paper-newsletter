author: Linrui Tian
date: '2024-02-28'
link: https://huggingface.co/papers/2402.17485
opinion: placeholder
summary: A new method called EMO is proposed to generate more realistic and expressive
  talking head videos by directly synthesizing videos from audio, bypassing the need
  for intermediate 3D models or facial landmarks. EMO is able to produce highly expressive
  and lifelike animations, even for singing videos in various styles, and outperforms
  existing state-of-the-art methodologies....
tags:
- Deep Learning
- Computer Vision
- Speech Recognition and Synthesis
- Natural Language Processing
- Emerging Applications of Machine Learning
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/Jpuu71oibjBK1VrDwppG5.png
title: 'EMO: Emote Portrait Alive - Generating Expressive Portrait Videos with Audio2Video
  Diffusion Model under Weak Conditions'
translated_paths:
  INT: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.17485/paper.en.html
  KR: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.17485/paper.ko.html
