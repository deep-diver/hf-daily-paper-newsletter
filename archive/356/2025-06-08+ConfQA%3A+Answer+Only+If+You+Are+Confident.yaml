date: "2025-06-08"
author: Yin Huang
title: 'ConfQA: Answer Only If You Are Confident'
thumbnail: ""
link: https://huggingface.co/papers/2506.07309
summary: The authors propose a fine-tuning strategy called ConfQA to reduce factual hallucinations in Large Language Models. By incorporating a dampening prompt and leveraging simple factual statements, the strategy effectively guides the models to admit uncertainty and improve confidence calibration, leading to potential accuracy gains beyond 95%....
opinion: placeholder
tags:
    - ML
