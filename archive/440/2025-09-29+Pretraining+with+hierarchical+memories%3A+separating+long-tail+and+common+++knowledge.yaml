date: "2025-09-29"
author: Hadi Pouransari
title: 'Pretraining with hierarchical memories: separating long-tail and common   knowledge'
thumbnail: ""
link: https://huggingface.co/papers/2510.02375
summary: The authors propose a new method for language modeling that uses a small model combined with a large, hierarchical memory bank. This approach allows the model to access vast amounts of world knowledge without requiring a large number of parameters, making it more efficient and practical for use on devices with limited resources. Experiments show that this method can achieve performance comparable to larger models, and the authors provide insights into the optimal design of these memory banks for...
opinion: placeholder
tags:
    - ML
