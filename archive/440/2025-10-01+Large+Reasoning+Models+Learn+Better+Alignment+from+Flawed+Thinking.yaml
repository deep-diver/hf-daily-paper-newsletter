date: "2025-10-01"
author: ShengYun Peng
title: Large Reasoning Models Learn Better Alignment from Flawed Thinking
thumbnail: ""
link: https://huggingface.co/papers/2510.00938
summary: The study presents a method called RECAP to enhance the safety and reasoning abilities of large reasoning models. This technique trains models to identify and correct flawed reasoning, making them more reliable and resistant to biases, without requiring additional resources or altering the existing reinforcement learning setup....
opinion: placeholder
tags:
    - ML
