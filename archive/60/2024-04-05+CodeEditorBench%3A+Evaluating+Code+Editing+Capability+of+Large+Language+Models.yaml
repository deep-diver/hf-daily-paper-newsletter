date: "2024-04-05"
author: Jiawei Guo
title: 'CodeEditorBench: Evaluating Code Editing Capability of Large Language Models'
thumbnail: https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.03543.png
link: https://huggingface.co/papers/2404.03543
summary: CodeEditorBench is an evaluation framework that assesses the performance of large language models in code editing tasks such as debugging, translating, and requirement switching. The results show that closed-source models outperform open-source models, and the framework aims to help advance LLMs in code editing by providing a robust platform for assessment....
opinion: placeholder
tags:
    - Natural Language Processing
    - Deep Learning
