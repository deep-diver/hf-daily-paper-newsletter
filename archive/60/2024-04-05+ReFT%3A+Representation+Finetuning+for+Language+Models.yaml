date: "2024-04-05"
author: Zhengxuan Wu
title: 'ReFT: Representation Finetuning for Language Models'
thumbnail: https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.03592.png
link: https://huggingface.co/papers/2404.03592
summary: This paper introduces a new method called Representation Finetuning (ReFT) that modifies a frozen base model by learning task-specific interventions on hidden representations, and shows that it is more parameter-efficient and performs better than previous methods....
opinion: placeholder
tags:
    - Supervised Learning
    - Natural Language Processing
