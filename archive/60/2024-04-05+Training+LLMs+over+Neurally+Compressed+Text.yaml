date: "2024-04-05"
author: Brian Lester
title: Training LLMs over Neurally Compressed Text
thumbnail: https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.03626.png
link: https://huggingface.co/papers/2404.03626
summary: The paper proposes a novel compression technique called Equal-Info Windows that allows for effective learning over highly compressed text using large language models, improving inference speed and reducing latency, although it has worse perplexity than subword tokenizers for models trained with the same parameter count....
opinion: placeholder
tags:
    - Deep Learning
    - Natural Language Processing
