date: "2025-04-29"
author: Zayd M. K. Zuhri
title: 'Softpick: No Attention Sink, No Massive Activations with Rectified   Softmax'
thumbnail: ""
link: https://huggingface.co/papers/2504.20966
summary: The study presents a new method called softpick that replaces the softmax function in transformer attention mechanisms, eliminating issues like attention sink and massive activations. Experiments show that softpick matches the performance of softmax while using lower memory and computational resources, and it also improves model performance when using low-precision quantization....
opinion: placeholder
tags:
    - ML
