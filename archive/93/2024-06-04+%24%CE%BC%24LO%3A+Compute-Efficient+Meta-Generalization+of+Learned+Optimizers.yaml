date: "2024-06-04"
author: Benjamin Thérien
title: '$μ$LO: Compute-Efficient Meta-Generalization of Learned Optimizers'
thumbnail: https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.00153.png
link: https://huggingface.co/papers/2406.00153
summary: Learned optimizers (LOs) can significantly reduce the wall-clock training time of neural networks, substantially reducing training costs. However, they often suffer from poor meta-generalization, especially when training networks larger than those seen during meta-training. To address this, we use the recently proposed Maximal Update Parametrization (muP), which allows zero-shot generalization of optimizer hyperparameters from smaller to larger models. We extend muP theory to learned optimizers,...
opinion: placeholder
tags:
    - Optimization and Learning Algorithms
    - Deep Learning
