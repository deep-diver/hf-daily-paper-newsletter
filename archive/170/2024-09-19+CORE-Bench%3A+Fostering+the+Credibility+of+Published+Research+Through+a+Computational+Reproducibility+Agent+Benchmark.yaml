date: "2024-09-19"
author: Zachary S. Siegel
title: 'CORE-Bench: Fostering the Credibility of Published Research Through a Computational Reproducibility Agent Benchmark'
thumbnail: ""
link: https://huggingface.co/papers/2409.11363
summary: Introducing CORE-Bench, a benchmark to test AI agents' ability to reproduce scientific research results, with 270 tasks based on 90 papers across three disciplines. Two baseline agents were evaluated, with the best achieving 21% accuracy on the hardest task, highlighting the need for improvement in automating scientific tasks. CORE-Bench aims to advance the development of research agents and improve reproducibility in science....
opinion: placeholder
tags:
    - ML
