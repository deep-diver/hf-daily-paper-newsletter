date: "2025-12-01"
author: Shwai He
title: Understanding and Harnessing Sparsity in Unified Multimodal Models
thumbnail: ""
link: https://huggingface.co/papers/2512.02351
summary: This study analyzes unified multimodal models to understand and reduce inefficiencies, finding that understanding components can be compressed without significant loss of performance, while generation components are more sensitive to compression. The researchers propose a Mixture-of-Experts (MoE) Adaptation technique to improve generation quality by partitioning the generation module into multiple experts and enabling sparse activation, resulting in a model that performs as well as the full mode...
opinion: placeholder
tags:
    - ML
