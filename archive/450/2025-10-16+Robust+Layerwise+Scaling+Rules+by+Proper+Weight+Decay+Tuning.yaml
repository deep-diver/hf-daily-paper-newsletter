date: "2025-10-16"
author: Zhiyuan Fan
title: Robust Layerwise Scaling Rules by Proper Weight Decay Tuning
thumbnail: ""
link: https://huggingface.co/papers/2510.15262
summary: This study proposes a method to maintain consistency in sublayer gains across different widths in modern neural networks by introducing a weight-decay scaling rule for AdamW. The rule enables zero-shot transfer of both learning rate and weight decay from proxy to target widths, eliminating the need for per-width hyperparameter sweeps, and is validated on LLaMA-style Transformers and in a minimal synthetic setting....
opinion: placeholder
tags:
    - ML
