date: "2025-03-26"
author: Haoqin Tu
title: 'ViLBench: A Suite for Vision-Language Process Reward Modeling'
thumbnail: ""
link: https://huggingface.co/papers/2503.20271
summary: The study benchmarks current vision large language models (VLLMs) as output and process reward models on various vision-language tasks, finding no consistent superiority. It then introduces ViLBench, a challenging benchmark designed to require intensive process reward signals, with which GPT-4o with CoT only achieves 27.3% accuracy. Lastly, a 3B model trained with 73.6K vision-language process reward data improves performance on ViLBench by up to 2.5% compared to its untrained counterpart....
opinion: placeholder
tags:
    - ML
