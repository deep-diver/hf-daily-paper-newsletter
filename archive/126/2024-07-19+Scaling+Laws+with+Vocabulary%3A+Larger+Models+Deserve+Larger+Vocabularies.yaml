date: "2024-07-19"
author: Chaofan Tao
title: 'Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies'
thumbnail: ""
link: https://huggingface.co/papers/2407.13623
summary: Larger vocabularies enable more efficient tokenization and improve downstream performance in large language models, but the optimal vocabulary size depends on the available compute budget. Our approaches predict that larger models deserve larger vocabularies, and most LLMs use too small vocabulary sizes. By increasing the vocabulary size, we improve performance on ARC-Challenge from 29.1 to 32.0 with the same FLOPs budget....
opinion: placeholder
tags:
    - ML
