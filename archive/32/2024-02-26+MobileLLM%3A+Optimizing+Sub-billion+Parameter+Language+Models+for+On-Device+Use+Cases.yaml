author: Zechun Liu
date: '2024-02-26'
link: https://huggingface.co/papers/2402.14905
opinion: placeholder
summary: This paper introduces a new language model called MobileLLM that is optimized
  for use on mobile devices. It focuses on creating high-quality models with fewer
  than a billion parameters, which is practical for mobile deployment. The paper also
  proposes a new technique called block-wise weight sharing that further improves
  the accuracy of the model without increasing its size or latency....
tags:
- Supervised Learning
- Deep Learning
- Natural Language Processing
- Optimization and Decision Making
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/SZoqAKLrSSD-cy5SreL4T.png
title: 'MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device
  Use Cases'
translated_paths:
  INT: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.14905/paper.en.html
  KR: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.14905/paper.ko.html
