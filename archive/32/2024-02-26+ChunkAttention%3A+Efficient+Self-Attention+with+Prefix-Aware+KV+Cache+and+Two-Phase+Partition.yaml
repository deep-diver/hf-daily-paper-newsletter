author: Lu Ye
date: '2024-02-26'
link: https://huggingface.co/papers/2402.15220
opinion: placeholder
summary: This paper presents ChunkAttention, a prefix-aware self-attention module
  that improves the memory utilization and speed of self-attention computation in
  large language models by detecting matching prefixes across multiple requests and
  sharing their key/value tensors in memory. This is achieved by breaking key/value
  tensors into smaller chunks and using a two-phase partition algorithm to improve
  data locality during self-attention computation. Experiments show that ChunkAttention
  speeds up the se...
tags:
- Deep Learning
- Natural Language Processing
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/WOW9WqXM5dHIdKaVVO-gj.png
title: 'ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase
  Partition'
translated_paths:
  INT: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.15220/paper.en.html
  KR: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.15220/paper.ko.html
