date: "2025-01-28"
author: An Yang
title: Qwen2.5-1M Technical Report
thumbnail: ""
link: https://huggingface.co/papers/2501.15383
summary: The extended Qwen2.5-1M models, Qwen2.5-7B-Instruct-1M and Qwen2.5-14B-Instruct-1M, along with the API model Qwen2.5-Turbo, significantly improve long-context task performance compared to previous 128K versions. These models achieve this through long-context pre-training and post-training, while reducing training costs with techniques like length extrapolation, sparse attention, and inference engine optimizations. The Qwen2.5-14B-Instruct-1M model, in particular, surpasses GPT-4o-mini in long-co...
opinion: placeholder
tags:
    - ML
