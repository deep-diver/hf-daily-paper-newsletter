date: "2024-02-16"
author: Yao Fu
title: Data Engineering for Scaling Language Models to 128K Context
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/ehuohhxYHTO_a2Y9CRP6j.png
link: https://huggingface.co/papers/2402.10171
summary: The paper presents a lightweight way to scale the context lengths of language models to 128K by continual pretraining on appropriate data mixtures, and investigates the quantity and quality of the data needed for this process. The recipe outperforms existing long-context models and approaches the performance of models like GPT-4 128K....
opinion: placeholder
tags:
    - Natural Language Processing
