author: James Liu
date: '2024-02-16'
link: https://huggingface.co/papers/2402.10193
opinion: placeholder
summary: The paper presents BitDelta, a method that successfully quantizes the weights
  of fine-tuned models down to 1 bit without compromising performance, highlighting
  the potential redundancy of information added during fine-tuning and its implications
  for multi-tenant serving and storage of fine-tuned models....
tags:
- Deep Learning
- Optimization and Learning Algorithms
thumbnail: https://github.com/deep-diver/hf-daily-paper-newsletter/blob/main/assets/2402.10193.gif?raw=true
title: 'BitDelta: Your Fine-Tune May Only Be Worth One Bit'
translated_paths:
  INT: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.10193/paper.en.html
  KR: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.10193/paper.ko.html
