author: Penghui Qi
date: '2024-01-22'
link: https://huggingface.co/papers/2401.10241
opinion: placeholder
summary: This paper introduces a scheduling strategy for synchronous training to achieve
  zero pipeline bubbles, a common issue with pipeline parallelism in large-scale distributed
  training. The method involves splitting the backward computation into two parts
  and novel pipeline schedules that outperform the baselines. An algorithm is also
  developed to find an optimal schedule based on specific model configuration and
  memory limit. The results show a significant increase in throughput up to 31% compared
  t...
tags:
- Optimization and Learning Algorithms
- Deep Learning
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/gITPhCb7DEvKRiVeZUuI0.png
title: Zero Bubble Pipeline Parallelism
translated_paths:
  KR: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2401.10241/paper.ko.html
