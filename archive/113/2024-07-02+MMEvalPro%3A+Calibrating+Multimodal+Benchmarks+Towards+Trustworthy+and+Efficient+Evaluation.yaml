date: "2024-07-02"
author: Jinsheng Huang
title: 'MMEvalPro: Calibrating Multimodal Benchmarks Towards Trustworthy and Efficient Evaluation'
thumbnail: ""
link: https://huggingface.co/papers/2407.00468
summary: MMEvalPro is a benchmark designed to evaluate the performance of Large Multimodal Models (LMMs) in cross-modal understanding and reasoning. It uses a trilogy evaluation pipeline and more rigorous metrics to avoid systematic biases and Type-I errors in existing benchmarks. MMEvalPro has 6,414 distinct questions, two-thirds of which are manually labeled by human experts. Experiments show that MMEvalPro is more challenging and trustworthy compared to existing benchmarks, and can help advance future...
opinion: placeholder
tags:
    - ML
