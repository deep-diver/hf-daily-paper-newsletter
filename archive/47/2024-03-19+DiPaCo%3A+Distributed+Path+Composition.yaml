author: Arthur Douillard
date: '2024-03-19'
link: https://huggingface.co/papers/2403.10616
opinion: placeholder
summary: DiPaCo is a new approach for training ML models in a distributed manner,
  by distributing computation through shared modules. It reduces communication and
  allows training on poorly connected and heterogeneous workers, and at inference
  time only a single path is executed without model compression. Experiments show
  improved performance compared to a 1 billion-parameter dense transformer model....
tags:
- Distributed Learning
- Deep Learning
- Optimization and Learning Algorithms
- Emerging Applications of Machine Learning
thumbnail: https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.10616.png
title: 'DiPaCo: Distributed Path Composition'
translated_paths:
  INT: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2403.10616/paper.en.html
  KR: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2403.10616/paper.ko.html
