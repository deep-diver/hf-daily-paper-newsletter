date: "2025-01-31"
author: Yue Wang
title: 'Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs'
thumbnail: ""
link: https://huggingface.co/papers/2501.18585
summary: The paper discusses a phenomenon called 'underthinking' in large language models like o1, where they switch between reasoning thoughts without fully exploring each path, leading to decreased performance. The authors propose a decoding strategy with thought switching penalty TIP to encourage deeper exploration of each reasoning path, improving accuracy across challenging datasets....
opinion: placeholder
tags:
    - ML
