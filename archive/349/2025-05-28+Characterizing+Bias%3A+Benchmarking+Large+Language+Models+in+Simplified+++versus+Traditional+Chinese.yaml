date: "2025-05-28"
author: Hanjia Lyu
title: 'Characterizing Bias: Benchmarking Large Language Models in Simplified   versus Traditional Chinese'
thumbnail: ""
link: https://huggingface.co/papers/2505.22645
summary: The study examines how Large Language Models (LLMs) perform when prompted in Simplified and Traditional Chinese, using benchmark tasks like regional term choice and regional name choice. Results show that LLMs have biases based on the task and prompting language, which could be due to training data representation, character preferences, and tokenization differences. The research highlights the need for more analysis of LLM biases and provides an open-sourced benchmark dataset for future evaluati...
opinion: placeholder
tags:
    - ML
