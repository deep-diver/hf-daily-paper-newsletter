date: "2025-05-25"
author: Mahdi Nikdan
title: Efficient Data Selection at Scale via Influence Distillation
thumbnail: ""
link: https://huggingface.co/papers/2505.19051
summary: The authors present a new method called Influence Distillation that selects efficient data for training Large Language Models by calculating the impact of each sample on the target distribution. This approach is faster and performs as well or better than existing methods, making it a promising tool for LLM fine-tuning....
opinion: placeholder
tags:
    - ML
