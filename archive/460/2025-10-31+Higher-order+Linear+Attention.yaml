date: "2025-10-31"
author: Yifan Zhang
title: Higher-order Linear Attention
thumbnail: ""
link: https://huggingface.co/papers/2510.27258
summary: The authors propose a new method called Higher-order Linear Attention (HLA) that can handle longer contexts in language models more efficiently than previous methods. HLA can compute per-token outputs in linear time without using large matrices, and it can be extended to higher orders, making it a promising building block for future language models....
opinion: placeholder
tags:
    - ML
