date: "2025-10-05"
author: Haiquan Qiu
title: 'Why Low-Precision Transformer Training Fails: An Analysis on Flash   Attention'
thumbnail: ""
link: https://huggingface.co/papers/2510.04212
summary: This study explains why training transformer models with flash attention in low-precision formats often fails due to similar low-rank representations and biased rounding errors, leading to loss explosions. The researchers propose a minimal modification to flash attention that reduces rounding errors, successfully stabilizing the training process....
opinion: placeholder
tags:
    - ML
