date: "2025-06-17"
author: Zhengxiang Cheng
title: Optimizing Length Compression in Large Reasoning Models
thumbnail: ""
link: https://huggingface.co/papers/2506.14755
summary: This study identifies 'invalid thinking' as a cause of verbosity in Large Reasoning Models and proposes two new principles, Brevity and Sufficiency, to eliminate redundancy while preserving critical reasoning steps. The researchers introduce LC-R1, a post-training method that significantly reduces sequence length with minimal accuracy loss, demonstrating its robustness and providing insights for more efficient LRMs....
opinion: placeholder
tags:
    - ML
