date: "2024-10-07"
author: Dung Nguyen Manh
title: 'CodeMMLU: A Multi-Task Benchmark for Assessing Code Understanding Capabilities of CodeLLMs'
thumbnail: ""
link: https://huggingface.co/papers/2410.01999
summary: CodeMMLU is a benchmark for evaluating the code understanding capabilities of large language models. It includes 10,000 questions on code analysis, defect detection, and software engineering principles across multiple programming languages. The benchmark assesses models' ability to reason about code rather than just generate it, providing insights into their grasp of complex software concepts and systems. Evaluation shows that even state-of-the-art models struggle with CodeMMLU, highlighting the...
opinion: placeholder
tags:
    - ML
