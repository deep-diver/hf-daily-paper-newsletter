date: "2024-10-07"
author: Yaniv Leviathan
title: Selective Attention Improves Transformer
thumbnail: ""
link: https://huggingface.co/papers/2410.02703
summary: The paper introduces Selective Attention, a modification to the standard attention mechanism that improves language modeling performance and reduces memory and compute requirements during inference by reducing attention to unneeded elements. This leads to meaningful reductions in memory and compute requirements during inference, as shown by the example of transformers with 100M parameters trained on C4 with context sizes of 512, 1,024, and 2,048 needing 16X, 25X, and 47X less memory for their at...
opinion: placeholder
tags:
    - ML
