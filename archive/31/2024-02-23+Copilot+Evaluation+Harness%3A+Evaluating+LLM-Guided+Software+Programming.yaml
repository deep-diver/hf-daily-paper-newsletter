author: Anisha Agarwal
date: '2024-02-23'
link: https://huggingface.co/papers/2402.14261
opinion: placeholder
summary: The authors present a new tool, called the Copilot evaluation harness, to
  evaluate different Large Language Models (LLMs) for their effectiveness as programming
  assistants in development environments. The tool has various metrics to measure
  success, such as generating code, documentation, test cases, fixing bugs, and understanding
  the workspace. It provides a more robust and detailed evaluation of LLMs in IDEs
  compared to existing systems....
tags:
- Supervised Learning
- Deep Learning
- Natural Language Processing
- Software Engineering
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/DT1GZteSe00AqW0yXx_nw.png
title: 'Copilot Evaluation Harness: Evaluating LLM-Guided Software Programming'
translated_paths:
  INT: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.14261/paper.en.html
  KR: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.14261/paper.ko.html
