date: "2025-05-21"
author: Xinyin Ma
title: 'dKV-Cache: The Cache for Diffusion Language Models'
thumbnail: ""
link: https://huggingface.co/papers/2505.15781
summary: This study presents a new caching strategy, dKV-Cache, to speed up the inference process of diffusion language models (DLMs). The proposed method offers 2-10 times faster inference, improving performance on long sequences and making DLMs more competitive with autoregressive language models in terms of speed....
opinion: placeholder
tags:
    - ML
