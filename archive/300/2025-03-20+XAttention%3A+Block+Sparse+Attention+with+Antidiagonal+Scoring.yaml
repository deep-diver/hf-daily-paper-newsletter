date: "2025-03-20"
author: Ruyi Xu
title: 'XAttention: Block Sparse Attention with Antidiagonal Scoring'
thumbnail: ""
link: https://huggingface.co/papers/2503.16428
summary: XAttention is a block sparse attention framework that improves computation efficiency in long-context Transformer models by using antidiagonal sum values as a proxy for block importance, resulting in substantial acceleration and comparable accuracy to full attention....
opinion: placeholder
tags:
    - ML
