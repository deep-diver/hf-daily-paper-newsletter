date: "2025-06-23"
author: Zichong Li
title: 'SlimMoE: Structured Compression of Large MoE Models via Expert Slimming   and Distillation'
thumbnail: ""
link: https://huggingface.co/papers/2506.18349
summary: The paper presents a method to make large Mixture of Experts (MoE) models smaller and more efficient by reducing their memory requirements, enabling deployment in resource-limited environments. The proposed framework, SlimMoE, creates smaller MoE models that maintain high performance, as demonstrated by the creation of Phi-mini-MoE and Phi-tiny-MoE, which outperform other models of similar size and remain competitive with larger ones....
opinion: placeholder
tags:
    - ML
