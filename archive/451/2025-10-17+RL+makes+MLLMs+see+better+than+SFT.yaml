date: "2025-10-17"
author: Junha Song
title: RL makes MLLMs see better than SFT
thumbnail: ""
link: https://huggingface.co/papers/2510.16333
summary: This study compares the effects of two training methods, RL and SFT, on vision encoders in Multimodal Language Models (MLLM). The results show that RL creates stronger and more precise visual representations than SFT, leading to better performance in vision-related tasks. The researchers then propose a new training method, PIVOT, which significantly improves vision encoder performance with less computational cost....
opinion: placeholder
tags:
    - ML
