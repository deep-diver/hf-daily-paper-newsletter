date: "2025-06-26"
author: Ziyue Li
title: Where to find Grokking in LLM Pretraining? Monitor   Memorization-to-Generalization without Test
thumbnail: ""
link: https://huggingface.co/papers/2506.21551
summary: This study investigates the phenomenon of grokking in large language models during pretraining, finding that training samples' pathways evolve from random to more structured, indicating a conversion from memorization to generalization. The researchers also develop new metrics to predict generalization improvement, which can monitor model performance without the need for finetuning or testing....
opinion: placeholder
tags:
    - ML
