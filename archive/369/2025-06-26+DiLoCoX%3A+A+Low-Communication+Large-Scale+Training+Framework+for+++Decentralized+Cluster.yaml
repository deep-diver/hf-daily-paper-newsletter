date: "2025-06-26"
author: Ji Qi
title: 'DiLoCoX: A Low-Communication Large-Scale Training Framework for   Decentralized Cluster'
thumbnail: ""
link: https://huggingface.co/papers/2506.21263
summary: The authors present DiLoCoX, a framework that enables efficient training of large language models on slow, decentralized networks by combining pipeline parallelism, dual optimizer policy, and adaptive gradient compression. This approach allows for a 357x speedup in training compared to traditional methods, even for models with over 100 billion parameters....
opinion: placeholder
tags:
    - ML
