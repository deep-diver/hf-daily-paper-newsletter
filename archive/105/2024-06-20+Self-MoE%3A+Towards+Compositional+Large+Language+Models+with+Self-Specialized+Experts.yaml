date: "2024-06-20"
author: Junmo Kang
title: 'Self-MoE: Towards Compositional Large Language Models with Self-Specialized Experts'
thumbnail: ""
link: https://huggingface.co/papers/2406.12034
summary: Self-MoE is a new approach that turns a large language model into a system of smaller, specialized experts. These experts are trained using synthetic data and can handle different tasks better than the original model, without needing a lot of human-labeled data. Self-MoE also has advantages over other methods and offers better flexibility and interpretability....
opinion: placeholder
tags:
    - ML
