date: "2025-03-04"
author: Disen Lan
title: 'Liger: Linearizing Large Language Models to Gated Recurrent Structures'
thumbnail: ""
link: https://huggingface.co/papers/2503.01496
summary: Transformers with linear recurrent modeling offer linear-time training and constant-memory inference. Despite their demonstrated efficiency and performance, pretraining such non-standard architectures from scratch remains costly and risky. The linearization of large language models (LLMs) transforms pretrained standard models into linear recurrent structures, enabling more efficient deployment. However, current linearization methods typically introduce additional feature map modules that require...
opinion: placeholder
tags:
    - ML
