date: "2024-02-19"
author: Yuri Kuratov
title: 'In Search of Needles in a 10M Haystack: Recurrent Memory Finds What LLMs Miss'
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/Put7Y4YOMN1HqP7JPDK2L.png
link: https://huggingface.co/papers/2402.10790
summary: This paper addresses the challenge of processing long documents using generative transformer models. To evaluate different approaches, we introduce BABILong, a new benchmark designed to assess model capabilities in extracting and processing distributed facts within extensive texts. Our evaluation, which includes benchmarks for GPT-4 and RAG, reveals that common methods are effective only for sequences up to 10^4 elements. In contrast, fine-tuning GPT-2 with recurrent memory augmentations enables...
opinion: placeholder
tags:
    - Natural Language Processing
