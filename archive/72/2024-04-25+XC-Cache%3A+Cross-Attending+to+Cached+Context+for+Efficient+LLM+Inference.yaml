date: "2024-04-25"
author: Jo√£o Monteiro
title: 'XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference'
thumbnail: https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.15420.png
link: https://huggingface.co/papers/2404.15420
summary: This paper introduces a new method, XC-Cache, for caching and using context during language model generation. Their method uses cross-attention and is inspired by the encoder-decoder architecture. They evaluate their method on Question-Answering (QA) tasks and find that it outperforms in-context learning and reduces the space footprint compared to standard KV caching....
opinion: placeholder
tags:
    - Deep Learning
    - Natural Language Processing
