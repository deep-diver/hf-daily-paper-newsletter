date: "2024-10-01"
author: Gabriel Mongaras
title: 'Cottention: Linear Transformers With Cosine Attention'
thumbnail: ""
link: https://huggingface.co/papers/2409.18747
summary: Cottention is a new type of attention mechanism that uses cosine similarity instead of the softmax operation, which makes it more memory-efficient than softmax attention. It can be made to use a constant amount of memory during inference and performs just as well as softmax attention while using less memory....
opinion: placeholder
tags:
    - ML
