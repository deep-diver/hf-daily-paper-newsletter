date: "2025-10-09"
author: Ruizhe Wang
title: 'Recycling Pretrained Checkpoints: Orthogonal Growth of   Mixture-of-Experts for Efficient Large Language Model Pre-Training'
thumbnail: ""
link: https://huggingface.co/papers/2510.08008
summary: The authors propose a method to recycle existing Large Language Model checkpoints by expanding their parameters and continuing training, which proves to be more efficient and cost-effective. Experiments show that using more 'sunk' cost (pretrained checkpoints) leads to better performance, and their approach achieves a significant accuracy gain over training from scratch....
opinion: placeholder
tags:
    - ML
