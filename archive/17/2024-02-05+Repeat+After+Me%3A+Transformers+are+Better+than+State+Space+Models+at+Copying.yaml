author: Samy Jelassi
date: '2024-02-05'
link: https://huggingface.co/papers/2402.01032
opinion: placeholder
summary: This paper compares transformer models and generalized state space models
  (GSSMs) and finds that transformer models are better at tasks that require copying
  information from an input context, both in theory and in practice, due to their
  ability to copy strings of exponential length....
tags:
- Supervised Learning
- Deep Learning
- Natural Language Processing
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/tIDlj9EG58UZBx74gENaf.png
title: 'Repeat After Me: Transformers are Better than State Space Models at Copying'
translated_paths:
  INT: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.01032/paper.en.html
  KR: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.01032/paper.ko.html
