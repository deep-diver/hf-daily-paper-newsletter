date: "2025-05-15"
author: Binghai Wang
title: 'WorldPM: Scaling Human Preference Modeling'
thumbnail: ""
link: https://huggingface.co/papers/2505.10527
summary: The study explores how preference modeling improves with larger models and datasets, similar to language modeling. They gather preference data from various online communities and train models with 1.5 billion to 72 billion parameters. The results show that deceptive feature detection and objective knowledge improve with larger models, while subjective preferences do not. The proposed World Preference Modeling technique enhances performance on various preference benchmarks and integrated RLHF pip...
opinion: placeholder
tags:
    - ML
