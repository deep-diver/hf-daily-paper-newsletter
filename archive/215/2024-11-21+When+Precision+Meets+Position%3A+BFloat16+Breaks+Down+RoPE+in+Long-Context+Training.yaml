date: "2024-11-21"
author: Haonan Wang
title: 'When Precision Meets Position: BFloat16 Breaks Down RoPE in Long-Context Training'
thumbnail: ""
link: https://huggingface.co/papers/2411.13476
summary: Extending context window sizes allows large language models (LLMs) to process longer sequences and handle more complex tasks. Rotary Positional Embedding (RoPE) has become the de facto standard due to its relative positional encoding properties that benefit long-context training. However, we observe that using RoPE with BFloat16 format results in numerical issues, causing it to deviate from its intended relative positional encoding, especially in long-context scenarios. This issue arises from BF...
opinion: placeholder
tags:
    - ML
