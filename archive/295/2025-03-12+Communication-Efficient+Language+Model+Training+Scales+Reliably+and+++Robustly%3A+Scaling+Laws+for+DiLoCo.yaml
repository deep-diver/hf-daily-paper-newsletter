date: "2025-03-12"
author: Zachary Charles
title: 'Communication-Efficient Language Model Training Scales Reliably and   Robustly: Scaling Laws for DiLoCo'
thumbnail: ""
link: https://huggingface.co/papers/2503.09799
summary: The work investigates DiLoCo, a less frequent synchronization approach for language model training, and finds it scales predictably and robustly with model size. When optimized, DiLoCo can outperform data-parallel training with larger models and even at small sizes, with benefits including larger optimal batch sizes, better downstream generalization, and improved evaluation loss for a fixed token budget....
opinion: placeholder
tags:
    - ML
