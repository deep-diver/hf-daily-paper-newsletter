date: "2025-03-13"
author: Jiachen Zhu
title: Transformers without Normalization
thumbnail: ""
link: https://huggingface.co/papers/2503.10622
summary: The paper presents Dynamic Tanh (DyT) as a replacement for normalization layers in Transformer models, achieving comparable or better performance across various tasks. The use of DyT, which resembles a tanh function, simplifies the architecture and challenges the traditional belief that normalization layers are necessary in modern neural networks....
opinion: placeholder
tags:
    - ML
