date: "2024-11-11"
author: Weiquan Huang
title: 'LLM2CLIP: Powerful Language Model Unlock Richer Visual Representation'
thumbnail: ""
link: https://huggingface.co/papers/2411.04997
summary: This paper proposes a method called LLM2CLIP that uses the capabilities of large language models (LLMs) to improve multimodal representation learning in CLIP. By fine-tuning the LLM in the caption space and using it as a teacher for CLIP's visual encoder, the method improves the output layer's textual discriminability and allows for the incorporation of longer and more complex captions. The approach brings substantial improvements in cross-modal tasks....
opinion: placeholder
tags:
    - ML
