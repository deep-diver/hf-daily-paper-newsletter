date: "2024-03-26"
author: Junyuan Hong
title: 'Decoding Compressed Trust: Scrutinizing the Trustworthiness of Efficient LLMs Under Compression'
thumbnail: https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.15447.png
link: https://huggingface.co/papers/2403.15447
summary: This study evaluates the trustworthiness of compressed large language models (LLMs) using five state-of-the-art compression techniques and finds that quantization is more effective than pruning for achieving efficiency and trustworthiness. The study provides practical recommendations for simultaneously achieving high utility, efficiency, and trustworthiness in LLMs....
opinion: placeholder
tags:
    - Supervised Learning
    - Natural Language Processing
    - Optimization and Learning Algorithms
