author: Peiqin Lin
date: '2024-01-25'
link: https://huggingface.co/papers/2401.13303
opinion: placeholder
summary: The authors introduce MaLA-500, a large language model designed to cover
  a wide range of 534 languages by extending vocabulary and continued pretraining.
  MaLA-500 achieves state-of-the-art results on in-context learning tasks on SIB-200....
tags:
- Natural Language Processing
- Deep Learning
- Supervised Learning
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/IU4V5Ei5MnxVNsNkOMvdg.png
title: 'MaLA-500: Massive Language Adaptation of Large Language Models'
translated_paths:
  INT: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2401.13303/paper.en.html
  KR: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2401.13303/paper.ko.html
