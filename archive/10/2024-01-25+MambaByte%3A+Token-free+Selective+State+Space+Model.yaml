author: Junxiong Wang
date: '2024-01-25'
link: https://huggingface.co/papers/2401.13660
opinion: placeholder
summary: MambaByte is a new language model that learns directly from raw bytes instead
  of words or subwords. It is an autoregressive model that can handle long sequences
  better than other byte-level models and shows competitive performance with state-of-the-art
  subword models. MambaByte also has faster inference compared to Transformers due
  to its linear scaling in length....
tags:
- Deep Learning
- Natural Language Processing
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/B-gai_aNx76hER6seSPYz.png
title: 'MambaByte: Token-free Selective State Space Model'
translated_paths:
  INT: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2401.13660/paper.en.html
  KR: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2401.13660/paper.ko.html
