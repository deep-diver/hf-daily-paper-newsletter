date: "2025-02-06"
author: Daniil Tiapkin
title: On Teacher Hacking in Language Model Distillation
thumbnail: ""
link: https://huggingface.co/papers/2502.02671
summary: The paper explores the concept of 'teacher hacking' in language model distillation, where the student model learns to mimic the teacher model's imperfections rather than the true distribution. It presents a controlled experiment to study this phenomenon and finds that using fixed offline datasets leads to teacher hacking, while online data generation techniques, particularly data diversity, can help mitigate it....
opinion: placeholder
tags:
    - ML
