date: "2024-04-11"
author: Tsendsuren Munkhdalai
title: 'Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention'
thumbnail: https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.07143.png
link: https://huggingface.co/papers/2404.07143
summary: This paper introduces a new attention technique called Infini-attention that allows for scaling up Large Language Models (LLMs) to infinitely long inputs with limited memory and computation. The approach is effective in long-context language modeling tasks, and it introduces minimal memory parameters....
opinion: placeholder
tags:
    - Deep Learning
    - Natural Language Processing
    - Optimization and Learning Algorithms
