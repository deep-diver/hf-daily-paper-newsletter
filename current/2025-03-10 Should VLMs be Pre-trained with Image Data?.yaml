date: "2025-03-10"
author: Sedrick Keh
title: Should VLMs be Pre-trained with Image Data?
thumbnail: ""
link: https://huggingface.co/papers/2503.07603
summary: This study examines the impact of integrating image data during pre-training for vision-language models (VLMs), comparing single- and two-step pipelines. Results indicate that pre-training with both image and text data enhances performance on vision-language tasks, while maintaining strong text-only evaluation results. For a 1B model, introducing visual tokens 80% through pre-training yields an average 2% improvement over a fully pre-trained model, across 6 diverse tasks....
opinion: placeholder
tags:
    - ML
