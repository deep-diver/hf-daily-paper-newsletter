date: "2024-01-18"
author: Lianghui Zhu
title: 'Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model'
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/3CR9QVEIjbOr4I8mOvDy1.png
link: https://huggingface.co/papers/2401.09417
summary: 'This paper proposes a new vision backbone, Vision Mamba (Vim), for efficient visual representation learning using bidirectional state space models (SSMs) with position embeddings. Vim is shown to achieve higher performance compared to existing vision transformers like DeiT, while using less computational resources and memory. This makes Vim a potential next-generation backbone for foundation vision models, and code is available at: https://github.com/hustvl/Vim....'
opinion: placeholder
tags:
    - Computer Vision
    - Deep Learning
