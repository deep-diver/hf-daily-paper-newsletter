date: "2024-10-11"
author: Yuxin Xiao
title: 'SFTMix: Elevating Language Model Instruction Tuning with Mixup Recipe'
thumbnail: ""
link: https://huggingface.co/papers/2410.05248
summary: To induce desired behaviors in large language models (LLMs) for interaction-driven tasks, the instruction-tuning stage typically trains LLMs on instruction-response pairs using the next-token prediction (NTP) loss. Previous work aiming to improve instruction-tuning performance often emphasizes the need for higher-quality supervised fine-tuning (SFT) datasets, which typically involves expensive data filtering with proprietary LLMs or labor-intensive data generation by human annotators. However, t...
opinion: placeholder
tags:
    - ML
