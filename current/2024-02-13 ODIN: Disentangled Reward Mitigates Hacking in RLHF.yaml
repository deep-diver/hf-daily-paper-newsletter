date: "2024-02-13"
author: Lichang Chen
title: 'ODIN: Disentangled Reward Mitigates Hacking in RLHF'
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/nNhbEB8zih6j60ulOZqtA.png
link: https://huggingface.co/papers/2402.07319
summary: In this work, we study the issue of reward hacking on the response length, a challenge emerging in Reinforcement Learning from Human Feedback (RLHF) on LLMs. A well-formatted, verbose but less helpful response from the LLMs can often deceive LLMs or even human evaluators to achieve high scores. The same issue also holds for some reward models in RL. To address the challenges in both training and evaluation, we establish a more reliable evaluation protocol for comparing different training configu...
opinion: placeholder
tags:
    - Reinforcement Learning
    - Natural Language Processing
