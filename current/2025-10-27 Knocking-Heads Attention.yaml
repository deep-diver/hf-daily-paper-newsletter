date: "2025-10-27"
author: Zhanchao Zhou
title: Knocking-Heads Attention
thumbnail: ""
link: https://huggingface.co/papers/2510.23052
summary: The authors propose a new attention mechanism called Knocking-Heads Attention (KHA) that allows attention heads to interact with each other, improving representation learning in large language models. KHA adds minimal parameters and can be integrated into various attention variants, demonstrating superior performance in training a 6.1B parameter MoE model....
opinion: placeholder
tags:
    - ML
