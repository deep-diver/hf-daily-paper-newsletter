date: "2024-02-26"
author: Zechun Liu
title: 'MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases'
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/SZoqAKLrSSD-cy5SreL4T.png
link: https://huggingface.co/papers/2402.14905
summary: This paper introduces a new language model called MobileLLM that is optimized for use on mobile devices. It focuses on creating high-quality models with fewer than a billion parameters, which is practical for mobile deployment. The paper also proposes a new technique called block-wise weight sharing that further improves the accuracy of the model without increasing its size or latency....
opinion: placeholder
tags:
    - Supervised Learning
    - Deep Learning
    - Natural Language Processing
    - Optimization and Decision Making
