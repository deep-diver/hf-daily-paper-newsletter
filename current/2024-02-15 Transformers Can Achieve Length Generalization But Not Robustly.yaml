date: "2024-02-15"
author: Yongchao Zhou
title: Transformers Can Achieve Length Generalization But Not Robustly
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/guy11n8ESAiCLOa_LX6q-.png
link: https://huggingface.co/papers/2402.09371
summary: The paper tests the Transformer's ability to generalize from shorter training sequences to longer test ones for the task of addition of two integers and finds that it is fragile, varying significantly with factors like weight initialization and data order, but with the right data format and position encodings, can extrapolate to 2.5x input length....
opinion: placeholder
tags:
    - Supervised Learning
    - Deep Learning
    - Optimization and Learning Algorithms
    - Natural Language Processing
