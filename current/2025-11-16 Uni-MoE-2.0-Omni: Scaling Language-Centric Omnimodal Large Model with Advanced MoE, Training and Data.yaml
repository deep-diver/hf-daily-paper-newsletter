date: "2025-11-16"
author: Yunxin Li
title: 'Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data'
thumbnail: ""
link: https://huggingface.co/papers/2511.12609
summary: The Lychee family's Uni-MoE 2.0 is a fully open-source omnimodal large model that improves language-centric multimodal understanding and generation. It uses a dynamic-capacity MoE design, a progressive training strategy, and a multimodal data matching technique to achieve superior performance in various benchmarks compared to leading models....
opinion: placeholder
tags:
    - ML
