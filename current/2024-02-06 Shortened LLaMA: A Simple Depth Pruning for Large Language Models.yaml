author: Bo-Kyeong Kim
date: '2024-02-06'
link: https://huggingface.co/papers/2402.02834
opinion: placeholder
summary: This paper explores a simple depth pruning technique for large language models,
  which removes entire layers, and finds that it performs as effectively as more complex
  width pruning methods in terms of task performance while improving inference speed,
  particularly in memory-constrained settings....
tags:
- Deep Learning
- Natural Language Processing
- Optimization and Learning Algorithms
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/9WG9Jjj3VuYe5iPRe1HPe.png
title: 'Shortened LLaMA: A Simple Depth Pruning for Large Language Models'
translated_paths:
  INT: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.02834/paper.en.html
  KR: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.02834/paper.ko.html
