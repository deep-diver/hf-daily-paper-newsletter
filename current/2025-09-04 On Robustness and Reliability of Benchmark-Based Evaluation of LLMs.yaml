date: "2025-09-04"
author: Riccardo Lunardi
title: On Robustness and Reliability of Benchmark-Based Evaluation of LLMs
thumbnail: ""
link: https://huggingface.co/papers/2509.04013
summary: This study tests how well large language models (LLMs) work with questions worded differently and found that while the models' rankings stayed the same, their scores dropped significantly, suggesting they struggle with varied language and calling into question the reliability of existing evaluation methods....
opinion: placeholder
tags:
    - ML
