date: "2025-02-07"
author: Yik Siu Chan
title: 'Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions'
thumbnail: ""
link: https://huggingface.co/papers/2502.04322
summary: This research examines how large language models (LLMs) can be manipulated into performing harmful actions through simple, multi-step, and multilingual interactions. The study introduces a new attack framework, Speak Easy, and a metric, HarmScore, to measure the effectiveness of LLM responses in enabling harmful actions. Results show that the framework can significantly increase the Attack Success Rate and HarmScore in both open-source and proprietary LLMs across various safety benchmarks....
opinion: placeholder
tags:
    - ML
