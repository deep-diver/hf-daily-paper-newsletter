date: "2025-05-21"
author: Yuqing Yang
title: When Do LLMs Admit Their Mistakes? Understanding the Role of Model   Belief in Retraction
thumbnail: ""
link: https://huggingface.co/papers/2505.16170
summary: The study investigates why large language models (LLMs) sometimes fail to correct their mistakes, even when they know they're wrong. Researchers found that LLMs are less likely to retract incorrect answers if they 'believe' them to be correct, and that improving the models' internal beliefs can significantly enhance their retraction performance....
opinion: placeholder
tags:
    - ML
