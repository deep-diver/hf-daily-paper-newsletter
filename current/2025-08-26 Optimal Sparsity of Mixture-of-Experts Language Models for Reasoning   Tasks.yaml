date: "2025-08-26"
author: Taishi Nakamura
title: Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning   Tasks
thumbnail: ""
link: https://huggingface.co/papers/2508.18672
summary: This study examines how sparsity in Mixture-of-Experts (MoE) models affects their performance in memorization and reasoning tasks. The researchers trained various MoE Transformers, adjusting parameters and sparsity levels, and found that reasoning abilities plateau and may even decline with increased sparsity, while memorization improves with more parameters....
opinion: placeholder
tags:
    - ML
