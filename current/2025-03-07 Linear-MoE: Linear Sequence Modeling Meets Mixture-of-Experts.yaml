date: "2025-03-07"
author: Weigao Sun
title: 'Linear-MoE: Linear Sequence Modeling Meets Mixture-of-Experts'
thumbnail: ""
link: https://huggingface.co/papers/2503.05447
summary: The paper describes a production-level system called Linear-MoE, which combines Linear Sequence Modeling (LSM) and Mixture-of-Experts (MoE) to improve performance and efficiency in large-scale model training. Linear-MoE comprises a modeling subsystem supporting LSM instances and a training subsystem using Sequence Parallelism technology, achieving efficiency gains and competitive performance on various benchmarks....
opinion: placeholder
tags:
    - ML
