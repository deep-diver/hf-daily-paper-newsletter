date: "2024-07-11"
author: Alexandre Matton
title: On Leakage of Code Generation Evaluation Datasets
thumbnail: ""
link: https://huggingface.co/papers/2407.07565
summary: 'This paper discusses contamination in code generation test sets, specifically in modern large language models. The paper identifies three sources of contamination: direct data leakage, indirect leakage through synthetic data, and overfitting to evaluation sets during model selection. The study uses a new dataset of 161 prompts and their associated Python solutions, available at https://huggingface.co/datasets/CohereForAI/lbpp, to support their findings....'
opinion: placeholder
tags:
    - ML
