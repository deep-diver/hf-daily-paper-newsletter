date: "2024-07-08"
author: Zhexin Zhang
title: 'Safe Unlearning: A Surprisingly Effective and Generalizable Solution to Defend Against Jailbreak Attacks'
thumbnail: ""
link: https://huggingface.co/papers/2407.02855
summary: This paper proposes a new approach to defending against jailbreak attacks on large language models (LLMs) by unlearning harmful knowledge. The approach is shown to be effective and generalizable, reducing the attack success rate on out-of-distribution harmful questions by 92% compared to a model fine-tuned on safety alignment samples. The generalization ability of the approach is attributed to the intrinsic relatedness among harmful responses across harmful questions....
opinion: placeholder
tags:
    - ML
