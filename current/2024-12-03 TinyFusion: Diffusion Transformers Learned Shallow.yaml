date: "2024-12-03"
author: Gongfan Fang
title: 'TinyFusion: Diffusion Transformers Learned Shallow'
thumbnail: ""
link: https://huggingface.co/papers/2412.01199
summary: TinyFusion is a method that removes unnecessary layers from diffusion transformers to make them more efficient, while still keeping their performance high after fine-tuning. It uses a technique called learnable pruning, which helps the model recover its performance after being made smaller. Experiments show that TinyFusion works well for different types of diffusion transformers and can make them faster and more efficient without sacrificing much performance....
opinion: placeholder
tags:
    - ML
