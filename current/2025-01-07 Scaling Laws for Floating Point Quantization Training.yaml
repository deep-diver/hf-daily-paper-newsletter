date: "2025-01-07"
author: Xingwu Sun
title: Scaling Laws for Floating Point Quantization Training
thumbnail: ""
link: https://huggingface.co/papers/2501.02423
summary: This paper explores the effects of floating-point quantization targets, exponent bits, mantissa bits, and the calculation granularity of the scaling factor in floating-point quantization training performance of LLM models. It presents an accurate floating-point quantization unified scaling law and provides valuable suggestions for the community, including the optimal exponent-mantissa bit ratio and the critical data size for low-precision LLM training. The optimal floating-point quantization pre...
opinion: placeholder
tags:
    - ML
