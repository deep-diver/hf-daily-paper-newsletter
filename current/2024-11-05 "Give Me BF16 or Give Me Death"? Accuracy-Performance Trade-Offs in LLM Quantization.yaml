date: "2024-11-05"
author: Eldar Kurtic
title: '"Give Me BF16 or Give Me Death"? Accuracy-Performance Trade-Offs in LLM Quantization'
thumbnail: ""
link: https://huggingface.co/papers/2411.02355
summary: An empirical study of quantized accuracy in LLMs across academic benchmarks and real-world tasks, examining various quantization formats and finding that FP8 weight and activation quantization is lossless, INT8 weight and activation quantization incurs low accuracy degradation, and INT4 weight-only quantization is competitive. Inference performance analysis suggests the best format for deployment based on scale and performance requirements....
opinion: placeholder
tags:
    - ML
