date: "2025-03-07"
author: Seongheon Park
title: How to Steer LLM Latents for Hallucination Detection?
thumbnail: ""
link: https://huggingface.co/papers/2503.01917
summary: The paper presents a method to improve hallucination detection in Large Language Models (LLMs) by introducing a Truthfulness Separator Vector (TSV). This steering vector reshapes the model's representation space during inference, enhancing the separation between truthful and hallucinated outputs without altering model parameters. The authors propose a two-stage framework, first training TSV on a small set of labeled exemplars and then augmenting this set with unlabeled LLM generations for improv...
opinion: placeholder
tags:
    - ML
