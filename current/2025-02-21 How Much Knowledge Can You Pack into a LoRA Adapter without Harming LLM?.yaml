date: "2025-02-21"
author: Sergey Pletenev
title: How Much Knowledge Can You Pack into a LoRA Adapter without Harming LLM?
thumbnail: ""
link: https://huggingface.co/papers/2502.14502
summary: The study investigates the integration of new facts into Large Language Models (LLMs) using LoRA without compromising previously learned knowledge. It was found that the best results are obtained when the training data contains a mixture of known and new facts, but that the model's performance on external question-answering benchmarks declines after fine-tuning, and that the model may regress to few overrepresented answers or refuse to provide an answer in some cases....
opinion: placeholder
tags:
    - ML
