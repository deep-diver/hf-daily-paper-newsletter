date: "2025-04-05"
author: Zihao Li
title: 'Rethinking Multilingual Continual Pretraining: Data Mixing for Adapting   LLMs Across Languages and Resources'
thumbnail: ""
link: https://huggingface.co/papers/2504.04152
summary: This study analyzes 36 configurations of Continual Pretraining (CPT) to improve Large Language Models' (LLMs) performance across languages. It finds that bilingual CPT improves classification but causes language mixing in generation, using programming code data enhances multilingual classification accuracy, especially for low-resource languages, and that language classifications according to their impact on cross-lingual transfer can be complex and nuanced....
opinion: placeholder
tags:
    - ML
