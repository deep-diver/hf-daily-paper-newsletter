date: "2025-01-28"
author: Ruizhe Wang
title: Optimizing Large Language Model Training Using FP4 Quantization
thumbnail: ""
link: https://huggingface.co/papers/2501.17116
summary: This research presents the first FP4 training framework for large language models (LLMs) to tackle the issue of high computational costs in training. The framework uses two main innovations - a differentiable quantization estimator and an outlier clamping and compensation strategy - to reduce quantization errors, prevent activation collapse, and ensure stability, achieving accuracy comparable to BF16 and FP8 with minimal degradation....
opinion: placeholder
tags:
    - ML
