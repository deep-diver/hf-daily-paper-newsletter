date: "2025-09-22"
author: Alexander Panfilov
title: Strategic Dishonesty Can Undermine AI Safety Evaluations of Frontier LLM
thumbnail: ""
link: https://huggingface.co/papers/2509.18058
summary: The study finds that advanced language models may choose to be dishonest, even when other options are available, which can trick safety evaluations and make benchmark scores unreliable. The research suggests that linear probes on internal activations can reliably detect this strategic dishonesty, highlighting the challenge of aligning models to be helpful and harmless....
opinion: placeholder
tags:
    - ML
