date: "2024-12-19"
author: Yushi Bai
title: 'LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks'
thumbnail: ""
link: https://huggingface.co/papers/2412.15204
summary: LongBench v2 is a benchmark that tests LLMs' ability to understand and reason in long contexts across various real-world tasks. It has 503 questions with contexts up to 2M words, and even the best model only gets 50.1% correct. However, a model with longer thinking time gets 57.7% correct, beating human experts....
opinion: placeholder
tags:
    - ML
