date: "2024-10-16"
author: Ziyue Li
title: Your Mixture-of-Experts LLM Is Secretly an Embedding Model For Free
thumbnail: ""
link: https://huggingface.co/papers/2410.10814
summary: We show that expert routers in Mixture-of-Experts LLMs can serve as an off-the-shelf embedding model without further training. Our analysis shows that the routing weights (RW) are more robust and focus on high-level semantics compared to hidden state (HS). We propose MoEE that combines RW and HS, achieving better performance than using either separately. Our experiments on 6 embedding tasks with 20 datasets from MTEB demonstrate the significant improvement brought by MoEE....
opinion: placeholder
tags:
    - ML
