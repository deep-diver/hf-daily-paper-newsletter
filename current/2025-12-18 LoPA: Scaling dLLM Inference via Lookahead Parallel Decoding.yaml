date: "2025-12-18"
author: Chenkai Xu
title: 'LoPA: Scaling dLLM Inference via Lookahead Parallel Decoding'
thumbnail: ""
link: https://huggingface.co/papers/2512.16229
summary: This study presents LoPA, a new algorithm that improves the speed of inference in large language models by optimizing the order in which tokens are filled. The method significantly increases the number of tokens processed per forward pass, enhancing efficiency and enabling faster processing of language models....
opinion: placeholder
tags:
    - ML
