date: "2024-10-18"
author: Peng Jin
title: 'MoH: Multi-Head Attention as Mixture-of-Head Attention'
thumbnail: ""
link: https://huggingface.co/papers/2410.11842
summary: The paper proposes a new architecture called Mixture-of-Head attention (MoH) to enhance the efficiency of the multi-head attention mechanism in the Transformer model. MoH allows each token to select the most suitable attention heads, improving inference efficiency without compromising accuracy or increasing the number of parameters. MoH also introduces flexibility to the attention mechanism by replacing the standard summation with a weighted summation. Experiments show that MoH outperforms multi...
opinion: placeholder
tags:
    - ML
