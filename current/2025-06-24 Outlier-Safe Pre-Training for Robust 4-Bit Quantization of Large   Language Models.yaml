date: "2025-06-24"
author: Jungwoo Park
title: Outlier-Safe Pre-Training for Robust 4-Bit Quantization of Large   Language Models
thumbnail: ""
link: https://huggingface.co/papers/2506.19697
summary: 'The study presents Outlier-Safe Pre-Training (OSP), a new method to prevent activation outliers in Large Language Models (LLMs) that hinder efficient on-device deployment. OSP uses three innovations: the Muon optimizer, Single-Scale RMSNorm, and a learnable embedding projection, to reduce outliers and improve quantization performance, as demonstrated by a 1.4B-parameter model trained on 1 trillion tokens....'
opinion: placeholder
tags:
    - ML
