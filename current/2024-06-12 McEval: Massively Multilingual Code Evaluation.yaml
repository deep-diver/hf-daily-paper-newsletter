date: "2024-06-12"
author: Linzheng Chai
title: 'McEval: Massively Multilingual Code Evaluation'
thumbnail: ""
link: https://huggingface.co/papers/2406.07436
summary: The paper proposes a massively multilingual code benchmark covering 40 programming languages with 16K test samples, which aims to evaluate the capability of different LLMs in code understanding, completion, and generation tasks in multilingual scenarios. The benchmark contains challenging code completion, understanding, and generation evaluation tasks with finely curated massively multilingual instruction corpora McEval-Instruct. The instruction corpora, evaluation benchmark, and leaderboard are...
opinion: placeholder
tags:
    - ML
