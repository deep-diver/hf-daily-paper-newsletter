date: "2025-04-08"
author: Gleb Rodionov
title: 'Hogwild! Inference: Parallel LLM Generation via Concurrent Attention'
thumbnail: ""
link: https://huggingface.co/papers/2504.06261
summary: This work introduces Hogwild! Inference, a parallel LLM inference engine that allows multiple instances of the same LLM to run in parallel with a shared Key-Value cache, improving parallel hardware utilization and avoiding recomputation using Rotary Position Embeddings. The approach enables LLMs to devise their own collaboration strategy for the task at hand while seeing each other's partial progress in a concurrent cache....
opinion: placeholder
tags:
    - ML
