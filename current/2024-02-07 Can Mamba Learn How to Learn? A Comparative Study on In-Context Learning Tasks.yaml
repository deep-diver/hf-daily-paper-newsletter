author: Jongho Park
date: '2024-02-07'
link: https://huggingface.co/papers/2402.04248
opinion: placeholder
summary: This study compares Mamba, an alternative language model using state-space
  models, with Transformer models in in-context learning tasks. Mamba performs well
  in some tasks, but a hybrid model combining Mamba with Transformer's attention blocks
  performs better overall....
tags:
- Supervised Learning
- Natural Language Processing
- Deep Learning
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/wjHOhC5IL0JB9W9bgqTPo.png
title: Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks
translated_paths:
  INT: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.04248/paper.en.html
  KR: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.04248/paper.ko.html
