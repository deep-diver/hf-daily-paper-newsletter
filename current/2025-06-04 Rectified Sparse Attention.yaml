date: "2025-06-04"
author: Yutao Sun
title: Rectified Sparse Attention
thumbnail: ""
link: https://huggingface.co/papers/2506.04108
summary: The authors present a new method called Rectified Sparse Attention (ReSA) that enhances the efficiency of long-sequence generation in Large Language Models. ReSA minimizes approximation errors and preserves generation quality by combining block-sparse attention with periodic dense rectification, resulting in faster processing times and maintaining near-perfect generation quality....
opinion: placeholder
tags:
    - ML
