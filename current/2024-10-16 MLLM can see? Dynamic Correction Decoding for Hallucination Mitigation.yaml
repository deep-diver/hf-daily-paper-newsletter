date: "2024-10-16"
author: Chenxi Wang
title: MLLM can see? Dynamic Correction Decoding for Hallucination Mitigation
thumbnail: ""
link: https://huggingface.co/papers/2410.11779
summary: This paper investigates why Multimodal Large Language Models (MLLMs) often exhibit hallucination phenomena and proposes a novel dynamic correction decoding method (DeCo) to mitigate hallucinations. DeCo adaptively selects the appropriate preceding layers and proportionally integrates knowledge into the final layer to adjust the output logits. DeCo is model agnostic and can be applied to different MLLMs. The method is evaluated on widely-used benchmarks and demonstrates a significant reduction in...
opinion: placeholder
tags:
    - ML
