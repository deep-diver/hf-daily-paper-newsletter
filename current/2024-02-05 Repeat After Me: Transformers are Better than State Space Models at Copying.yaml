date: "2024-02-05"
author: Samy Jelassi
title: 'Repeat After Me: Transformers are Better than State Space Models at Copying'
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/tIDlj9EG58UZBx74gENaf.png
link: https://huggingface.co/papers/2402.01032
summary: This paper compares transformer models and generalized state space models (GSSMs) and finds that transformer models are better at tasks that require copying information from an input context, both in theory and in practice, due to their ability to copy strings of exponential length....
opinion: placeholder
tags:
    - Supervised Learning
    - Deep Learning
    - Natural Language Processing
