author: Yushi Lan
date: '2024-03-19'
link: https://huggingface.co/papers/2403.12019
opinion: placeholder
summary: This paper presents LN3Diff, a novel framework for fast, high-quality, and
  generic conditional 3D generation using a 3D-aware architecture and variational
  autoencoder (VAE) to encode the input image into a structured, compact, and 3D latent
  space. The latent is decoded by a transformer-based decoder into a high-capacity
  3D neural field. LN3Diff outperforms existing 3D diffusion methods in terms of inference
  speed and is state-of-the-art for 3D generation on ShapeNet and other datasets....
tags:
- Unsupervised Learning
- Deep Learning
- Computer Vision
- Emerging Applications of Machine Learning
thumbnail: https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.12019.png
title: 'LN3Diff: Scalable Latent Neural Fields Diffusion for Speedy 3D Generation'
translated_paths:
  INT: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2403.12019/paper.en.html
  KR: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2403.12019/paper.ko.html
