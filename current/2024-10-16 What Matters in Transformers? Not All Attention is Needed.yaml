date: "2024-10-16"
author: Shwai He
title: What Matters in Transformers? Not All Attention is Needed
thumbnail: ""
link: https://huggingface.co/papers/2406.15786
summary: Researchers examine redundancy in Transformer-based large language models (LLMs) and find that a significant portion of attention layers can be pruned without negatively impacting performance. They propose a method to drop both attention and MLP layers, achieving speedups and maintaining most of the model's performance. The code for their research is available at <https://github.com/Shwai-He/LLM-Drop>....
opinion: placeholder
tags:
    - ML
