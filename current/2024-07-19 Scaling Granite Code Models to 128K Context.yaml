date: "2024-07-19"
author: Matt Stallone
title: Scaling Granite Code Models to 128K Context
thumbnail: ""
link: https://huggingface.co/papers/2407.13739
summary: This paper presents long-context Granite code models that can handle context windows up to 128K tokens. The models are scaled up by gradually increasing their RoPE base frequency with repository-level file packing and length-upsampled long-context data. Additionally, instruction-tuned models with long-context support are derived by further finetuning the long context base models on a mix of short and long-context instruction-response pairs. The long-context models perform significantly better on...
opinion: placeholder
tags:
    - ML
