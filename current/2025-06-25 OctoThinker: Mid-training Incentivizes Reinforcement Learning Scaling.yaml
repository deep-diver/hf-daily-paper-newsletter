date: "2025-06-25"
author: Zengzhi Wang
title: 'OctoThinker: Mid-training Incentivizes Reinforcement Learning Scaling'
thumbnail: ""
link: https://huggingface.co/papers/2506.20512
summary: This study explores how mid-training strategies affect reinforcement learning in two language model families, Qwen and Llama. The researchers found that using high-quality math corpora and QA-style data significantly improves performance, while a new two-stage mid-training strategy, Stable-then-Decay, leads to stronger RL outcomes and reduces the performance gap between model families. They also released their open-source models and a math reasoning-intensive corpus for further research....
opinion: placeholder
tags:
    - ML
