date: "2025-05-12"
author: Seongjae Kang
title: Simple Semi-supervised Knowledge Distillation from Vision-Language   Models via texttt{D}ual-texttt{H}ead   texttt{O}ptimization
thumbnail: ""
link: https://huggingface.co/papers/2505.07675
summary: The authors present a new method called Dual-Head Optimization (DHO) that simplifies the process of transferring knowledge from vision-language models to smaller, task-specific models in semi-supervised settings. DHO uses two prediction heads that learn independently and combine their outputs during inference, reducing gradient conflicts and improving feature learning, resulting in better performance than existing methods, especially on the ImageNet dataset....
opinion: placeholder
tags:
    - ML
