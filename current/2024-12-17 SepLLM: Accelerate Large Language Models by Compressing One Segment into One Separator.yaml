date: "2024-12-17"
author: Guoxuan Chen
title: 'SepLLM: Accelerate Large Language Models by Compressing One Segment into One Separator'
thumbnail: ""
link: https://huggingface.co/papers/2412.12094
summary: SepLLM is a framework that speeds up language model inference by compressing certain segments and eliminating redundant tokens, resulting in a 50% reduction in KV cache while maintaining performance. It also processes long sequences in streaming settings while maintaining language modeling capabilities....
opinion: placeholder
tags:
    - ML
