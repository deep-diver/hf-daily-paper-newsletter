date: "2024-06-03"
author: Tri Dao
title: 'Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality'
thumbnail: https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.21060.png
link: https://huggingface.co/papers/2405.21060
summary: While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matric...
opinion: placeholder
tags:
    - Deep Learning
    - Natural Language Processing
