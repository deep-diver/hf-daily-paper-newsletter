date: "2024-04-12"
author: Aleksandar Botev
title: 'RecurrentGemma: Moving Past Transformers for Efficient Open Language Models'
thumbnail: https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.07839.png
link: https://huggingface.co/papers/2404.07839
summary: This paper presents RecurrentGemma, an open language model based on Google's Griffin architecture. Griffin combines linear recurrences and local attention for efficient and effective language processing. The model has a fixed-sized state, reducing memory use and enabling efficient inference on long sequences. Pre-trained and instruction-tuned variants of RecurrentGemma are provided, achieving comparable performance to Gemma-2B with fewer training tokens....
opinion: placeholder
tags:
    - Natural Language Processing
    - Deep Learning
    - Unsupervised Learning
