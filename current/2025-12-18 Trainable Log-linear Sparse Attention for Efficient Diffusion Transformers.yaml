date: "2025-12-18"
author: Yifan Zhou
title: Trainable Log-linear Sparse Attention for Efficient Diffusion Transformers
thumbnail: ""
link: https://huggingface.co/papers/2512.16615
summary: The study presents a new attention mechanism called Log-linear Sparse Attention (LLSA) that significantly improves the efficiency of visual generation models like Diffusion Transformers. LLSA reduces the computational cost from quadratic to log-linear by using a hierarchical structure, allowing for faster attention inference and training while maintaining image generation quality....
opinion: placeholder
tags:
    - ML
