date: "2025-10-15"
author: Yang Li
title: 'Attention Illuminates LLM Reasoning: The Preplan-and-Anchor Rhythm   Enables Fine-Grained Policy Optimization'
thumbnail: ""
link: https://huggingface.co/papers/2510.13554
summary: This study reveals that attention can be used to understand the reasoning process of large language models (LLMs) by distinguishing between locally and globally focused attention heads. The researchers introduce new methods for optimizing LLM reasoning by targeting critical nodes in the attention process, leading to improved performance on various reasoning tasks....
opinion: placeholder
tags:
    - ML
