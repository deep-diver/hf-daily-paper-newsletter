date: "2025-06-05"
author: Noy Sternlicht
title: 'Debatable Intelligence: Benchmarking LLM Judges via Debate Speech   Evaluation'
thumbnail: ""
link: https://huggingface.co/papers/2506.05062
summary: The researchers created a new test called Debate Speech Evaluation to measure the performance of language models in understanding debate speeches. They used a dataset of 600 annotated debate speeches and found that while large models can mimic some aspects of human judgment, they differ significantly in their overall judgment behavior. Additionally, they discovered that advanced language models can generate persuasive speeches at a human level....
opinion: placeholder
tags:
    - ML
