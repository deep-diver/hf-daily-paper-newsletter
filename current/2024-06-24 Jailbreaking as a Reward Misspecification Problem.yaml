date: "2024-06-24"
author: Zhihui Xie
title: Jailbreaking as a Reward Misspecification Problem
thumbnail: ""
link: https://huggingface.co/papers/2406.14393
summary: This paper proposes a new way of looking at the problem of large language models being vulnerable to attacks. It introduces a way to measure how much these models are misaligned with their goals, and uses this to create a system for finding and exploiting weaknesses in these models. This system is better than others at finding these weaknesses while still making sense to humans....
opinion: placeholder
tags:
    - ML
