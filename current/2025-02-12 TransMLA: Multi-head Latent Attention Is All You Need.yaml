date: "2025-02-12"
author: Fanxu Meng
title: 'TransMLA: Multi-head Latent Attention Is All You Need'
thumbnail: ""
link: https://huggingface.co/papers/2502.07864
summary: The paper introduces TransMLA, a method that converts popular GQA-based pre-trained models into MLA-based models, allowing for additional training to enhance expressiveness without increasing KV cache size. The authors also plan to develop MLA-specific inference acceleration techniques to maintain low latency in transformed models....
opinion: placeholder
tags:
    - ML
