author: Jiacheng Liu
date: '2024-02-01'
link: https://huggingface.co/papers/2401.17377
opinion: placeholder
summary: This paper presents Infini-gram, a trillion-token n-gram language model that
  allows for arbitrarily large n values and utilizes a new backoff method. The Infini-gram
  engine, powered by suffix arrays, can compute infty-gram and n-gram probabilities
  with low latency. The model is used to analyze human-written and machine-generated
  text and complement neural language models to reduce their language modeling perplexities....
tags:
- Natural Language Processing
- Deep Learning
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/REHvHBoxCtCfDveoGyPbH.png
title: 'Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion Tokens'
translated_paths:
  INT: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2401.17377/paper.en.html
  KR: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2401.17377/paper.ko.html
