date: "2024-07-22"
author: Qichen Fu
title: 'LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference'
thumbnail: ""
link: https://huggingface.co/papers/2407.14057
summary: The LazyLLM method improves the efficiency of long context LLM inference by dynamically selecting important tokens for the next prediction, which can significantly accelerate the generation process without compromising accuracy....
opinion: placeholder
tags:
    - ML
