date: "2024-10-25"
author: Zesen Cheng
title: 'Breaking the Memory Barrier: Near Infinite Batch Size Scaling for Contrastive Loss'
thumbnail: ""
link: https://huggingface.co/papers/2410.17243
summary: A new method for contrastive learning is proposed to increase batch size without increasing memory usage. This is achieved by breaking down the contrastive loss calculation into smaller blocks and using a multi-level tiling strategy. The method is able to train a CLIP-ViT-L/14 model with a batch size of 4M or 12M using 8 or 32 A800 80GB, respectively, without sacrificing accuracy. It also reduces memory usage by two orders of magnitude compared to existing solutions while maintaining similar spe...
opinion: placeholder
tags:
    - ML
