date: "2024-10-25"
author: Chenxin An
title: Why Does the Effective Context Length of LLMs Fall Short?
thumbnail: ""
link: https://huggingface.co/papers/2410.18745
summary: The paper identifies a limitation in large language models (LLMs) where their effective context lengths fall short, typically not exceeding half of their training lengths. This is attributed to the left-skewed frequency distribution of relative positions formed in LLMs pretraining and post-training stages. The paper introduces ShifTed Rotray position embeddING (STRING) to address this challenge by shifting well-trained positions to overwrite the original ineffective positions during inference. S...
opinion: placeholder
tags:
    - ML
