date: "2024-11-08"
author: Weixin Liang
title: 'Mixture-of-Transformers: A Sparse and Scalable Architecture for Multi-Modal Foundation Models'
thumbnail: ""
link: https://huggingface.co/papers/2411.04996
summary: This paper presents Mixture-of-Transformers (MoT), a sparse multi-modal transformer architecture that reduces the computational costs of training large language models. MoT decouples non-embedding parameters of the model by modality, enabling modality-specific processing with global self-attention over the full input sequence. The paper evaluates MoT across multiple settings and model scales, and shows that it matches or outperforms dense baselines with significantly fewer FLOPs and wall-clock t...
opinion: placeholder
tags:
    - ML
