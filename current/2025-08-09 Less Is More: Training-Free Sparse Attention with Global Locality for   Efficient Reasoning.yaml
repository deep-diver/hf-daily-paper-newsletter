date: "2025-08-09"
author: Lijie Yang
title: 'Less Is More: Training-Free Sparse Attention with Global Locality for   Efficient Reasoning'
thumbnail: ""
link: https://huggingface.co/papers/2508.07101
summary: This study presents a new method called LessIsMore, which reduces the computational overhead of large reasoning models without compromising accuracy. LessIsMore is a training-free sparse attention mechanism that improves generalization and efficiency by aggregating token selections from local attention heads and enabling unified cross-head token ranking, resulting in faster processing and attending to fewer tokens compared to existing methods....
opinion: placeholder
tags:
    - ML
