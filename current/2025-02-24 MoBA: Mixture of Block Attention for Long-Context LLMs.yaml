date: "2025-02-24"
author: Enzhe Lu
title: 'MoBA: Mixture of Block Attention for Long-Context LLMs'
thumbnail: ""
link: https://huggingface.co/papers/2502.13189
summary: The authors propose MoBA (Mixture of Block Attention), a new approach that applies Mixture of Experts (MoE) to the attention mechanism for better performance in long-context tasks in large language models (LLMs) without compromising efficiency. It has been deployed to support Kimi's long-context requests and can be found on https://github.com/MoonshotAI/MoBA....
opinion: placeholder
tags:
    - ML
