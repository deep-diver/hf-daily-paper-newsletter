date: "2024-06-25"
author: Chao Lou
title: 'Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers'
thumbnail: ""
link: https://huggingface.co/papers/2406.16747
summary: SPARSEK Attention, a new sparse attention method, is introduced to handle long sequences efficiently in autoregressive Transformers by selecting a constant number of KV pairs for each query. It offers linear time complexity and constant memory footprint during generation, outperforming previous sparse attention methods and providing speed improvements in language modeling and downstream tasks. It can be easily integrated into pre-trained Large Language Models (LLMs) with minimal fine-tuning....
opinion: placeholder
tags:
    - ML
