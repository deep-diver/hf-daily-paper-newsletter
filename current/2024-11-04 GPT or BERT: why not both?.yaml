date: "2024-11-04"
author: Lucas Georges Gabriel Charpentier
title: 'GPT or BERT: why not both?'
thumbnail: ""
link: https://huggingface.co/papers/2410.24159
summary: We combine masked language modeling and causal language modeling into a single transformer stack called GPT-BERT. It can be used like a standard causal or masked language model and outperforms models that only use one type of language modeling. We share the models, training data, and code for others to use....
opinion: placeholder
tags:
    - ML
