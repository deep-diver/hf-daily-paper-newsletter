date: "2025-11-28"
author: Xiang Hu
title: 'Every Token Counts: Generalizing 16M Ultra-Long Context in Large Language Models'
thumbnail: ""
link: https://huggingface.co/papers/2511.23319
summary: The study presents a new attention mechanism called Hierarchical Sparse Attention (HSA) that efficiently handles ultra-long contexts in large language models, generalizing to 16M contexts with high accuracy. The researchers integrate HSA into Transformers, creating an 8B-parameter model called HSA-UltraLong, which performs well on various tasks and out-of-domain contexts, paving the way for future research in this area....
opinion: placeholder
tags:
    - ML
