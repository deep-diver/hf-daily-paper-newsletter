author: Sohee Yang
date: '2024-02-27'
link: https://huggingface.co/papers/2402.16837
opinion: placeholder
summary: This paper investigates whether large language models (LLMs) can perform
  multi-hop reasoning by analyzing their responses to complex prompts. The study finds
  evidence of latent multi-hop reasoning for certain relation types, but the utilization
  is contextual and varies across different types of prompts. The evidence for the
  second hop and full multi-hop traversal is moderate, and there is a clear scaling
  trend for the first hop but not for the second hop....
tags:
- Natural Language Processing
- Deep Learning
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/Az5DrQf0hQpNDciaF5taj.png
title: Do Large Language Models Latently Perform Multi-Hop Reasoning?
translated_paths:
  INT: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.16837/paper.en.html
  KR: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.16837/paper.ko.html
