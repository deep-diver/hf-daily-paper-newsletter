date: "2024-02-27"
author: Sohee Yang
title: Do Large Language Models Latently Perform Multi-Hop Reasoning?
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/Az5DrQf0hQpNDciaF5taj.png
link: https://huggingface.co/papers/2402.16837
summary: This paper investigates whether large language models (LLMs) can perform multi-hop reasoning by analyzing their responses to complex prompts. The study finds evidence of latent multi-hop reasoning for certain relation types, but the utilization is contextual and varies across different types of prompts. The evidence for the second hop and full multi-hop traversal is moderate, and there is a clear scaling trend for the first hop but not for the second hop....
opinion: placeholder
tags:
    - Natural Language Processing
    - Deep Learning
