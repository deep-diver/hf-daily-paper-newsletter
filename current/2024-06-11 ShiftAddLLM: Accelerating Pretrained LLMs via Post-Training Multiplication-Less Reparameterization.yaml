date: "2024-06-11"
author: Haoran You
title: 'ShiftAddLLM: Accelerating Pretrained LLMs via Post-Training Multiplication-Less Reparameterization'
thumbnail: https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.05981.png
link: https://huggingface.co/papers/2406.05981
summary: ShiftAddLLM is a method that improves the efficiency of large language models by replacing costly multiplications with simpler operations, reducing memory usage and latency while maintaining or improving accuracy. It achieves better results than competitive quantized LLMs in terms of memory and energy usage, with an average improvement of 5.6 and 22.7 points in perplexity at 3 and 2 bits, respectively. The code and models are available at <https://github.com/GATECH-EIC/ShiftAddLLM>....
opinion: placeholder
tags:
    - ML
