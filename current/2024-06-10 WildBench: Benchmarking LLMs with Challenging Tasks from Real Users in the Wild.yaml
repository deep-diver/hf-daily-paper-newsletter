date: "2024-06-10"
author: Bill Yuchen Lin
title: 'WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild'
thumbnail: https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.04770.png
link: https://huggingface.co/papers/2406.04770
summary: We introduce WildBench, an automated evaluation framework designed to benchmark large language models (LLMs) using challenging, real-world user queries. WildBench consists of 1,024 tasks carefully selected from over one million human-chatbot conversation logs. For automated evaluation with WildBench, we have developed two metrics, WB-Reward and WB-Score, which are computable using advanced LLMs such as GPT-4-turbo. WildBench evaluation uses task-specific checklists to evaluate model outputs syst...
opinion: placeholder
tags:
    - ML
