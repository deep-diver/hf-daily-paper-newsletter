date: "2024-04-16"
author: Xuezhe Ma
title: 'Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length'
thumbnail: https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.08801.png
link: https://huggingface.co/papers/2404.08801
summary: The paper introduces Megalodon, a new neural architecture for efficient sequence modeling with unlimited context length. Megalodon improves upon previous methods and achieves better efficiency than Transformer in a controlled head-to-head comparison with Llama2....
opinion: placeholder
tags:
    - Deep Learning
    - Natural Language Processing
