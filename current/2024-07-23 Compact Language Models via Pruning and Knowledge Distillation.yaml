date: "2024-07-23"
author: Saurav Muralidharan
title: Compact Language Models via Pruning and Knowledge Distillation
thumbnail: ""
link: https://huggingface.co/papers/2407.14679
summary: This paper explores a more efficient way to create smaller versions of large language models by pruning an existing model and then retraining it with less data. They found the best way to do this by experimenting with different techniques and ended up with models that perform just as well as bigger models trained from scratch but use less computing power....
opinion: placeholder
tags:
    - ML
