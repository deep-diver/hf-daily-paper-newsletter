date: "2024-01-30"
author: Pratyush Maini
title: 'Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling'
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/WApmi8a7YcGCsMoVlFfXu.png
link: https://huggingface.co/papers/2401.16380
summary: Large language models are trained on massive scrapes of the web, which are often unstructured, noisy, and poorly phrased. Current scaling laws show that learning from such data requires an abundance of both compute and data, which grows with the size of the model being trained. This is infeasible both because of the large compute costs and duration associated with pre-training, and the impending scarcity of high-quality data on the web. In this work, we propose Web Rephrase Augmented Pre-trainin...
opinion: placeholder
tags:
    - Natural Language Processing
