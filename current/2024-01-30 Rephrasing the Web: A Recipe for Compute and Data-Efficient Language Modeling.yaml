author: Pratyush Maini
date: '2024-01-30'
link: https://huggingface.co/papers/2401.16380
opinion: placeholder
summary: Large language models are trained on massive scrapes of the web, which are
  often unstructured, noisy, and poorly phrased. Current scaling laws show that learning
  from such data requires an abundance of both compute and data, which grows with
  the size of the model being trained. This is infeasible both because of the large
  compute costs and duration associated with pre-training, and the impending scarcity
  of high-quality data on the web. In this work, we propose Web Rephrase Augmented
  Pre-trainin...
tags:
- Natural Language Processing
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/WApmi8a7YcGCsMoVlFfXu.png
title: 'Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling'
translated_paths:
  INT: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2401.16380/paper.en.html
  KR: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2401.16380/paper.ko.html
