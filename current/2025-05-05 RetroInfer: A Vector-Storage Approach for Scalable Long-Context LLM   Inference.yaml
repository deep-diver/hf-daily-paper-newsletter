date: "2025-05-05"
author: Yaoqi Chen
title: 'RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM   Inference'
thumbnail: ""
link: https://huggingface.co/papers/2505.02922
summary: The authors propose RetroInfer, a new system that speeds up long-context language model inference by using the KV cache as a vector storage system and exploiting attention sparsity. This approach, which includes the wave index and wave buffer, offers significant speedups over previous methods without sacrificing accuracy....
opinion: placeholder
tags:
    - ML
