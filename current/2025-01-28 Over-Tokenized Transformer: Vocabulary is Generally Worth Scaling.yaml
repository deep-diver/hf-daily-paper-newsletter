date: "2025-01-28"
author: Hongzhi Huang
title: 'Over-Tokenized Transformer: Vocabulary is Generally Worth Scaling'
thumbnail: ""
link: https://huggingface.co/papers/2501.16975
summary: The paper presents Over-Tokenized Transformers, a new framework that separates input and output vocabularies to improve language modeling performance. By scaling up the input vocabulary to use multi-gram tokens, it finds a log-linear relationship between vocabulary size and training loss, showing that larger vocabularies consistently improve model performance, even with no additional cost....
opinion: placeholder
tags:
    - ML
