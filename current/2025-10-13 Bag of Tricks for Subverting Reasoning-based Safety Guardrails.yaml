date: "2025-10-13"
author: Shuo Chen
title: Bag of Tricks for Subverting Reasoning-based Safety Guardrails
thumbnail: ""
link: https://huggingface.co/papers/2510.11570
summary: The study reveals that reasoning-based safety measures for large language models can be easily bypassed through subtle input manipulations, leading to harmful responses. The researchers present various jailbreak methods that exploit these vulnerabilities, achieving high attack success rates across different models, highlighting the need for improved safety measures in open-source language models....
opinion: placeholder
tags:
    - ML
