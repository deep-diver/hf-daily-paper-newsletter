date: "2024-02-28"
author: Biao Zhang
title: 'When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method'
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/OHI5lrgnO3IsfEp2p9AJm.png
link: https://huggingface.co/papers/2402.17193
summary: This paper studies the effect of different scaling factors, including LLM model size, pretraining data size, finetuning parameter size, and finetuning data size, on the performance of LLM finetuning. The study finds that LLM finetuning follows a power-based multiplicative joint scaling law and that PET parameter scaling is generally ineffective. The optimal finetuning method is highly task- and finetuning data-dependent....
opinion: placeholder
tags:
    - Supervised Learning
    - Deep Learning
    - Optimization and Learning Algorithms
    - Natural Language Processing
