author: Biao Zhang
date: '2024-02-28'
link: https://huggingface.co/papers/2402.17193
opinion: placeholder
summary: This paper studies the effect of different scaling factors, including LLM
  model size, pretraining data size, finetuning parameter size, and finetuning data
  size, on the performance of LLM finetuning. The study finds that LLM finetuning
  follows a power-based multiplicative joint scaling law and that PET parameter scaling
  is generally ineffective. The optimal finetuning method is highly task- and finetuning
  data-dependent....
tags:
- Supervised Learning
- Deep Learning
- Optimization and Learning Algorithms
- Natural Language Processing
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/OHI5lrgnO3IsfEp2p9AJm.png
title: 'When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning
  Method'
translated_paths:
  INT: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.17193/paper.en.html
  KR: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.17193/paper.ko.html
