date: "2024-10-22"
author: Yantao Liu
title: 'RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style'
thumbnail: ""
link: https://huggingface.co/papers/2410.16184
summary: This paper introduces RM-Bench, a new benchmark for evaluating reward models used in language model alignment and response selection. RM-Bench focuses on subtle content differences and style biases, which are often overlooked in existing benchmarks. The authors find that current reward models have room for improvement, as they perform poorly on RM-Bench, especially when dealing with style biases....
opinion: placeholder
tags:
    - ML
