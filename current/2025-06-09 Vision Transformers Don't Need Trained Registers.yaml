date: "2025-06-09"
author: Nick Jiang
title: Vision Transformers Don't Need Trained Registers
thumbnail: ""
link: https://huggingface.co/papers/2506.08010
summary: The study explores why certain high-norm tokens in Vision Transformers lead to noisy attention maps, and proposes a new, training-free method to mitigate these artifacts. By shifting high-norm activations from specific neurons to an additional untrained token, the method improves attention and feature maps, enhances performance, and achieves results comparable to models with register tokens, without the need for retraining....
opinion: placeholder
tags:
    - ML
