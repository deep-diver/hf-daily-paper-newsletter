date: "2024-03-27"
author: Andrey Gromov
title: The Unreasonable Ineffectiveness of the Deeper Layers
thumbnail: https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.17887.png
link: https://huggingface.co/papers/2403.17887
summary: This paper explores a simple layer-pruning strategy for popular open-weight pretrained LLMs and finds that removing up to half of the layers has minimal impact on performance. The results suggest that layer pruning methods can be used to reduce computational resources and improve memory and latency during inference, and may imply that current pretraining methods are not effectively utilizing deeper layers or that shallow layers play a critical role in storing knowledge....
opinion: placeholder
tags:
    - Supervised Learning
    - Deep Learning
