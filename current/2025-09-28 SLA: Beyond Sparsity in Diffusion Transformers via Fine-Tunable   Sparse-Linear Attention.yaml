date: "2025-09-28"
author: Jintao Zhang
title: 'SLA: Beyond Sparsity in Diffusion Transformers via Fine-Tunable   Sparse-Linear Attention'
thumbnail: ""
link: https://huggingface.co/papers/2509.24006
summary: Researchers developed SLA, a new attention method for diffusion transformers, which significantly speeds up video generation by reducing attention computation by 95% without sacrificing quality. They achieved this by categorizing attention weights and applying different attention mechanisms to each category, resulting in a 13.7x speedup in attention computation and a 2.2x overall speedup in video generation....
opinion: placeholder
tags:
    - ML
