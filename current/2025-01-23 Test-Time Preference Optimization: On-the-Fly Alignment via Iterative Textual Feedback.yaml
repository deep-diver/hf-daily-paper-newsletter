date: "2025-01-23"
author: Yafu Li
title: 'Test-Time Preference Optimization: On-the-Fly Alignment via Iterative Textual Feedback'
thumbnail: ""
link: https://huggingface.co/papers/2501.12895
summary: The work presents 'Test-time Preference Optimization' or TPO, a method that allows language models to adapt to human preferences during inference without retraining. TPO translates reward signals into textual feedback and uses it to iteratively refine its responses, improving alignment with human preferences and even outperforming a model trained with human preferences after just a few steps....
opinion: placeholder
tags:
    - ML
