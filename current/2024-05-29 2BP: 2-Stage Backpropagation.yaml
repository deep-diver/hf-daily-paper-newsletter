date: "2024-05-29"
author: Christopher Rae
title: '2BP: 2-Stage Backpropagation'
thumbnail: https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.18047.png
link: https://huggingface.co/papers/2405.18047
summary: This paper proposes 2-stage backpropagation (2BP) to reduce idle compute time and increase throughput for training large Deep Neural Networks (DNNs) using pipeline parallelism. 2BP was tested on various model architectures and pipelining schedules, achieving increases in throughput in all cases, including a 1.70x increase in throughput when training a LLaMa-like transformer with 7 billion parameters across 4 GPUs....
opinion: placeholder
tags:
    - Supervised Learning
    - Deep Learning
    - Optimization and Learning Algorithms
