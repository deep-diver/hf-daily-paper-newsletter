date: "2024-06-27"
author: Yushun Zhang
title: 'Adam-mini: Use Fewer Learning Rates To Gain More'
thumbnail: ""
link: https://huggingface.co/papers/2406.16793
summary: Adam-mini is an optimizer that performs as well as or better than AdamW with less memory usage. It reduces memory by using fewer learning rates and assigns a single learning rate to each group of parameters. Adam-mini is tested and performs well on various language models, and it also reduces the time it takes to pre-train these models by reducing communication overheads among GPUs and CPUs....
opinion: placeholder
tags:
    - ML
