date: "2025-12-24"
author: Lichao Wu
title: 'GateBreaker: Gate-Guided Attacks on Mixture-of-Expert LLMs'
thumbnail: https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21008.png
link: https://huggingface.co/papers/2512.21008
summary: New, efficient AI models (MoE LLMs) are increasingly used, but their unique safety mechanisms haven't been thoroughly checked like older AI models. A method called GateBreaker can find and disable tiny 'safety switch' parts in these models, making them much more likely to produce harmful content by turning off only a small number of specific neurons....
opinion: placeholder
tags:
    - Mixture-of-Experts
    - Adversarial Attacks
poster_path: https://raw.githubusercontent.com/deep-diver/hf-daily-paper-newsletter/main/posters/poster_2512.21008_landscape_english_nopanel.png
