date: "2025-06-13"
author: Xiaoran Liu
title: 'Beyond Homogeneous Attention: Memory-Efficient LLMs via   Fourier-Approximated KV Cache'
thumbnail: ""
link: https://huggingface.co/papers/2506.11886
summary: This study presents FourierAttention, a method that optimizes memory usage in Large Language Models by utilizing the varying roles of transformer head dimensions. By projecting certain dimensions onto orthogonal Fourier bases, it approximates their temporal evolution with fixed-length spectral coefficients, improving long-context accuracy without sacrificing performance....
opinion: placeholder
tags:
    - ML
