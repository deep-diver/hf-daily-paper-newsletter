date: "2025-01-14"
author: Tianjin Huang
title: 'SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training'
thumbnail: ""
link: https://huggingface.co/papers/2501.06842
summary: This paper presents SPAM, a novel optimizer designed to counteract gradient spikes during Large Language Model training, which can be up to 1000 times larger than typical gradients and deteriorate model performance. SPAM consistently outperforms Adam and its variants across various tasks, and facilitates memory-efficient training by enabling sparse momentum. Code is available at the provided link....
opinion: placeholder
tags:
    - ML
