date: "2025-03-06"
author: Lin Sun
title: 'TinyR1-32B-Preview: Boosting Accuracy with Branch-Merge Distillation'
thumbnail: ""
link: https://huggingface.co/papers/2503.04872
summary: The Branch-Merge distillation approach is proposed to create smaller, high-performing LLMs by selectively distilling knowledge from a large teacher model into specialized student models in the Branch Phase and merging them in the Merge Phase for cross-domain knowledge transfer, improving generalization. This approach outperforms previous methods in multiple benchmarks, such as Mathematics, Coding, and Science, while achieving near-equal performance to the original model on the AIME 2024 benchmar...
opinion: placeholder
tags:
    - ML
