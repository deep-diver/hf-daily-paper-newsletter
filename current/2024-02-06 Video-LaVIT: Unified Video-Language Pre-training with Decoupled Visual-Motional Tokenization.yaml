author: Yang Jin
date: '2024-02-06'
link: https://huggingface.co/papers/2402.03161
opinion: placeholder
summary: This paper introduces a video-language pre-training method called Video-LaVIT,
  which breaks down videos into keyframes and motions, and then uses a large language
  model to pre-train on this data. The goal is to enable computers to understand and
  generate both images and videos, and the method is shown to perform well on various
  benchmarks for image and video understanding and generation. ...
tags:
- Deep Learning
- Natural Language Processing
- Computer Vision
- Video Understanding and Generation
thumbnail: https://github.com/deep-diver/hf-daily-paper-newsletter/blob/main/assets/2402.03161.gif?raw=true
title: 'Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional
  Tokenization'
