date: "2024-09-17"
author: Wei Shen
title: Policy Filtration in RLHF to Fine-Tune LLM for Code Generation
thumbnail: ""
link: https://huggingface.co/papers/2409.06957
summary: Reinforcement learning from human feedback (RLHF) is one of the key techniques that helps large language models (LLMs) to follow instructions and provide helpful and harmless responses. While direct policy optimization methods exist, state-of-the-art LLMs adopt RL-based methods (usually PPO) in RLHF to train the policy to generate good responses guided by a reward model learned from preference data. The main challenge of these methods is the inaccuracy of the intermediate reward model, especiall...
opinion: placeholder
tags:
    - ML
