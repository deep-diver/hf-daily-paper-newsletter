date: "2024-12-05"
author: Jungwoo Park
title: 'Monet: Mixture of Monosemantic Experts for Transformers'
thumbnail: ""
link: https://huggingface.co/papers/2412.04139
summary: Understanding the internal computations of large language models (LLMs) is crucial for aligning them with human values and preventing undesirable behaviors like toxic content generation. However, mechanistic interpretability is hindered by polysemanticity -- where individual neurons respond to multiple, unrelated concepts. While Sparse Autoencoders (SAEs) have attempted to disentangle these features through sparse dictionary learning, they have compromised LLM performance due to reliance on post...
opinion: placeholder
tags:
    - ML
