date: "2025-05-21"
author: Zhexin Zhang
title: 'How Should We Enhance the Safety of Large Reasoning Models: An Empirical   Study'
thumbnail: ""
link: https://huggingface.co/papers/2505.15404
summary: This study investigates how to improve the safety of Large Reasoning Models (LRMs) through Supervised Fine-Tuning. The researchers found that addressing key failure patterns during data distillation and using short or template-based reasoning processes can significantly enhance safety, without the need for complex reasoning chains. They also discovered that mixing math reasoning data during safety fine-tuning helps balance safety and over-refusal....
opinion: placeholder
tags:
    - ML
