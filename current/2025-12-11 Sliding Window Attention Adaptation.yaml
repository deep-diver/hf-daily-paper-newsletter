date: "2025-12-11"
author: Yijiong Yu
title: Sliding Window Attention Adaptation
thumbnail: ""
link: https://huggingface.co/papers/2512.10411
summary: This study presents Sliding Window Attention Adaptation (SWAA), a collection of methods to improve the performance of Transformer-based Large Language Models (LLMs) during long-context inference. The proposed techniques, such as applying SWA only during prefilling and interleaving FA/SWA layers, help recover the original long-context performance while balancing performance-efficiency trade-offs for various scenarios....
opinion: placeholder
tags:
    - ML
