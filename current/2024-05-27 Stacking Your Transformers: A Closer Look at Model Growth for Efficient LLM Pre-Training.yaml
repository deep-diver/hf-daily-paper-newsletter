date: "2024-05-27"
author: Wenyu Du
title: 'Stacking Your Transformers: A Closer Look at Model Growth for Efficient LLM Pre-Training'
thumbnail: https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.15319.png
link: https://huggingface.co/papers/2405.15319
summary: This paper introduces a method called G_stack that uses smaller models to train larger language models more efficiently. The method is tested on models up to 7 billion parameters and is found to be scalable and effective. The paper also provides guidelines for using G_stack in general language model pre-training....
opinion: placeholder
tags:
    - Deep Learning
    - Natural Language Processing
    - Optimization and Learning Algorithms
