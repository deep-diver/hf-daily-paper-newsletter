date: "2024-02-06"
author: Dejiao Zhang
title: Code Representation Learning At Scale
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/UTrwanKFD7-DrbqYNWT5S.png
link: https://huggingface.co/papers/2402.01935
summary: This paper introduces a two-stage pretraining scheme that enhances the performance of code representation learning on downstream tasks. The model uses a mix of randomness and structure aspects of programming language to train encoders, and then uses contrastive learning with hard negatives and positives. The findings suggest that a customized token-level denoising scheme, hard negatives and positives, and bimodal contrastive learning contribute to the success of the model....
opinion: placeholder
tags:
    - Deep Learning
    - Natural Language Processing
    - Optimization and Learning Algorithms
