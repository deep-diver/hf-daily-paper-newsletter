date: "2025-10-08"
author: Jusen Du
title: Native Hybrid Attention for Efficient Sequence Modeling
thumbnail: ""
link: https://huggingface.co/papers/2510.07019
summary: The authors present a new model called Native Hybrid Attention (NHA) that combines the efficiency of linear attention with the context recall of full attention in a unified design. NHA outperforms Transformers and other hybrid models in recall-intensive tasks and can also be applied to pretrained language models for improved efficiency without sacrificing accuracy....
opinion: placeholder
tags:
    - ML
