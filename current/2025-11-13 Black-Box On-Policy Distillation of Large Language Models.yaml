date: "2025-11-13"
author: Tianzhu Ye
title: Black-Box On-Policy Distillation of Large Language Models
thumbnail: ""
link: https://huggingface.co/papers/2511.10643
summary: This research presents a new method called Generative Adversarial Distillation (GAD) that trains a student large language model (LLM) by making it compete in a game against a discriminator model, which is trained to distinguish the student's responses from those of a proprietary teacher model. Experiments show that GAD outperforms traditional distillation methods and results in a student model that is nearly as good as the teacher model....
opinion: placeholder
tags:
    - ML
