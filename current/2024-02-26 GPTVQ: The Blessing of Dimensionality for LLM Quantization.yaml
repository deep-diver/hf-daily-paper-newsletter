author: Mart van Baalen
date: '2024-02-26'
link: https://huggingface.co/papers/2402.15319
opinion: placeholder
summary: A new method called GPTVQ is introduced, which increases the dimensionality
  of neural network quantization to improve the size versus accuracy trade-off. GPTVQ
  is a fast method for post-training vector quantization that works well with Large
  Language Models, setting a new state-of-the-art in the size vs accuracy trade-offs
  while being efficient and improving latency compared to using a 4-bit integer format....
tags:
- Supervised Learning
- Deep Learning
- Natural Language Processing
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/TD0kC_hrvKDrbzKZlz1RH.png
title: 'GPTVQ: The Blessing of Dimensionality for LLM Quantization'
translated_paths:
  INT: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.15319/paper.en.html
  KR: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.15319/paper.ko.html
