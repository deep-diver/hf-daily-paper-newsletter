date: "2024-11-05"
author: Xingwu Sun
title: 'Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent'
thumbnail: ""
link: https://huggingface.co/papers/2411.02265
summary: Hunyuan-Large is the largest open-source Transformer-based mixture of experts model with 52 billion activation parameters. It outperforms LLama3.1-70B and has comparable performance to LLama3.1-405B. It uses large-scale synthetic data, mixed expert routing strategy, key-value cache compression technique, and expert-specific learning rate strategy. The code and checkpoints are released to facilitate future innovations and applications....
opinion: placeholder
tags:
    - ML
