date: "2025-12-10"
author: Tiwei Bie
title: 'LLaDA2.0: Scaling Up Diffusion Language Models to 100B'
thumbnail: ""
link: https://huggingface.co/papers/2512.15745
summary: The authors propose LLaDA2.0, a new method to create large language models up to 100 billion parameters by converting pre-trained models into a more efficient format. This approach, which includes a three-phase training process, results in two new models, LLaDA2.0-mini and LLaDA2.0-flash, that offer improved performance and efficiency compared to existing models....
opinion: placeholder
tags:
    - ML
