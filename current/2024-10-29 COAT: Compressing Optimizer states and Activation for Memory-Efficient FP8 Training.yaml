date: "2024-10-29"
author: Haocheng Xi
title: 'COAT: Compressing Optimizer states and Activation for Memory-Efficient FP8 Training'
thumbnail: ""
link: https://huggingface.co/papers/2410.19313
summary: COAT is a new method for training large models that uses less memory and is faster than existing methods. It does this by compressing the information used during training and using a new way to represent numbers. This allows for larger models to be trained on fewer GPUs and for the batch size to be doubled in distributed training settings....
opinion: placeholder
tags:
    - ML
