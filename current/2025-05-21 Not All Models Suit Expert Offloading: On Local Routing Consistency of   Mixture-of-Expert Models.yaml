date: "2025-05-21"
author: Jingcong Liang
title: 'Not All Models Suit Expert Offloading: On Local Routing Consistency of   Mixture-of-Expert Models'
thumbnail: ""
link: https://huggingface.co/papers/2505.16056
summary: The study measures the efficiency of using a subset of experts in Mixture-of-Experts models for large language models on devices with limited memory. They introduce two metrics to evaluate how well a fixed group of experts can cover the needs of a segment of tokens and found that models using MoE on every layer and not sharing experts perform best. The research provides insights for designing and deploying memory-efficient models without sacrificing speed....
opinion: placeholder
tags:
    - ML
