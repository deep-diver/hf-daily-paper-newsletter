author: Yazhou Xing
date: '2024-02-28'
link: https://huggingface.co/papers/2402.17723
opinion: placeholder
summary: This paper proposes a framework for generating visual and audio content together,
  using existing models and a latent aligner to bridge the gap between them, instead
  of training giant models from scratch. The framework is shown to be effective in
  joint video-audio generation, visual-steered audio generation, and audio-steered
  visual generation tasks....
tags:
- Deep Learning
- Natural Language Processing
- Computer Vision
- Speech Recognition and Synthesis
thumbnail: https://github.com/deep-diver/hf-daily-paper-newsletter/blob/main/assets/2402.17723.gif?raw=true
title: 'Seeing and Hearing: Open-domain Visual-Audio Generation with Diffusion Latent
  Aligners'
translated_paths:
  INT: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.17723/paper.en.html
  KR: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.17723/paper.ko.html
