date: "2024-11-11"
author: Zhaofeng Wu
title: 'The Semantic Hub Hypothesis: Language Models Share Semantic Representations Across Languages and Modalities'
thumbnail: ""
link: https://huggingface.co/papers/2411.04986
summary: By training on diverse data types, language models learn to represent semantically similar inputs close together, regardless of their language or modality. This shared representation space is actively used by the model during input processing and can be interpreted using the model's dominant pretraining language....
opinion: placeholder
tags:
    - ML
