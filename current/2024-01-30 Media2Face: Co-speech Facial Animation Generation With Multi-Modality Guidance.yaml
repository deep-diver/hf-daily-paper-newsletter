author: Qingcheng Zhao
date: '2024-01-30'
link: https://huggingface.co/papers/2401.15687
opinion: placeholder
summary: This paper presents a new method called Media2Face for generating 3D facial
  animations from speech with multi-modality guidance. The method uses a variational
  auto-encoder called Generalized Neural Parametric Facial Asset (GNPFA) to extract
  high-quality expressions and accurate head poses from a large array of videos. It
  also proposes a diffusion model in GNPFA latent space for co-speech facial animation
  generation, which can accept rich multi-modality guidances from audio, text, and
  image. The ...
tags:
- Deep Learning
- Computer Vision
- Natural Language Processing
thumbnail: https://github.com/deep-diver/hf-daily-paper-newsletter/blob/main/assets/2401.15687.gif?raw=true
title: 'Media2Face: Co-speech Facial Animation Generation With Multi-Modality Guidance'
