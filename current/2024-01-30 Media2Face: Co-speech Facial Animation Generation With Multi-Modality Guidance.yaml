date: "2024-01-30"
author: Qingcheng Zhao
title: 'Media2Face: Co-speech Facial Animation Generation With Multi-Modality Guidance'
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/6258561f4d4291e8e63d8ae6/bD020RRfSYX4URGMY68P9.mp4
link: https://huggingface.co/papers/2401.15687
summary: This paper presents a new method called Media2Face for generating 3D facial animations from speech with multi-modality guidance. The method uses a variational auto-encoder called Generalized Neural Parametric Facial Asset (GNPFA) to extract high-quality expressions and accurate head poses from a large array of videos. It also proposes a diffusion model in GNPFA latent space for co-speech facial animation generation, which can accept rich multi-modality guidances from audio, text, and image. The ...
opinion: placeholder
tags:
    - Deep Learning
    - Computer Vision
    - Natural Language Processing
