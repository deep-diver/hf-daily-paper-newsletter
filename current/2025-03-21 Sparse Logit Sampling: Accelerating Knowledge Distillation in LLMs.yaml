date: "2025-03-21"
author: Anshumann
title: 'Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs'
thumbnail: ""
link: https://huggingface.co/papers/2503.16870
summary: The paper discusses a method to optimize knowledge distillation in large language models, which often requires significant computational resources. The authors found that existing methods, like caching top-K probabilities, can lead to biased estimates and suboptimal performance. They propose a new method called 'Random Sampling Knowledge Distillation,' which provides unbiased estimates and requires storing significantly sparser logits. This method allows for faster training of student models wit...
opinion: placeholder
tags:
    - ML
