date: "2024-11-01"
author: Ming Li
title: 'What Happened in LLMs Layers when Trained for Fast vs. Slow Thinking: A Gradient Perspective'
thumbnail: ""
link: https://huggingface.co/papers/2410.23743
summary: This paper investigates the training patterns of different layers in large language models (LLMs) through the lens of gradient, when training with different responses and initial models. It specifically focuses on how fast vs. slow thinking affects the layer-wise gradients, and how the pre-trained LLMs are less affected by the instability of fast thinking than instruction-tuned LLMs. The paper also studies whether the gradient patterns can reflect the correctness of responses when training diffe...
opinion: placeholder
tags:
    - ML
