date: "2025-04-11"
author: Wissam Antoun
title: ModernBERT or DeBERTaV3? Examining Architecture and Data Influence on   Transformer Encoder Models Performance
thumbnail: ""
link: https://huggingface.co/papers/2504.08716
summary: The study compares ModernBERT and DeBERTaV3 by pretraining ModernBERT on the same dataset as CamemBERTaV2, another DeBERTaV3 model. Results reveal that DeBERTaV3 remains superior in sample efficiency and overall benchmark performance, while ModernBERT's main advantage lies in faster training and inference speed. Additionally, high-quality pre-training data accelerates convergence but doesn't significantly improve final performance, suggesting benchmark saturation....
opinion: placeholder
tags:
    - ML
