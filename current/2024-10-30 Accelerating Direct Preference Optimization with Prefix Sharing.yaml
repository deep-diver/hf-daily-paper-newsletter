date: "2024-10-30"
author: Franklin Wang
title: Accelerating Direct Preference Optimization with Prefix Sharing
thumbnail: ""
link: https://huggingface.co/papers/2410.20305
summary: A new technique called prefix sharing for preference tuning is introduced to improve the training throughput of DPO datasets by processing chosen and rejected responses as one sequence with a shared prefix. This method achieves 1.1-1.5 times improvement in training throughput and is applicable to other paired preference tuning methods, making preference-based fine-tuning more accessible for a wider range of applications and model sizes....
opinion: placeholder
tags:
    - ML
