date: "2025-10-08"
author: Yunhao Fang
title: Artificial Hippocampus Networks for Efficient Long-Context Modeling
thumbnail: ""
link: https://huggingface.co/papers/2510.07318
summary: The authors present a new method for long-sequence modeling that combines the benefits of both RNN-like models and attention-based Transformers. Their approach, inspired by cognitive science, uses a sliding window of the Transformer's KV cache as short-term memory and a learnable module called Artificial Hippocampus Network to compress out-of-window information into a fixed-size long-term memory. This results in faster computations and reduced memory usage without sacrificing performance....
opinion: placeholder
tags:
    - ML
