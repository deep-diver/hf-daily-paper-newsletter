date: "2024-03-01"
author: Simran Arora
title: Simple linear attention language models balance the recall-throughput tradeoff
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/YfWiKNu_gMu7gX8CBQB2E.png
link: https://huggingface.co/papers/2402.18668
summary: The paper proposes a new language model architecture called BASED that balances the trade-off between recall and memory consumption. BASED uses a combination of linear and sliding window attention and can adjust its state size to improve recall without consuming too much memory. The model outperforms other attention alternatives in recall-intensive tasks and has higher throughput than standard attention implementations using IO-aware algorithms....
opinion: placeholder
tags:
    - Deep Learning
    - Natural Language Processing
