author: Simran Arora
date: '2024-03-01'
link: https://huggingface.co/papers/2402.18668
opinion: placeholder
summary: The paper proposes a new language model architecture called BASED that balances
  the trade-off between recall and memory consumption. BASED uses a combination of
  linear and sliding window attention and can adjust its state size to improve recall
  without consuming too much memory. The model outperforms other attention alternatives
  in recall-intensive tasks and has higher throughput than standard attention implementations
  using IO-aware algorithms....
tags:
- Deep Learning
- Natural Language Processing
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/YfWiKNu_gMu7gX8CBQB2E.png
title: Simple linear attention language models balance the recall-throughput tradeoff
translated_paths:
  INT: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.18668/paper.en.html
  KR: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.18668/paper.ko.html
