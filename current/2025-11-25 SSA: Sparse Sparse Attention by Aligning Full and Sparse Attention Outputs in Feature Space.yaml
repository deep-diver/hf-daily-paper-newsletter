date: "2025-11-25"
author: Zhenyi Shen
title: 'SSA: Sparse Sparse Attention by Aligning Full and Sparse Attention Outputs in Feature Space'
thumbnail: ""
link: https://huggingface.co/papers/2511.20102
summary: This study presents SSA, a new training framework for sparse attention in large language models that addresses the paradox of low sparsity in native sparse-attention methods. By enforcing bidirectional alignment between sparse and full attention at every layer, SSA preserves gradient flow and promotes stronger sparsity, resulting in state-of-the-art performance and flexible compute-performance trade-offs....
opinion: placeholder
tags:
    - ML
