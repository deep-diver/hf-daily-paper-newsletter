date: "2024-04-10"
author: Xingwei Qu
title: 'MuPT: A Generative Symbolic Music Pretrained Transformer'
thumbnail: https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.06393.png
link: https://huggingface.co/papers/2404.06393
summary: This paper explores using Large Language Models for pre-training music composition, finding that they perform better with ABC Notation than MIDI. The authors propose Synchronized Multi-Track ABC Notation for handling misaligned measures, and introduce models capable of handling 8192 tokens. They also investigate the Symbolic Music Scaling Law's impact on model performance, offering open-source contributions for future research....
opinion: placeholder
tags:
    - Unsupervised Learning
    - Deep Learning
    - Natural Language Processing
