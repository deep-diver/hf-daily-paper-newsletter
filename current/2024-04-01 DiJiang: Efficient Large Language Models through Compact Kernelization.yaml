date: "2024-04-01"
author: Hanting Chen
title: 'DiJiang: Efficient Large Language Models through Compact Kernelization'
thumbnail: https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.19928.png
link: https://huggingface.co/papers/2403.19928
summary: This paper presents DiJiang, a novel linear attention approach that transforms a pre-trained Transformer model into a model with linear complexity, reducing computational load and training costs. The approach uses a weighted Quasi-Monte Carlo method for sampling and Discrete Cosine Transform operations, resulting in comparable performance to the original Transformer with faster inference speeds and reduced training costs....
opinion: placeholder
tags:
    - Supervised Learning
    - Deep Learning
