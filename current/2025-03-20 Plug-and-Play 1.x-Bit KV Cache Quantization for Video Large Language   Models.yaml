date: "2025-03-20"
author: Keda Tao
title: Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language   Models
thumbnail: ""
link: https://huggingface.co/papers/2503.16257
summary: The research introduces VidKV, a novel method to compress the key-value (KV) cache in video large language models (VideoLLMs) to lower than 2 bits, using mixed-precision quantization and selective token filtering for better trade-offs between precision and model performance....
opinion: placeholder
tags:
    - ML
