date: "2025-06-09"
author: Zeju Qiu
title: Reparameterized LLM Training via Orthogonal Equivalence Transformation
thumbnail: ""
link: https://huggingface.co/papers/2506.08001
summary: The authors present a new training algorithm for large language models called POET, which uses Orthogonal Equivalence Transformation to optimize neurons and improve generalization. POET is efficient, flexible, and scalable for training large-scale neural networks, as demonstrated by extensive experiments....
opinion: placeholder
tags:
    - ML
