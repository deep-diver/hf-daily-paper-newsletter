date: "2024-01-25"
author: Junxiong Wang
title: 'MambaByte: Token-free Selective State Space Model'
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/B-gai_aNx76hER6seSPYz.png
link: https://huggingface.co/papers/2401.13660
summary: MambaByte is a new language model that learns directly from raw bytes instead of words or subwords. It is an autoregressive model that can handle long sequences better than other byte-level models and shows competitive performance with state-of-the-art subword models. MambaByte also has faster inference compared to Transformers due to its linear scaling in length....
opinion: placeholder
tags:
    - Deep Learning
    - Natural Language Processing
