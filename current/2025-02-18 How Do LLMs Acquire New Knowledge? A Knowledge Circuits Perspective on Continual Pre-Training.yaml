date: "2025-02-18"
author: Yixin Ou
title: How Do LLMs Acquire New Knowledge? A Knowledge Circuits Perspective on Continual Pre-Training
thumbnail: ""
link: https://huggingface.co/papers/2502.11196
summary: The paper explores how Large Language Models (LLMs) acquire new knowledge and the process of embedding it in their neural computations through knowledge circuits. Key findings include the influence of relevance to pre-existing knowledge, a phase shift from formation to optimization, and a deep-to-shallow evolution pattern. Understanding these mechanisms can help improve continual pre-training strategies for better model performance....
opinion: placeholder
tags:
    - ML
