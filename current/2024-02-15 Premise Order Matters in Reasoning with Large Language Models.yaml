date: "2024-02-15"
author: Xinyun Chen
title: Premise Order Matters in Reasoning with Large Language Models
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/TJjo8qFrgsLh972-ksi2q.png
link: https://huggingface.co/papers/2402.08939
summary: This study finds that large language models (LLMs) are sensitive to the order of premises and perform best when the premise order aligns with the context required in intermediate reasoning steps. The study shows that changing the premise order can cause a performance drop of over 30% and recommends releasing the benchmark R-GSM to examine the ordering effect for mathematical problem-solving....
opinion: placeholder
tags:
    - Natural Language Processing
