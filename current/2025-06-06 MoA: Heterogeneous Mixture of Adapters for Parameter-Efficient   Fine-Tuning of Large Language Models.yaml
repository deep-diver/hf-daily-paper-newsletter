date: "2025-06-06"
author: Jie Cao
title: 'MoA: Heterogeneous Mixture of Adapters for Parameter-Efficient   Fine-Tuning of Large Language Models'
thumbnail: ""
link: https://huggingface.co/papers/2506.05928
summary: The study presents a new method called MoA that uses a diverse mix of adapters to improve the performance and efficiency of fine-tuning large language models, addressing issues like representation collapse and expert load imbalance in existing methods....
opinion: placeholder
tags:
    - ML
