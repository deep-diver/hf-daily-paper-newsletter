date: "2025-04-14"
author: Minqian Liu
title: 'LLM Can be a Dangerous Persuader: Empirical Study of Persuasion Safety   in Large Language Models'
thumbnail: ""
link: https://huggingface.co/papers/2504.10430
summary: The study examines the safety risks of Large Language Models (LLMs) in persuasion, focusing on their ability to reject unethical tasks and avoid unethical strategies, as well as the impact of influencing factors like personality traits. The research presents PersuSafety, a framework for assessing persuasion safety across 8 popular LLMs, revealing significant safety concerns in most models....
opinion: placeholder
tags:
    - ML
