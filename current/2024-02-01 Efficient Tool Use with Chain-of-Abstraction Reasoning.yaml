date: "2024-02-01"
author: Silin Gao
title: Efficient Tool Use with Chain-of-Abstraction Reasoning
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/4GMhSwUSbIcRXS7lpkjVu.png
link: https://huggingface.co/papers/2401.17464
summary: The paper presents a new method called Chain-of-Abstraction (CoA) that improves the usage of tools in multi-step reasoning for large language models (LLMs). CoA trains LLMs to first decode reasoning chains with abstract placeholders and then call domain tools to fill in specific knowledge, allowing for parallel decoding and tool calling. The method shows consistent improvement in mathematical reasoning and Wiki QA tasks, with faster inference speeds compared to previous baselines....
opinion: placeholder
tags:
    - Supervised Learning
    - Deep Learning
    - Natural Language Processing
