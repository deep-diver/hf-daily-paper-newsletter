date: "2024-10-17"
author: Jixuan Leng
title: 'Taming Overconfidence in LLMs: Reward Calibration in RLHF'
thumbnail: ""
link: https://huggingface.co/papers/2410.09724
summary: This study investigates the overconfidence phenomenon in large language models (LLMs) trained with Reinforcement Learning from Human Feedback (RLHF) and proposes two PPO variants, PPO-M and PPO-C, to address this issue. The methods are evaluated on Llama3-8B and Mistral-7B across six diverse datasets, and the results show that both methods can reduce calibration error and maintain performance comparable to standard PPO without compromising model capabilities in open-ended conversation settings....
opinion: placeholder
tags:
    - ML
