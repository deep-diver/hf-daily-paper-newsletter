date: "2024-10-10"
author: Jingwei Zuo
title: 'Falcon Mamba: The First Competitive Attention-free 7B Language Model'
thumbnail: ""
link: https://huggingface.co/papers/2410.05355
summary: Falcon Mamba 7B is a new language model based on the Mamba architecture that outperforms leading open-weight models based on Transformers, such as Mistral 7B, Llama3.1 8B, and Falcon2 11B. It is faster at inference and requires less memory for long sequence generation. The weights of our implementation of Falcon Mamba 7B are publicly available on https://huggingface.co/tiiuae/falcon-mamba-7b, under a permissive license....
opinion: placeholder
tags:
    - ML
