date: "2024-03-07"
author: Xin Men
title: 'ShortGPT: Layers in Large Language Models are More Redundant Than You Expect'
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/8My5G3ybxfrV5_fP1CE47.png
link: https://huggingface.co/papers/2403.03853
summary: Researchers found that many layers in large language models are redundant and proposed a method called ShortGPT that removes these layers based on their Block Influence scores. This method significantly outperforms previous state-of-the-art methods in model pruning and can be used in conjunction with other methods to further reduce parameters and computation....
opinion: placeholder
tags:
    - Natural Language Processing
    - Deep Learning
