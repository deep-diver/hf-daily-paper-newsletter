date: "2024-03-01"
author: Soham De
title: 'Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models'
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/5e68p09De9fBpLfPl_uUJ.png
link: https://huggingface.co/papers/2402.19427
summary: This paper proposes Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that mixes gated linear recurrences with local attention. Griffin matches the performance of Llama-2 despite being trained on 6 times fewer tokens and can extrapolate on long sequences. Griffin is also more efficient in hardware during inference than Transformers....
opinion: placeholder
tags:
    - Natural Language Processing
    - Deep Learning
    - Optimization and Learning Algorithms
