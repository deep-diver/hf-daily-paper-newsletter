date: "2024-08-26"
author: Kaizhao Liang
title: Memory-Efficient LLM Training with Online Subspace Descent
thumbnail: ""
link: https://huggingface.co/papers/2408.12857
summary: This paper introduces a new memory-efficient LLM training algorithm called Online Subspace Descent. It provides the first convergence guarantee for arbitrary update rules of the projection matrix and reduces the overhead of training. The algorithm achieves lower perplexity and better downstream tasks performance than state-of-the-art low-rank training methods, narrowing the gap with full-rank baselines for pretraining LLaMA models....
opinion: placeholder
tags:
    - ML
