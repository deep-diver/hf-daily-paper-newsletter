date: "2024-10-25"
author: Zhanchao Zhou
title: Value Residual Learning For Alleviating Attention Concentration In Transformers
thumbnail: ""
link: https://huggingface.co/papers/2410.17897
summary: This paper introduces the Transformer with residual value (ResFormer) and the Transformer with single layer value (SVFormer). ResFormer mitigates attention concentration in deeper layers and enhances representation across most layers, while SVFormer trains faster and performs better than other methods. Both methods outperform the vanilla Transformer in training error and downstream tasks....
opinion: placeholder
tags:
    - ML
