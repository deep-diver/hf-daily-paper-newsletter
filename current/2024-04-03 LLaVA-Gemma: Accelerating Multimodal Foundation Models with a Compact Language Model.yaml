date: "2024-04-03"
author: Musashi Hinck
title: 'LLaVA-Gemma: Accelerating Multimodal Foundation Models with a Compact Language Model'
thumbnail: https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.01331.png
link: https://huggingface.co/papers/2404.01331
summary: The paper presents LLaVA-Gemma, a suite of multimodal foundation models using the Gemma family of large language models. The models are tested with different design features, but do not improve upon existing comparable models. The results show that skipping pretraining reduces performance, bigger vision models can improve performance, and increasing language model size has mixed effects. The models are publicly available....
opinion: placeholder
tags:
    - Supervised Learning
    - Deep Learning
    - Natural Language Processing
