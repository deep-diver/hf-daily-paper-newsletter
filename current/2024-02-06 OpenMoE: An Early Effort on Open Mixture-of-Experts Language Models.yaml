author: Fuzhao Xue
date: '2024-02-06'
link: https://huggingface.co/papers/2402.01739
opinion: placeholder
summary: This study presents OpenMoE, a series of fully open-sourced and reproducible
  language models that use a Mixture-of-Experts approach. The researchers analyze
  the routing mechanisms within OpenMoE models and find that routing decisions are
  primarily based on token IDs and that assignments are determined early in the pre-training
  phase. They also propose strategies for improving MoE language model designs....
tags:
- Natural Language Processing
- Deep Learning
- Explainable AI and Interpretability
- Emerging Applications of Machine Learning
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/ckrD-RyY0OSqFChHuqTxJ.png
title: 'OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models'
translated_paths:
  INT: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.01739/paper.en.html
  KR: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.01739/paper.ko.html
