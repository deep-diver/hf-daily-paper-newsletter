author: Zhuofeng Wu
date: '2024-02-26'
link: https://huggingface.co/papers/2402.15000
opinion: placeholder
summary: 'This paper proposes a strategy to break down reasoning tasks into two phases:
  problem decomposition and problem solving. The authors show that it is easier to
  distill the problem decomposition phase into a smaller model compared to the problem
  solving phase, which requires large amounts of domain knowledge. They evaluate the
  impact of distilling these two capabilities on reasoning outcomes and inference
  cost, and find that distilling the problem decomposition phase can achieve good
  generalizatio...'
tags:
- Supervised Learning
- Deep Learning
- Natural Language Processing
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/hr_KNu9asHJ_Kk6XOb0ex.png
title: Divide-or-Conquer? Which Part Should You Distill Your LLM?
translated_paths:
  INT: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.15000/paper.en.html
  KR: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.15000/paper.ko.html
