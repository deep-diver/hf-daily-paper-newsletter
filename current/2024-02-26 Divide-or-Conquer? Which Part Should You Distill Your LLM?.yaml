date: "2024-02-26"
author: Zhuofeng Wu
title: Divide-or-Conquer? Which Part Should You Distill Your LLM?
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/hr_KNu9asHJ_Kk6XOb0ex.png
link: https://huggingface.co/papers/2402.15000
summary: 'This paper proposes a strategy to break down reasoning tasks into two phases: problem decomposition and problem solving. The authors show that it is easier to distill the problem decomposition phase into a smaller model compared to the problem solving phase, which requires large amounts of domain knowledge. They evaluate the impact of distilling these two capabilities on reasoning outcomes and inference cost, and find that distilling the problem decomposition phase can achieve good generalizatio...'
opinion: placeholder
tags:
    - Supervised Learning
    - Deep Learning
    - Natural Language Processing
