date: "2024-06-18"
author: Wenxuan Zhou
title: 'WPO: Enhancing RLHF with Weighted Preference Optimization'
thumbnail: ""
link: https://huggingface.co/papers/2406.11827
summary: Reinforcement learning from human feedback (RLHF) is a promising solution to align large language models (LLMs) more closely with human values. Off-policy preference optimization, where the preference data is obtained from other models, is widely adopted due to its cost efficiency and scalability. However, off-policy preference optimization often suffers from a distributional gap between the policy used for data collection and the target policy, leading to suboptimal optimization. In this paper,...
opinion: placeholder
tags:
    - ML
