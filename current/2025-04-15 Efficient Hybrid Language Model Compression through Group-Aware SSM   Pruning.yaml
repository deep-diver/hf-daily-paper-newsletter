date: "2025-04-15"
author: Ali Taghibakhshi
title: Efficient Hybrid Language Model Compression through Group-Aware SSM   Pruning
thumbnail: ""
link: https://huggingface.co/papers/2504.11409
summary: The paper presents a novel group-aware pruning strategy for compressing Hybrid LLM architectures, which combine Attention and State Space Models (SSMs. The approach improves accuracy and inference speed compared to traditional methods by preserving the structural integrity of SSM blocks and their sequence modeling capabilities, resulting in a 4B parameter model that outperforms similarly-sized models with 2x faster inference....
opinion: placeholder
tags:
    - ML
