date: "2025-03-07"
author: Yujiao Yang
title: 'Union of Experts: Adapting Hierarchical Routing to Equivalently Decomposed Transformer'
thumbnail: ""
link: https://huggingface.co/papers/2503.02495
summary: The authors present a novel Mixture-of-Experts (MoE) approach called Union-of-Experts (UoE) that decomposes transformer into a group of experts, implementing dynamic routing on input data and experts for improved performance. UoE introduces key innovations like equitant expert decomposition, patch-wise data and expert selection, and Selective Multi-Head Attention (SMHA) and Union-of-MLP-Experts (UoME) architecture, which outperform state-of-the-art MoEs and efficient transformers in various imag...
opinion: placeholder
tags:
    - ML
