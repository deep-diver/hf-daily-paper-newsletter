date: "2024-04-16"
author: Dongseong Hwang
title: 'TransformerFAM: Feedback attention is working memory'
thumbnail: https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.09173.png
link: https://huggingface.co/papers/2404.09173
summary: This paper proposes a new architecture, Feedback Attention Memory (FAM), to enable Transformers to process infinitely long inputs by creating a working memory within the network. TransformerFAM, a model using FAM, significantly improves performance on long-context tasks without requiring additional weights....
opinion: placeholder
tags:
    - Deep Learning
    - Natural Language Processing
