author: Shiyuan Yang
date: '2024-02-06'
link: https://huggingface.co/papers/2402.03162
opinion: placeholder
summary: This paper presents Direct-a-Video, a system that enables users to independently
  control object motion and camera movement in video generation, allowing for customized
  video creation. The system utilizes spatial cross-attention modulation for object
  motion and introduces temporal cross-attention layers for camera movement, with
  both components operating independently and generalizing to open-domain scenarios....
tags:
- Computer Vision
- Deep Learning
- Emerging Applications of Machine Learning
- Human-Computer Interaction (HCI) and User Interfaces
- Optimization and Learning Algorithms
thumbnail: https://github.com/deep-diver/hf-daily-paper-newsletter/blob/main/assets/2402.03162.gif?raw=true
title: 'Direct-a-Video: Customized Video Generation with User-Directed Camera Movement
  and Object Motion'
translated_paths:
  INT: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.03162/paper.en.html
  KR: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.03162/paper.ko.html
