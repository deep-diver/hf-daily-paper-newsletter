date: "2024-04-24"
author: Anej Svete
title: Transformers Can Represent $n$-gram Language Models
thumbnail: https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.14994.png
link: https://huggingface.co/papers/2404.14994
summary: This paper explores the relationship between Transformer language models and n-gram language models, showing that Transformer models using hard or sparse attention mechanisms can exactly represent any n-gram model, giving a lower bound on their probabilistic representational capacity....
opinion: placeholder
tags:
    - Supervised Learning
    - Deep Learning
    - Natural Language Processing
