date: "2024-06-06"
author: Namgyu Ho
title: 'Block Transformer: Global-to-Local Language Modeling for Fast Inference'
thumbnail: https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.02657.png
link: https://huggingface.co/papers/2406.02657
summary: This paper presents the Block Transformer architecture which adopts hierarchical global-to-local modeling to autoregressive transformers to mitigate the inference bottlenecks of self-attention. To apply self-attention, the key-value (KV) cache of all previous sequences must be retrieved from memory at every decoding step. Thereby, this KV cache IO becomes a significant bottleneck in batch inference. We notice that these costs stem from applying self-attention on the global context, therefore we ...
opinion: placeholder
tags:
    - Natural Language Processing
