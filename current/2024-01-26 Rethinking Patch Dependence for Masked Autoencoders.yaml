date: "2024-01-26"
author: Letian Fu
title: Rethinking Patch Dependence for Masked Autoencoders
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/-5nF2xoe2-zFU8JpJn5n1.png
link: https://huggingface.co/papers/2401.14391
summary: This paper proposes a new pretraining framework called Cross-Attention Masked Autoencoders (CrossMAE) which reduces the decoding compute by up to 3.7 times compared to the existing masked autoencoders (MAE) without compromising performance....
opinion: placeholder
tags:
    - Deep Learning
