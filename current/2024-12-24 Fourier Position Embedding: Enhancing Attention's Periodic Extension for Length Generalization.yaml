date: "2024-12-24"
author: Ermo Hua
title: 'Fourier Position Embedding: Enhancing Attention''s Periodic Extension for Length Generalization'
thumbnail: ""
link: https://huggingface.co/papers/2412.17739
summary: Fourier Position Embedding (FoPE) is introduced as an enhancement to Rotary Position Embedding (RoPE) in Language Models (LMs). FoPE improves the periodic extension and length generalization of RoPE-based attention by addressing the adverse effects of linear layers and activation functions outside of attention, as well as insufficiently trained frequency components caused by time-domain truncation. FoPE constructs Fourier Series and zero-outs destructive frequency components, increasing model ro...
opinion: placeholder
tags:
    - ML
