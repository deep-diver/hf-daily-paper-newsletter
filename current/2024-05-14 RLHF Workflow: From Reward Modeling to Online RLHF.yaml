date: "2024-05-14"
author: Hanze Dong
title: 'RLHF Workflow: From Reward Modeling to Online RLHF'
thumbnail: https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.07863.png
link: https://huggingface.co/papers/2405.07863
summary: The technical report presents a detailed recipe for online iterative RLHF, a method for training language models using human feedback. The report uses proxy preference models and open-source datasets to approximate human feedback and achieves impressive performance on various benchmarks....
opinion: placeholder
tags:
    - Reinforcement Learning
    - Natural Language Processing
