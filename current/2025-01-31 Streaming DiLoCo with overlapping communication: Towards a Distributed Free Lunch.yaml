date: "2025-01-31"
author: Arthur Douillard
title: 'Streaming DiLoCo with overlapping communication: Towards a Distributed Free Lunch'
thumbnail: ""
link: https://huggingface.co/papers/2501.18512
summary: This study enhances the DiLoCo distributed algorithm for training large language models. It reduces peak bandwidth requirement by synchronizing subsets of parameters sequentially, decreases wall clock time by allowing worker nodes to continue training during synchronization, and further reduces bandwidth by quantizing data exchanged between nodes. These changes allow for similar quality training of billion-scale parameters while requiring two orders of magnitude less bandwidth....
opinion: placeholder
tags:
    - ML
