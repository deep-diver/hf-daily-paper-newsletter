date: "2025-05-12"
author: Yuanhang Yang
title: 'UMoE: Unifying Attention and FFN with Shared Experts'
thumbnail: ""
link: https://huggingface.co/papers/2505.07260
summary: The paper presents a new architecture called UMoE that combines attention and FFN layers in Sparse Mixture of Experts models. By restructuring the attention mechanism, UMoE reveals a similarity to FFN layers, allowing for efficient parameter sharing and improved performance compared to existing attention-based MoE layers....
opinion: placeholder
tags:
    - ML
