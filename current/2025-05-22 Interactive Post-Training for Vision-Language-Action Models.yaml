date: "2025-05-22"
author: Shuhan Tan
title: Interactive Post-Training for Vision-Language-Action Models
thumbnail: ""
link: https://huggingface.co/papers/2505.17016
summary: The study presents RIPT-VLA, a reinforcement learning method that improves pre-trained Vision-Language-Action models using sparse rewards, allowing for adaptation to new tasks and environments with minimal data. RIPT-VLA is efficient, generalizable, and can significantly enhance the performance of various VLA models, including improving a model's success rate from 4% to 97% with just one demonstration....
opinion: placeholder
tags:
    - ML
