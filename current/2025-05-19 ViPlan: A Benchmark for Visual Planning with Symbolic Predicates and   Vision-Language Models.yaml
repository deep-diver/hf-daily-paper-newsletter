date: "2025-05-19"
author: Matteo Merler
title: 'ViPlan: A Benchmark for Visual Planning with Symbolic Predicates and   Vision-Language Models'
thumbnail: ""
link: https://huggingface.co/papers/2505.13180
summary: The researchers present ViPlan, an open-source benchmark for visual planning using symbolic predicates and Vision-Language Models (VLMs). They test various VLM models in two domains and find that symbolic planning is better in tasks requiring accurate image grounding, while direct VLM planning performs better in tasks needing commonsense knowledge and error recovery....
opinion: placeholder
tags:
    - ML
