date: "2025-07-07"
author: Chenchen Zhang
title: 'ArtifactsBench: Bridging the Visual-Interactive Gap in LLM Code   Generation Evaluation'
thumbnail: ""
link: https://huggingface.co/papers/2507.04952
summary: A new benchmark called ArtifactsBench has been developed to evaluate the visual and interactive quality of code generated by large language models. This benchmark automatically assesses the generated artifacts' dynamic behavior and compares it with human-generated artifacts, achieving high consistency and agreement with human evaluation....
opinion: placeholder
tags:
    - ML
