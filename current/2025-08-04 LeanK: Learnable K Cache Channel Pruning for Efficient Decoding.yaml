date: "2025-08-04"
author: Yike Zhang
title: 'LeanK: Learnable K Cache Channel Pruning for Efficient Decoding'
thumbnail: ""
link: https://huggingface.co/papers/2508.02215
summary: The authors present a method called LeanK that optimizes the efficiency of large language models by selectively removing less important components in the key-value cache, thereby reducing memory usage and accelerating decoding without compromising accuracy. Experimental results show significant memory savings and improved performance, along with insights into model channels and attention heads during long-context inference....
opinion: placeholder
tags:
    - ML
