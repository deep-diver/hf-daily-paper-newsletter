date: "2024-03-14"
author: Adam Ibrahim
title: Simple and Scalable Strategies to Continually Pre-train Large Language Models
thumbnail: https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.08763.png
link: https://huggingface.co/papers/2403.08763
summary: A research paper presents simple and scalable strategies to continually pre-train large language models, achieving performance comparable to re-training from scratch, while saving significant compute. The strategies include learning rate re-warming, re-decaying, and replay of previous data, and are demonstrated at scale with different distribution shifts and model sizes....
opinion: placeholder
tags:
    - ML
