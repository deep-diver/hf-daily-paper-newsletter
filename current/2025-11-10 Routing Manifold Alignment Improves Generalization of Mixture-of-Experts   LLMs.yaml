date: "2025-11-10"
author: Zhongyang Li
title: Routing Manifold Alignment Improves Generalization of Mixture-of-Experts   LLMs
thumbnail: ""
link: https://huggingface.co/papers/2511.07419
summary: The study presents a method called Routing Manifold Alignment (RoMA) that enhances the performance of large language models with Sparse Mixture-of-Experts by aligning routing weights with task embedding. RoMA introduces a regularization term that encourages similar expert choices for samples targeting similar tasks, which significantly improves the generalization of these models. The method only requires lightweight finetuning and demonstrates substantial improvement in various benchmarks and co...
opinion: placeholder
tags:
    - ML
