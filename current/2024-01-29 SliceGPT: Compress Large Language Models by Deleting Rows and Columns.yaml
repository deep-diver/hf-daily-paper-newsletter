author: Saleh Ashkboos
date: '2024-01-29'
link: https://huggingface.co/papers/2401.15024
opinion: placeholder
summary: This paper presents SliceGPT, a new method for compressing large language
  models by replacing weight matrices with smaller ones, reducing computation and
  memory demands while maintaining performance. The method can remove up to 25% of
  the model parameters and run faster on less powerful GPUs, with a new insight on
  computational invariance in transformer networks that can inspire future advances
  in compression....
tags:
- Deep Learning
- Optimization and Learning Algorithms
- Natural Language Processing
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/X2ZXpHxYkhtCfKRwLZ0eP.png
title: 'SliceGPT: Compress Large Language Models by Deleting Rows and Columns'
translated_paths:
  INT: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2401.15024/paper.en.html
  KR: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2401.15024/paper.ko.html
