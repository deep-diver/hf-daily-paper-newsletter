date: "2024-10-18"
author: Isack Lee
title: Do LLMs Have Political Correctness? Analyzing Ethical Biases and Jailbreak Vulnerabilities in AI Systems
thumbnail: ""
link: https://huggingface.co/papers/2410.13334
summary: This paper examines intentional biases in large language models (LLMs) for safety purposes and proposes a defense method called PCDefense to prevent jailbreak attempts. The paper highlights the risks posed by these safety-induced biases and the need for responsible design and implementation of safety measures by LLM developers....
opinion: placeholder
tags:
    - ML
