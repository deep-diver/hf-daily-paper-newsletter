author: Tokiniaina Raharison Ralambomihanta
date: '2024-02-01'
link: https://huggingface.co/papers/2401.17574
opinion: placeholder
summary: This paper proposes using knowledge distillation to transfer ideas from transformer
  models to more efficient long convolution models, while addressing the challenge
  of processing long contextual information, and improving both accuracy and efficiency....
tags:
- Supervised Learning
- Deep Learning
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/FjsCL2gUQRX8STXYykG8h.png
title: 'Scavenging Hyena: Distilling Transformers into Long Convolution Models'
translated_paths:
  INT: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2401.17574/paper.en.html
  KR: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2401.17574/paper.ko.html
