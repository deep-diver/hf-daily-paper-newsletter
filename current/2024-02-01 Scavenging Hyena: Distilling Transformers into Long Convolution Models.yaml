date: "2024-02-01"
author: Tokiniaina Raharison Ralambomihanta
title: 'Scavenging Hyena: Distilling Transformers into Long Convolution Models'
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/FjsCL2gUQRX8STXYykG8h.png
link: https://huggingface.co/papers/2401.17574
summary: This paper proposes using knowledge distillation to transfer ideas from transformer models to more efficient long convolution models, while addressing the challenge of processing long contextual information, and improving both accuracy and efficiency....
opinion: placeholder
tags:
    - Supervised Learning
    - Deep Learning
