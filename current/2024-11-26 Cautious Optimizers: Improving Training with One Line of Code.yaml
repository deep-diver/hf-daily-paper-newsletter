date: "2024-11-26"
author: Kaizhao Liang
title: 'Cautious Optimizers: Improving Training with One Line of Code'
thumbnail: ""
link: https://huggingface.co/papers/2411.16085
summary: We propose a simple modification to any momentum-based optimizer called Cautious Optimizer, which improves training performance without compromising stability. Our theoretical analysis shows that this modification preserves the convergence guarantee under the Lyapunov analysis, and it reveals a new family of optimizers. Empirical experiments show up to 1.47 times speed-up on Llama and MAE pretraining. The code is available at https://github.com/kyleliang919/C-Optim....
opinion: placeholder
tags:
    - ML
