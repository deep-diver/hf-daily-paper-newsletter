date: "2025-09-01"
author: Andong Hua
title: Flaw or Artifact? Rethinking Prompt Sensitivity in Evaluating LLMs
thumbnail: ""
link: https://huggingface.co/papers/2509.01790
summary: The study questions whether the high sensitivity of large language models (LLMs) to different prompts is a real flaw or just a result of the way these models are evaluated. By testing 7 LLMs using various benchmarks and prompt templates, the researchers found that the perceived prompt sensitivity often comes from the evaluation methods, which can overlook correct responses that are phrased differently. When using a more advanced evaluation method, the performance of LLMs was more consistent, sug...
opinion: placeholder
tags:
    - ML
