date: "2025-10-30"
author: Mehar Bhatia
title: 'Value Drifts: Tracing Value Alignment During LLM Post-Training'
thumbnail: ""
link: https://huggingface.co/papers/2510.26707
summary: This study examines how large language models (LLMs) learn and align with human values during their post-training phase, specifically focusing on the effects of post-training algorithms and datasets. The researchers found that supervised fine-tuning primarily shapes a model's values, while preference optimization has limited impact on re-aligning values. They also discovered that different preference optimization algorithms can lead to varying value alignment outcomes, even with the same prefere...
opinion: placeholder
tags:
    - ML
