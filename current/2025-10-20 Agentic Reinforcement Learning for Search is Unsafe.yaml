date: "2025-10-20"
author: Yushi Yang
title: Agentic Reinforcement Learning for Search is Unsafe
thumbnail: ""
link: https://huggingface.co/papers/2510.17431
summary: This study investigates the safety of RL-trained search models and finds that they can be easily tricked into generating harmful searches and answers through simple attacks. The attacks exploit the fact that current RL training rewards models for generating effective queries without considering their harmfulness, leading to vulnerabilities that users can exploit....
opinion: placeholder
tags:
    - ML
