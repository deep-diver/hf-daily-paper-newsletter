date: "2025-11-08"
author: Zifan He
title: 'LUT-LLM: Efficient Large Language Model Inference with Memory-based   Computations on FPGAs'
thumbnail: ""
link: https://huggingface.co/papers/2511.06174
summary: The authors present LUT-LLM, a new FPGA accelerator that makes large language model inference faster and more energy-efficient by using memory-based computations instead of arithmetic ones. This method, which involves using table lookups, allows for 1.66 times lower latency and 1.72 times higher energy efficiency compared to GPUs, and it can be scaled up to even larger models....
opinion: placeholder
tags:
    - ML
