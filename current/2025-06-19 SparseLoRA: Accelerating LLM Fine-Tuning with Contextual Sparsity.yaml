date: "2025-06-19"
author: Samir Khaki
title: 'SparseLoRA: Accelerating LLM Fine-Tuning with Contextual Sparsity'
thumbnail: ""
link: https://huggingface.co/papers/2506.16500
summary: SparseLoRA is a method that speeds up fine-tuning large language models (LLMs) by using only a small set of important weights during training, without losing accuracy. It reduces computational cost by up to 2.2 times and improves training speed by up to 1.6 times, making it more efficient for various tasks like commonsense reasoning, code generation, and following instructions....
opinion: placeholder
tags:
    - ML
