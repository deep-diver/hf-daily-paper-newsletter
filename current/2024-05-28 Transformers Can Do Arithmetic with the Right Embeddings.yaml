date: "2024-05-28"
author: Sean McLeish
title: Transformers Can Do Arithmetic with the Right Embeddings
thumbnail: https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.17399.png
link: https://huggingface.co/papers/2405.17399
summary: Transformers can perform arithmetic tasks more accurately by adding embeddings that encode the position of each digit within a number. This allows for architectural modifications such as input injection and recurrent layers to improve performance even further. By resolving positions, transformers can solve larger and more complex arithmetic problems, achieving up to 99% accuracy on 100-digit addition problems. These gains in numeracy also improve performance on other multi-step reasoning tasks l...
opinion: placeholder
tags:
    - Deep Learning
    - Natural Language Processing
