date: "2024-02-28"
author: Shuming Ma
title: 'The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits'
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/MBjSFighU68b4s8iAx5Iv.png
link: https://huggingface.co/papers/2402.17764
summary: This paper presents BitNet b1.58, a 1-bit Large Language Model that matches the performance of full-precision LLMs while being more cost-effective. The 1.58-bit LLM defines a new scaling law and recipe for training new generations of LLMs, enabling a new computation paradigm and opening the door for specific hardware optimization....
opinion: placeholder
tags:
    - Deep Learning
    - Natural Language Processing
