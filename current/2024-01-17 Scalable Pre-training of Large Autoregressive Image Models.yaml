date: "2024-01-17"
author: Alaaeldin El-Nouby
title: Scalable Pre-training of Large Autoregressive Image Models
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/agQVTT_BcFBvRnhUcBL4R.png
link: https://huggingface.co/papers/2401.08541
summary: This paper proposes AIM, large vision models pre-trained with an autoregressive objective. The paper highlights that the performance of the models scales with model capacity and data quantity, and their objective function correlates with downstream task performance. The paper pre-trains a 7 billion parameter AIM on 2 billion images, achieving good performance on ImageNet-1k. The pre-training process is similar to that of LLMs and does not require image-specific strategies....
opinion: placeholder
tags:
    - Computer Vision
    - Deep Learning
