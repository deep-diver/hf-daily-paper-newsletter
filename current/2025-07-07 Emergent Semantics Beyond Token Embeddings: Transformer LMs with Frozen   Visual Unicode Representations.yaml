date: "2025-07-07"
author: A. Bochkov
title: 'Emergent Semantics Beyond Token Embeddings: Transformer LMs with Frozen   Visual Unicode Representations'
thumbnail: ""
link: https://huggingface.co/papers/2507.04886
summary: This study presents a new approach for training Transformer models where the embedding layer is fixed, using visual Unicode glyphs instead of semantic data. The resulting models, despite having non-trainable embeddings, perform better than traditional models with trainable embeddings on a reasoning benchmark, suggesting that high-level semantics are an emergent property of the model's architecture and data scale....
opinion: placeholder
tags:
    - ML
