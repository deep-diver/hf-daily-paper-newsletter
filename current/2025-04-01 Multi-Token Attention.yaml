date: "2025-04-01"
author: Olga Golovneva
title: Multi-Token Attention
thumbnail: ""
link: https://huggingface.co/papers/2504.00927
summary: The authors present a new attention method, Multi-Token Attention (MTA), that lets Large Language Models (LLMs) use multiple query and key vectors for attention weights, enhancing context understanding. MTA uses convolution operations to allow nearby queries and keys to affect each other's attention weights, providing more precise and richer information, which improves performance on various language tasks, especially those requiring searching within long contexts....
opinion: placeholder
tags:
    - ML
