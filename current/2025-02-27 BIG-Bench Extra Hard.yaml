date: "2025-02-27"
author: Mehran Kazemi
title: BIG-Bench Extra Hard
thumbnail: ""
link: https://huggingface.co/papers/2502.19187
summary: The BIG-Bench Extra Hard (BBEH) benchmark is introduced to evaluate the reasoning capabilities of large language models (LLMs) beyond mathematical and coding abilities. BBEH replaces each task in the BIG-Bench Hard dataset with a more challenging one, and initial evaluations reveal significant room for improvement, with the best general-purpose model achieving an average accuracy of 9.8%....
opinion: placeholder
tags:
    - ML
