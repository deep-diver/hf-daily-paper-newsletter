date: "2024-02-23"
author: Xingyou Song
title: 'OmniPred: Language Models as Universal Regressors'
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/ijyBcI4TRzqJmU2GlTOwr.png
link: https://huggingface.co/papers/2402.14547
summary: This paper proposes OmniPred, a framework for training language models as universal end-to-end regressors for predicting outcomes of different experiments. Experiments show that language models, even without numerical values, can predict outcomes with high accuracy, and outperform traditional regression models when trained on multiple tasks....
opinion: placeholder
tags:
    - Natural Language Processing
    - Deep Learning
    - Supervised Learning
