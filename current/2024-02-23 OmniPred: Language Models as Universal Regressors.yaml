author: Xingyou Song
date: '2024-02-23'
link: https://huggingface.co/papers/2402.14547
opinion: placeholder
summary: This paper proposes OmniPred, a framework for training language models as
  universal end-to-end regressors for predicting outcomes of different experiments.
  Experiments show that language models, even without numerical values, can predict
  outcomes with high accuracy, and outperform traditional regression models when trained
  on multiple tasks....
tags:
- Natural Language Processing
- Deep Learning
- Supervised Learning
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/ijyBcI4TRzqJmU2GlTOwr.png
title: 'OmniPred: Language Models as Universal Regressors'
translated_paths:
  INT: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.14547/paper.en.html
  KR: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.14547/paper.ko.html
