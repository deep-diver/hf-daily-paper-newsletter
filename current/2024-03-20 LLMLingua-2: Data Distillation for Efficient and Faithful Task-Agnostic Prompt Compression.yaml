date: "2024-03-20"
author: Zhuoshi Pan
title: 'LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression'
thumbnail: https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.12968.png
link: https://huggingface.co/papers/2403.12968
summary: This paper introduces a new approach for compressing prompts called LLMLingua-2. It uses a data distillation procedure to capture essential information from a language model, and formulates prompt compression as a token classification problem. The method leads to lower latency and improved generalization performance compared to existing approaches....
opinion: placeholder
tags:
    - Supervised Learning
