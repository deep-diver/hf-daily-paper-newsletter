author: Zhuoshi Pan
date: '2024-03-20'
link: https://huggingface.co/papers/2403.12968
opinion: placeholder
summary: This paper introduces a new approach for compressing prompts called LLMLingua-2.
  It uses a data distillation procedure to capture essential information from a language
  model, and formulates prompt compression as a token classification problem. The
  method leads to lower latency and improved generalization performance compared to
  existing approaches....
tags:
- Supervised Learning
thumbnail: https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.12968.png
title: 'LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt
  Compression'
translated_paths:
  INT: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2403.12968/paper.en.html
  KR: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2403.12968/paper.ko.html
