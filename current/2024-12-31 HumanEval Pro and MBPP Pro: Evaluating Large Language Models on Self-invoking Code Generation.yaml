date: "2024-12-31"
author: Zhaojian Yu
title: 'HumanEval Pro and MBPP Pro: Evaluating Large Language Models on Self-invoking Code Generation'
thumbnail: ""
link: https://huggingface.co/papers/2412.21199
summary: 'This paper introduces a new task called self-invoking code generation to evaluate the problem-solving abilities of Large Language Models (LLMs). The task involves solving a base problem and using its solution to address a more complex problem. The paper proposes three new benchmarks: HumanEval Pro, MBPP Pro, and BigCodeBench-Lite Pro, specifically designed for this task. The experimental results show that most LLMs perform well on traditional code generation benchmarks but struggle with self-inv...'
opinion: placeholder
tags:
    - ML
