date: "2025-02-04"
author: Xiang Liu
title: Can LLMs Maintain Fundamental Abilities under KV Cache Compression?
thumbnail: ""
link: https://huggingface.co/papers/2502.01941
summary: The study examines the effect of KV cache compression methods on LLMs' abilities across various tasks. Results show that arithmetic reasoning tasks are particularly sensitive to aggressive compression, and a new method called ShotKV is proposed to improve performance on long-context generation tasks under aggressive compression ratios....
opinion: placeholder
tags:
    - ML
