date: "2025-11-01"
author: Ahmet Erdem Pamuk
title: 'Superpositional Gradient Descent: Harnessing Quantum Principles for   Model Training'
thumbnail: ""
link: https://huggingface.co/papers/2511.01918
summary: The researchers have developed a new optimizer called Superpositional Gradient Descent (SGD) that uses quantum circuit perturbations to improve the convergence and performance of large language models compared to the classical AdamW optimizer. However, practical adoption of SGD is currently limited by scalability and hardware constraints....
opinion: placeholder
tags:
    - ML
