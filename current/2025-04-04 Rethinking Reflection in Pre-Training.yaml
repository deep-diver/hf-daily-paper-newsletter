date: "2025-04-04"
author: Essential AI
title: Rethinking Reflection in Pre-Training
thumbnail: ""
link: https://huggingface.co/papers/2504.04022
summary: The paper explores how language models' ability to reflect on their own reasoning starts emerging during pre-training, not just during reinforcement learning. They study this by introducing deliberate errors and observing if the model can correct itself. The experiment shows that this self-correction ability appears early and improves over time, as seen in an OLMo2-7B model pre-trained on 4 trillion tokens....
opinion: placeholder
tags:
    - ML
