date: "2025-03-09"
author: Yanling Wang
title: 'VisualSimpleQA: A Benchmark for Decoupled Evaluation of Large   Vision-Language Models in Fact-Seeking Question Answering'
thumbnail: ""
link: https://huggingface.co/papers/2503.06492
summary: VisualSimpleQA is a new multimodal fact-seeking benchmark that allows for separate evaluation of visual and linguistic modules in large vision-language models (LVLMs). Experiments on 15 LVLMs show that state-of-the-art models like GPT-4o achieve only 60%+ correctness on VisualSimpleQA and 30%+ on its challenging subset, VisualSimpleQA-hard, indicating significant potential for improvement in both visual and linguistic modules....
opinion: placeholder
tags:
    - ML
