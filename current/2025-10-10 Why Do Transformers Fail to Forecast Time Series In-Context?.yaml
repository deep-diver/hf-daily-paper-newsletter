date: "2025-10-10"
author: Yufa Zhou
title: Why Do Transformers Fail to Forecast Time Series In-Context?
thumbnail: ""
link: https://huggingface.co/papers/2510.09776
summary: The paper analyzes why Transformers struggle with time series forecasting compared to simpler models, focusing on In-Context Learning theory. It finds that linear models perform better, Transformers can only match them with infinite context, and predictions revert to the mean with Chain-of-Thought style inference. The study offers insights for improving forecasting models and encourages deeper theoretical exploration....
opinion: placeholder
tags:
    - ML
