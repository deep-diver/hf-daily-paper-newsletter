date: "2024-06-18"
author: Jeffrey Li
title: 'DataComp-LM: In search of the next generation of training sets for language models'
thumbnail: ""
link: https://huggingface.co/papers/2406.11794
summary: DataComp for Language Models (DCLM) is a testbed for controlled dataset experiments aiming to improve language models. It provides a standardized corpus, pretraining recipes, and 53 downstream evaluations. Model-based filtering is key to assembling a high-quality training set, and the resulting DCLM-Baseline dataset enables training a 7B parameter language model with improved performance on MMLU compared to previous open-data models, while using less compute....
opinion: placeholder
tags:
    - ML
