date: "2025-10-22"
author: Ling Team
title: 'Every Attention Matters: An Efficient Hybrid Architecture for   Long-Context Reasoning'
thumbnail: ""
link: https://huggingface.co/papers/2510.19338
summary: The Ring-linear model series, including Ring-mini-linear-2.0 and Ring-flash-linear-2.0, are introduced. These hybrid architecture models integrate linear and softmax attention, reducing inference costs by up to 1/10 compared to a 32 billion parameter dense model and over 50% compared to the original Ring series, while also improving training efficiency by 50% with the linghe operator library....
opinion: placeholder
tags:
    - ML
