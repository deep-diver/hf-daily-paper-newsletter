date: "2024-02-22"
author: Jonas Geiping
title: Coercing LLMs to do and reveal (almost) anything
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/kY063XaHt9uccw74IYmn2.png
link: https://huggingface.co/papers/2402.14020
summary: This paper discusses various ways that adversarial attacks on large language models can be used to coerce the models into revealing private information, interrupting service, and other harmful behaviors, and argues that the spectrum of possible attacks is much larger than previously thought....
opinion: placeholder
tags:
    - Supervised Learning
    - Natural Language Processing
