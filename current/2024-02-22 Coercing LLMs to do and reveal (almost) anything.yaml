author: Jonas Geiping
date: '2024-02-22'
link: https://huggingface.co/papers/2402.14020
opinion: placeholder
summary: This paper discusses various ways that adversarial attacks on large language
  models can be used to coerce the models into revealing private information, interrupting
  service, and other harmful behaviors, and argues that the spectrum of possible attacks
  is much larger than previously thought....
tags:
- Supervised Learning
- Natural Language Processing
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/kY063XaHt9uccw74IYmn2.png
title: Coercing LLMs to do and reveal (almost) anything
translated_paths:
  INT: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.14020/paper.en.html
  KR: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.14020/paper.ko.html
