date: "2025-10-13"
author: Jiayu Ding
title: Information-Preserving Reformulation of Reasoning Traces for   Antidistillation
thumbnail: ""
link: https://huggingface.co/papers/2510.11545
summary: The authors present PART, a method to protect detailed reasoning traces of Large Language Models from unauthorized distillation without losing important information. PART disrupts distillation by removing self-talk and reordering sub-conclusions, causing a significant decrease in performance for student models trained on reformulated traces....
opinion: placeholder
tags:
    - ML
