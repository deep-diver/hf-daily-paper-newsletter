date: "2024-08-22"
author: Sharath Turuvekere Sreenivas
title: 'LLM Pruning and Distillation in Practice: The Minitron Approach'
thumbnail: ""
link: https://huggingface.co/papers/2408.11796
summary: A report on compressing Llama 3.1 8B and Mistral NeMo 12B models to 4B and 8B parameters using pruning and distillation. The models are then aligned and tested in instruct-tuned versions. This approach produces a 4B model from Llama 3.1 8B and a state-of-the-art Mistral-NeMo-Minitron-8B model from Mistral NeMo 12B. Fine-tuning teacher models on the distillation dataset is beneficial. The base model weights are open-sourced on Hugging Face with a permissive license....
opinion: placeholder
tags:
    - ML
