date: "2024-11-15"
author: Erik Wijmans
title: Cut Your Losses in Large-Vocabulary Language Models
thumbnail: ""
link: https://huggingface.co/papers/2411.09009
summary: The paper introduces Cut Cross-Entropy (CCE), a method that reduces the memory footprint of large-vocabulary language models during training by only computing the logit for the correct token and evaluating the log-sum-exp over all logits on the fly. This reduces the memory footprint of the loss computation and the total training-time memory consumption of the classifier head by a significant amount, without sacrificing training speed or convergence....
opinion: placeholder
tags:
    - ML
