date: "2024-01-30"
author: Bin Lin
title: 'MoE-LLaVA: Mixture of Experts for Large Vision-Language Models'
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/3ta3Rnuv4Syt6mKIu_8BE.png
link: https://huggingface.co/papers/2401.15947
summary: This paper proposes a new training strategy called MoE-tuning for Large Vision-Language Models (LVLMs) that can construct a sparse model with a constant computational cost and effectively addresses performance degradation. The paper also presents the MoE-LLaVA framework, a MoE-based sparse LVLM architecture that uniquely activates only the top-k experts through routers during deployment. MoE-LLaVA demonstrates comparable performance to the LLaVA-1.5-7B on various visual understanding datasets an...
opinion: placeholder
tags:
    - Deep Learning
    - Computer Vision
    - Natural Language Processing
