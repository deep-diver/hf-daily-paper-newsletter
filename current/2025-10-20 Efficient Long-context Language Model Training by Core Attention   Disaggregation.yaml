date: "2025-10-20"
author: Yonghao Zhuang
title: Efficient Long-context Language Model Training by Core Attention   Disaggregation
thumbnail: ""
link: https://huggingface.co/papers/2510.18121
summary: The authors propose a new method called core attention disaggregation (CAD) that improves the training of long-context large language models by separating a crucial computation step from the rest of the model. This separation allows for more efficient use of computing resources and reduces imbalances and bottlenecks, leading to faster and more effective training....
opinion: placeholder
tags:
    - ML
