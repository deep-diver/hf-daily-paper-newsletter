date: "2024-06-27"
author: Ka Man Lo
title: A Closer Look into Mixture-of-Experts in Large Language Models
thumbnail: ""
link: https://huggingface.co/papers/2406.18219
summary: This paper takes a closer look at Mixture-of-Experts (MoE) in large language models. It studies the parametric and behavioral features of three recent MoE-based models and makes some interesting observations, such as neurons acting like fine-grained experts and the router selecting experts with larger output norms. The paper also provides suggestions for MoE practitioners and aims to shed light on future research on the MoE framework and other modular architectures. Code is available at https://...
opinion: placeholder
tags:
    - ML
