date: "2025-05-13"
author: Fangyuan Yu
title: Memorization-Compression Cycles Improve Generalization
thumbnail: ""
link: https://huggingface.co/papers/2505.08727
summary: The study shows that improving language model generalization can be achieved by compressing internal representations and introducing a new training algorithm called Gated Phase Transition (GAPT). GAPT reduces representation entropy and improves cross-entropy, leading to better out-of-distribution generalization and reduced interference during training....
opinion: placeholder
tags:
    - ML
