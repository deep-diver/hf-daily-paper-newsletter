author: Philipp Singer
date: '2024-01-31'
link: https://huggingface.co/papers/2401.16818
opinion: placeholder
summary: This paper introduces H2O-Danube-1.8B, a large language model trained on
  1.8 billion parameters and 1 trillion tokens. It performs well on various benchmarks
  and an additional chat model is also released. The model is available for free under
  Apache 2.0 license, making it accessible to a wider audience....
tags: []
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/VpOXsuLx5gdgEaMrFsB2H.png
title: H2O-Danube-1.8B Technical Report
translated_paths:
  INT: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2401.16818/paper.en.html
  KR: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2401.16818/paper.ko.html
