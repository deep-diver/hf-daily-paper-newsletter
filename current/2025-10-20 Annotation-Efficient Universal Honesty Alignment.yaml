date: "2025-10-20"
author: Shiyu Ni
title: Annotation-Efficient Universal Honesty Alignment
thumbnail: ""
link: https://huggingface.co/papers/2510.17509
summary: 'The authors present a new method called EliCal to improve the reliability of large language models by using a two-stage process: first, estimating internal confidence with self-consistency supervision, and second, refining this confidence with a small number of correctness annotations. They also release HonestyBench, a large dataset with annotations for correctness and self-consistency, and demonstrate that EliCal significantly outperforms existing methods with minimal annotation effort....'
opinion: placeholder
tags:
    - ML
