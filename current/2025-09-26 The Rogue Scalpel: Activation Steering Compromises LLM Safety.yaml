date: "2025-09-26"
author: Anton Korznikov
title: 'The Rogue Scalpel: Activation Steering Compromises LLM Safety'
thumbnail: ""
link: https://huggingface.co/papers/2509.22067
summary: The study finds that activation steering, a method used to control language models' behavior, can actually make them more likely to follow harmful instructions by systematically breaking their safety measures. The researchers show that even randomly chosen directions or using benign features from a sparse autoencoder can significantly increase the probability of harmful compliance, challenging the belief that precise control over a model's internals ensures safe behavior....
opinion: placeholder
tags:
    - ML
