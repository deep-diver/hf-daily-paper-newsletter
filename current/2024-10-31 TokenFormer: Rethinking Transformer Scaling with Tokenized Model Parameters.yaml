date: "2024-10-31"
author: Haiyang Wang
title: 'TokenFormer: Rethinking Transformer Scaling with Tokenized Model Parameters'
thumbnail: ""
link: https://huggingface.co/papers/2410.23168
summary: The paper introduces TokenFormer, a scalable architecture for transformer models that treats model parameters as tokens, allowing for efficient scaling without requiring retraining from scratch. TokenFormer achieves performance comparable to transformers trained from scratch while greatly reducing training costs....
opinion: placeholder
tags:
    - ML
