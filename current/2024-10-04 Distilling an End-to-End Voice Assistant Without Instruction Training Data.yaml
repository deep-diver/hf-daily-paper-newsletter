date: "2024-10-04"
author: William Held
title: Distilling an End-to-End Voice Assistant Without Instruction Training Data
thumbnail: ""
link: https://huggingface.co/papers/2410.02678
summary: This paper introduces a new method for training Speech Large Language Models without instruction data, using the response of a text-only LLM to transcripts as self-supervision. The proposed method, called Distilled Voice Assistant (DiVA), generalizes to various tasks and is shown to better meet user preferences, achieving a 72% win rate compared with state-of-the-art models, despite using >100x less training compute....
opinion: placeholder
tags:
    - ML
