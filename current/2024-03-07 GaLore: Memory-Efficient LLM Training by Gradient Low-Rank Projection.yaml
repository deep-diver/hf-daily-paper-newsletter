date: "2024-03-07"
author: Jiawei Zhao
title: 'GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection'
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/tymX0hLyiKdpwCt7sNZ2h.png
link: https://huggingface.co/papers/2403.03507
summary: GaLore is a new training strategy for Large Language Models (LLMs) that reduces memory usage without sacrificing performance. It allows full-parameter learning and reduces memory usage by up to 82.5% while pre-training and fine-tuning. GaLore demonstrates the feasibility of pre-training a 7B model on consumer GPUs with 24GB memory....
opinion: placeholder
tags:
    - Deep Learning
