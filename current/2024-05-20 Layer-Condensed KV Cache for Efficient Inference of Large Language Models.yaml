date: "2024-05-20"
author: Haoyi Wu
title: Layer-Condensed KV Cache for Efficient Inference of Large Language Models
thumbnail: https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.10637.png
link: https://huggingface.co/papers/2405.10637
summary: This paper proposes a method to save memory and improve inference throughput for large language models by only computing and caching the key-value pairs of a small number of layers. The method is orthogonal to existing transformer memory-saving techniques and can be integrated with them for further improvement....
opinion: placeholder
tags:
    - Deep Learning
    - Natural Language Processing
    - Optimization and Learning Algorithms
    - Emerging Applications of Machine Learning
