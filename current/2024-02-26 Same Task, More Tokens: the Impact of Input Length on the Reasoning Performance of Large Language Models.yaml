author: Mosh Levy
date: '2024-02-26'
link: https://huggingface.co/papers/2402.14848
opinion: placeholder
summary: This study examines how the length of text input affects the reasoning abilities
  of large language models. The results show that performance degrades much earlier
  than the models' technical maximum, and that traditional measures like perplexity
  are not useful for predicting their success in long input tasks. The study also
  identifies common failure modes that could guide future research....
tags:
- Natural Language Processing
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/ddkmh05FqmK6CoS3gg6TZ.png
title: 'Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance
  of Large Language Models'
translated_paths:
  INT: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.14848/paper.en.html
  KR: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.14848/paper.ko.html
