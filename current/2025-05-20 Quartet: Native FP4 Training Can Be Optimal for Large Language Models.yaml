date: "2025-05-20"
author: Roberto L. Castro
title: 'Quartet: Native FP4 Training Can Be Optimal for Large Language Models'
thumbnail: ""
link: https://huggingface.co/papers/2505.14669
summary: The paper presents a new method called Quartet that enables accurate, end-to-end training of large language models in FP4 precision, which is extremely low-precision arithmetic. Quartet is optimized for NVIDIA Blackwell GPUs and can achieve state-of-the-art accuracy for FP4 precision, making fully FP4-based training a competitive alternative to standard-precision and FP8 training....
opinion: placeholder
tags:
    - ML
