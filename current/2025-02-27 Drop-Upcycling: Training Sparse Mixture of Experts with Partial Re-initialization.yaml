date: "2025-02-27"
author: Taishi Nakamura
title: 'Drop-Upcycling: Training Sparse Mixture of Experts with Partial Re-initialization'
thumbnail: ""
link: https://huggingface.co/papers/2502.19261
summary: Drop-Upcycling is a new method for training Sparse Mixture of Experts (MoE) models that solves the problem of slower training progress and suboptimal performance when using upcycling. Compared to previous MoE construction methods, Drop-Upcycling significantly outperforms them in the long term, specifically when training on hundreds of billions of tokens or more, and reduces the training FLOPs by approximately 1/4....
opinion: placeholder
tags:
    - ML
