date: "2024-01-23"
author: Alexandre Ram√©
title: 'WARM: On the Benefits of Weight Averaged Reward Models'
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/T4eMvoGUt2g-EoLiQRDqP.png
link: https://huggingface.co/papers/2401.12187
summary: The paper proposes a method called Weight Averaged Reward Models (WARM) to mitigate reward hacking in large language models (LLMs) during reinforced learning. This method fine-tunes multiple RMs and averages them in the weight space. The paper claims that the strategy improves efficiency, reliability under distribution shifts, and robustness to preference inconsistencies. The experiment results on summarization tasks show that WARM improves the overall quality and alignment of LLM predictions....
opinion: placeholder
tags:
    - Supervised Learning
    - Reinforcement Learning
    - Natural Language Processing
