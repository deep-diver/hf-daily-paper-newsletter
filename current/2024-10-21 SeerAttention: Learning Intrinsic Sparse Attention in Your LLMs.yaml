date: "2024-10-21"
author: Yizhao Gao
title: 'SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs'
thumbnail: ""
link: https://huggingface.co/papers/2410.13276
summary: This paper introduces SeerAttention, a new attention mechanism that learns to identify and select important blocks in an attention map, making it more dynamic and adaptable to language-based tasks. SeerAttention is designed to balance accuracy and speed, and it can be applied to both post-training and long-context fine-tuning. The results show that SeerAttention outperforms other sparse attention methods and offers a significant speedup when applied to long-context fine-tuning with YaRN....
opinion: placeholder
tags:
    - ML
