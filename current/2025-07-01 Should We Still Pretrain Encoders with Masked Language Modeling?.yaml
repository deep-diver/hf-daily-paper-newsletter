date: "2025-07-01"
author: Hippolyte Gisserot-Boukhlef
title: Should We Still Pretrain Encoders with Masked Language Modeling?
thumbnail: ""
link: https://huggingface.co/papers/2507.00994
summary: The study compares two methods, Masked Language Modeling (MLM) and Causal Language Modeling (CLM), for training text encoders. It finds that while MLM generally performs better, CLM is more data-efficient and stable, and a two-step training strategy using both methods can optimize performance under a fixed computational budget....
opinion: placeholder
tags:
    - ML
