date: "2024-11-26"
author: Yijiong Yu
title: LLMs Do Not Think Step-by-step In Implicit Reasoning
thumbnail: ""
link: https://huggingface.co/papers/2411.15862
summary: This study investigates whether LLMs use step-by-step reasoning in implicit Chain-of-Thought (CoT) reasoning. By analyzing hidden states, the researchers find that LLMs rarely think about intermediate steps, suggesting they rely on experience rather than strict step-by-step reasoning. The study also highlights the susceptibility and instability of LLMs' implicit reasoning capabilities, emphasizing the importance of explicit CoT for complex tasks....
opinion: placeholder
tags:
    - ML
