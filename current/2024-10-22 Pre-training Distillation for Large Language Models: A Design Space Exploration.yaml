date: "2024-10-22"
author: Hao Peng
title: 'Pre-training Distillation for Large Language Models: A Design Space Exploration'
thumbnail: ""
link: https://huggingface.co/papers/2410.16215
summary: This paper explores the use of knowledge distillation (KD) in the pre-training phase of large language models (LLMs), called pre-training distillation (PD). The authors experiment with different configurations of PD and find that larger student LLMs generally benefit more from PD, while a larger teacher LLM does not necessarily guarantee better results. The authors hope their exploration of the design space will inform future practices in pre-training distillation....
opinion: placeholder
tags:
    - ML
