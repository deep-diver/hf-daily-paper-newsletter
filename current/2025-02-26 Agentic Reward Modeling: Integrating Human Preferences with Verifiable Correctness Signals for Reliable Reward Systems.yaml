date: "2025-02-26"
author: Hao Peng
title: 'Agentic Reward Modeling: Integrating Human Preferences with Verifiable Correctness Signals for Reliable Reward Systems'
thumbnail: ""
link: https://huggingface.co/papers/2502.19328
summary: The paper proposes agentic reward modeling, a system that combines human preference rewards with verifiable signals like factuality and instruction following to create more reliable rewards. Experiments show that the proposed method, implemented as RewardAgent, outperforms existing reward models and improves the performance of LLMs trained with the DPO objective....
opinion: placeholder
tags:
    - ML
