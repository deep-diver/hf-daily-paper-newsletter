date: "2025-10-22"
author: Kai Shi
title: 'DaMo: Data Mixing Optimizer in Fine-tuning Multimodal LLMs for Mobile   Phone Agents'
thumbnail: ""
link: https://huggingface.co/papers/2510.19336
summary: The authors present DaMo, a novel solution for optimizing training data mixtures in multimodal language models for mobile phone tasks. DaMo predicts optimal data mixtures by forecasting task performance and outperforms other methods by 3.38% on PhoneAgentBench and 2.57% on other established benchmarks, while maintaining robust scalability....
opinion: placeholder
tags:
    - ML
