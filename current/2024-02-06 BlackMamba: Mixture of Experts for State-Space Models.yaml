date: "2024-02-06"
author: Quentin Anthony
title: 'BlackMamba: Mixture of Experts for State-Space Models'
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/_hCUha2m5Pn1dGEZGL_Ze.png
link: https://huggingface.co/papers/2402.01771
summary: This paper presents BlackMamba, a new model that combines the benefits of Mamba state-space models (fast and memory-efficient) with Mixture-of-Experts (cheap and fast inference). BlackMamba achieves a competitive performance in both language modeling and long sequence processing tasks, and is fully trained on a custom dataset with both 340M/1.5B and 630M/2.8B weights and checkpoints available open-source. Inference code can be found at <https://github.com/Zyphra/BlackMamba>....
opinion: placeholder
tags:
    - Supervised Learning
    - Deep Learning
    - Natural Language Processing
