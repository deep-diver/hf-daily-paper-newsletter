author: Chenxin An
date: '2024-02-28'
link: https://huggingface.co/papers/2402.17463
opinion: placeholder
summary: The paper presents Dual Chunk Attention (DCA), a technique that enables a
  large language model to process context windows of more than 100k tokens without
  continual training. DCA is an improvement over existing methods and is able to capture
  relative positional information within and across chunks, making it a viable open-source
  alternative to proprietary models....
tags:
- ML
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/D6aUg6csG2bSP1w1DP9Uk.png
title: Training-Free Long-Context Scaling of Large Language Models
translated_paths:
  INT: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.17463/paper.en.html
  KR: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.17463/paper.ko.html
