date: "2024-02-28"
author: Chenxin An
title: Training-Free Long-Context Scaling of Large Language Models
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/D6aUg6csG2bSP1w1DP9Uk.png
link: https://huggingface.co/papers/2402.17463
summary: The paper presents Dual Chunk Attention (DCA), a technique that enables a large language model to process context windows of more than 100k tokens without continual training. DCA is an improvement over existing methods and is able to capture relative positional information within and across chunks, making it a viable open-source alternative to proprietary models....
opinion: placeholder
tags:
    - ML
