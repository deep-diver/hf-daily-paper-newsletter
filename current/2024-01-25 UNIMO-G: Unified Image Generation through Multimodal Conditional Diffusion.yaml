author: Wei Li
date: '2024-01-25'
link: https://huggingface.co/papers/2401.13388
opinion: placeholder
summary: Existing text-to-image diffusion models primarily generate images from text
  prompts. However, the inherent conciseness of textual descriptions poses challenges
  in faithfully synthesizing images with intricate details, such as specific entities
  or scenes. This paper presents UNIMO-G, a simple multimodal conditional diffusion
  framework that operates on multimodal prompts with interleaved textual and visual
  inputs, which demonstrates a unified ability for both text-driven and subject-driven
  image g...
tags:
- Supervised Learning
- Deep Learning
- Computer Vision
- Natural Language Processing
- Emerging Applications of Machine Learning
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/nmk4r-A2_nqlozVwQe3cD.png
title: 'UNIMO-G: Unified Image Generation through Multimodal Conditional Diffusion'
translated_paths:
  INT: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2401.13388/paper.en.html
  KR: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2401.13388/paper.ko.html
