date: "2024-12-24"
author: Ziteng Wang
title: 'ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing'
thumbnail: ""
link: https://huggingface.co/papers/2412.14711
summary: ReMoE is a fully differentiable Mixture-of-Experts architecture that uses ReLU as the router instead of TopK+Softmax routing. It offers efficient dynamic allocation of computation across tokens and layers, and exhibits domain specialization. ReMoE consistently outperforms vanilla TopK-routed MoE across various model sizes, expert counts, and levels of granularity, and exhibits superior scalability with respect to the number of experts....
opinion: placeholder
tags:
    - ML
