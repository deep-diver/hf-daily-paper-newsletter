date: "2024-05-29"
author: Shaohua Wu
title: 'Yuan 2.0-M32: Mixture of Experts with Attention Router'
thumbnail: https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.17976.png
link: https://huggingface.co/papers/2405.17976
summary: Yuan 2.0-M32 is a mixture-of-experts model with 32 experts and an attention router for efficient expert selection. It has 3.7B active parameters, 7.4 GFlops forward computation per token, and outperforms Llama3-70B on MATH and ARC-Challenge benchmarks....
opinion: placeholder
tags:
    - Deep Learning
