date: "2025-05-26"
author: Haoyu Wang
title: Lifelong Safety Alignment for Language Models
thumbnail: ""
link: https://huggingface.co/papers/2505.20259
summary: 'The study presents a framework that enables language models to adapt to new and evolving threats. This framework consists of two components: a Meta-Attacker, which discovers novel threats, and a Defender, which resists them. The framework significantly reduces attack success rates, improving language model safety in various environments....'
opinion: placeholder
tags:
    - ML
