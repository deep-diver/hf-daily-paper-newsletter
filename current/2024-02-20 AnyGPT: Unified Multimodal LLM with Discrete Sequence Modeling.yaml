author: Jun Zhan
date: '2024-02-20'
link: https://huggingface.co/papers/2402.12226
opinion: placeholder
summary: This paper presents AnyGPT, a multimodal language model that can process
  various modalities like speech, text, images, and music using discrete representations.
  AnyGPT can be easily integrated into existing language models and performs well
  in any-to-any multimodal conversations, as demonstrated in experiments. Demos are
  available at <https://junzhan2000.github.io/AnyGPT.github.io/>....
tags:
- Deep Learning
- Natural Language Processing
- Computer Vision
- Speech Recognition and Synthesis
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/91qtWOTzD4TxxP1jvGIQG.png
title: 'AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling'
translated_paths:
  INT: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.12226/paper.en.html
  KR: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.12226/paper.ko.html
