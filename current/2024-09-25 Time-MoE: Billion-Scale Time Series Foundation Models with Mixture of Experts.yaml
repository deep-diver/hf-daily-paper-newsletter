date: "2024-09-25"
author: Xiaoming Shi
title: 'Time-MoE: Billion-Scale Time Series Foundation Models with Mixture of Experts'
thumbnail: ""
link: https://huggingface.co/papers/2409.16040
summary: The paper introduces Time-MoE, a large-scale time series foundation model that uses a sparse mixture-of-experts design to enhance computational efficiency and reduce inference costs. It's pre-trained on a large-scale dataset and achieves improved forecasting precision, outperforming dense models with the same number of activated parameters or equivalent computation budgets....
opinion: placeholder
tags:
    - ML
