date: "2025-04-14"
author: Xingjian Leng
title: 'REPA-E: Unlocking VAE for End-to-End Tuning with Latent Diffusion   Transformers'
thumbnail: ""
link: https://huggingface.co/papers/2504.10483
summary: This research addresses the question of end-to-end training for latent diffusion models and VAE tokenizers, proposing a novel training recipe named REPA-E, which utilizes the representation-alignment (REPA) loss. The proposed method not only speeds up diffusion model training by over 17x and 45x over REPA and vanilla training recipes, respectively, but also improves the VAE itself, leading to better latent space structure and downstream generation performance. REPA-E sets a new state-of-the-art ...
opinion: placeholder
tags:
    - ML
