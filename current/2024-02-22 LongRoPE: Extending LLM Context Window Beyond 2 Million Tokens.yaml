date: "2024-02-22"
author: Yiran Ding
title: 'LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens'
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/VO-TU3OZvAP3C-BSvFMeI.png
link: https://huggingface.co/papers/2402.13753
summary: The paper presents LongRoPE, a method that extends the context window of pre-trained LLMs to an impressive 2048k tokens, with up to only 1k fine-tuning steps at within 256k training lengths, by exploiting non-uniformities in positional interpolation and using a progressive extension strategy. Experiments on LLaMA2 and Mistral demonstrate the effectiveness of the method. The models extended via LongRoPE retain the original architecture with minor modifications to the positional embedding and can ...
opinion: placeholder
tags:
    - Deep Learning
    - Natural Language Processing
