date: "2024-06-13"
author: Yixin Song
title: 'Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters'
thumbnail: ""
link: https://huggingface.co/papers/2406.05955
summary: The paper introduces a novel dReLU function to improve activation sparsity in large language models (LLMs) and uses a high-quality training data mixture ratio to facilitate effective sparsification. By applying the neuron sparsification method to the Mistral and Mixtral models, the paper achieves a 2-5x decoding speedup and an inference speed of 11 tokens per second on mobile phones....
opinion: placeholder
tags:
    - ML
