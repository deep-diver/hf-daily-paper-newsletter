date: "2024-02-26"
author: Lu Ye
title: 'ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition'
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/WOW9WqXM5dHIdKaVVO-gj.png
link: https://huggingface.co/papers/2402.15220
summary: This paper presents ChunkAttention, a prefix-aware self-attention module that improves the memory utilization and speed of self-attention computation in large language models by detecting matching prefixes across multiple requests and sharing their key/value tensors in memory. This is achieved by breaking key/value tensors into smaller chunks and using a two-phase partition algorithm to improve data locality during self-attention computation. Experiments show that ChunkAttention speeds up the se...
opinion: placeholder
tags:
    - Deep Learning
    - Natural Language Processing
