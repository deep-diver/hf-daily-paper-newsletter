date: "2024-01-17"
author: Yikai Zhang
title: Extending LLMs' Context Window with 100 Samples
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/Qkvd3tER1WtHEyZa69or6.png
link: https://huggingface.co/papers/2401.07004
summary: This paper proposes a novel method to extend the context window of LLMs, which enables them to handle longer inputs more efficiently. The method adjusts RoPE's base frequency and scales the attention logits, leading to better performance and robustness across various context-demanding tasks. The paper provides insights on fine-tuning LLMs and offers code and SFT data at <https://github.com/GAIR-NLP/Entropy-ABF>....
opinion: placeholder
tags:
    - Natural Language Processing
    - Deep Learning
