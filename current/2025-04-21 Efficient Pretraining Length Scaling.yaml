date: "2025-04-21"
author: Bohong Wu
title: Efficient Pretraining Length Scaling
thumbnail: ""
link: https://huggingface.co/papers/2504.14992
summary: The research introduces a new framework called PHD-Transformer that allows for efficient length scaling during pre-training of large language models, without sacrificing inference speed. This is achieved through a unique cache management system and two optimized variants, PHD-SWA and PHD-CSWA, which improve performance while reducing pre-filling time....
opinion: placeholder
tags:
    - ML
