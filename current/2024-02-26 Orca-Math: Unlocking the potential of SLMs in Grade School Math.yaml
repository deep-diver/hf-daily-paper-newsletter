author: Arindam Mitra
date: '2024-02-26'
link: https://huggingface.co/papers/2402.14830
opinion: placeholder
summary: The paper presents Orca-Math, a 7-billion-parameter small language model
  (SLM) that achieves high accuracy in mathematical word problem-solving without the
  need for external tools, ensembling or multiple model calls. It uses a synthetic
  dataset created by multi-agent collaboration and iterative preference learning to
  surpass the performance of larger models such as LLAMA-2-70B and ChatGPT-3.5....
tags:
- Supervised Learning
- Natural Language Processing
- Deep Learning
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/SNUBmAcBKlgg0OxeTRfvH.png
title: 'Orca-Math: Unlocking the potential of SLMs in Grade School Math'
translated_paths:
  INT: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.14830/paper.en.html
  KR: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.14830/paper.ko.html
