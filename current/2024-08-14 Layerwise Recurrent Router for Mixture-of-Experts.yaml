date: "2024-08-14"
author: Zihan Qiu
title: Layerwise Recurrent Router for Mixture-of-Experts
thumbnail: ""
link: https://huggingface.co/papers/2408.06793
summary: The authors introduce a new router for Mixture-of-Experts (MoE) models called RMoE, which uses a Gated Recurrent Unit (GRU) to create connections between routing decisions across different layers. This helps improve the model's performance and compatibility with other MoE architectures. The code for RMoE is available on GitHub....
opinion: placeholder
tags:
    - ML
