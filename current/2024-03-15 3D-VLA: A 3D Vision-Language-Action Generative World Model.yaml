author: Haoyu Zhen
date: '2024-03-15'
link: https://huggingface.co/papers/2403.09631
opinion: placeholder
summary: The paper proposes 3D-VLA, a generative world model for seamless integration
  of 3D perception, reasoning, and action. It uses a 3D-based large language model
  and interaction tokens to engage with the environment, and trains a series of embodied
  diffusion models for multimodal generation. The model is trained on a large-scale
  3D embodied instruction dataset and significantly improves reasoning and planning
  capabilities in embodied environments....
tags:
- Supervised Learning
- Unsupervised Learning
- Reinforcement Learning
- Deep Learning
- Natural Language Processing
- Computer Vision
- Robotics and Control
thumbnail: https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2403.09631.png
title: '3D-VLA: A 3D Vision-Language-Action Generative World Model'
translated_paths:
  INT: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2403.09631/paper.en.html
  KR: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2403.09631/paper.ko.html
