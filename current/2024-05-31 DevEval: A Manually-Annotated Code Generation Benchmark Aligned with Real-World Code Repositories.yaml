date: "2024-05-31"
author: Jia Li
title: 'DevEval: A Manually-Annotated Code Generation Benchmark Aligned with Real-World Code Repositories'
thumbnail: https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2405.19856.png
link: https://huggingface.co/papers/2405.19856
summary: DevEval is a new benchmark for evaluating the coding abilities of Large Language Models (LLMs) in real-world code repositories. It aligns with real-world repositories, is annotated by developers, and contains comprehensive annotations. DevEval covers 10 popular domains and has 1,874 testing samples from 117 repositories. Based on DevEval, we evaluate 8 popular LLMs and analyze their shortcomings. We hope DevEval can facilitate the development of LLMs in real code repositories....
opinion: placeholder
tags:
    - Supervised Learning
    - Natural Language Processing
