date: "2025-10-21"
author: Qi Li
title: 'Reasoning Language Model Inference Serving Unveiled: An Empirical Study'
thumbnail: ""
link: https://huggingface.co/papers/2510.18672
summary: This study compares the serving performance of reasoning large language models (RLLM) to traditional LLMs, discovering differences in memory usage, request times, adaptive running time, and domain preference. The researchers then test existing optimization techniques on RLLM, finding that model quantization and speculative decoding improve efficiency with minimal accuracy loss, while prefix caching and KV cache quantization may harm accuracy or performance....
opinion: placeholder
tags:
    - ML
