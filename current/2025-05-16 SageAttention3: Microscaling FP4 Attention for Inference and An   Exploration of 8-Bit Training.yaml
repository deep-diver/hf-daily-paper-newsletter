date: "2025-05-16"
author: Jintao Zhang
title: 'SageAttention3: Microscaling FP4 Attention for Inference and An   Exploration of 8-Bit Training'
thumbnail: ""
link: https://huggingface.co/papers/2505.11594
summary: 'The study presents two advancements in attention mechanism efficiency: utilizing new FP4 Tensor Cores in Blackwell GPUs for faster attention computation, resulting in a 5x speedup over the fastest FlashAttention on RTX5090, and introducing an accurate and efficient 8-bit attention for both training and inference tasks, which achieves lossless performance in fine-tuning tasks but shows slower convergence in pretraining tasks....'
opinion: placeholder
tags:
    - ML
