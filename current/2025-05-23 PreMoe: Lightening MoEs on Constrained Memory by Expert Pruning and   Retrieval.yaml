date: "2025-05-23"
author: Zehua Pei
title: 'PreMoe: Lightening MoEs on Constrained Memory by Expert Pruning and   Retrieval'
thumbnail: ""
link: https://huggingface.co/papers/2505.17639
summary: The study identifies that experts in MoE layers specialize in specific tasks and introduces PreMoe, a framework that reduces the memory footprint of large MoE models. PreMoe uses probabilistic expert pruning and task-adaptive expert retrieval to efficiently deploy massive MoE models in memory-constrained environments, significantly reducing memory usage without sacrificing accuracy....
opinion: placeholder
tags:
    - ML
