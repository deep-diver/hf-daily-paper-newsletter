date: "2025-03-07"
author: Mia Mohammad Imran
title: Understanding and Predicting Derailment in Toxic Conversations on GitHub
thumbnail: ""
link: https://huggingface.co/papers/2503.02191
summary: This study investigates the causes of toxic conversations on GitHub and how to predict them. It creates a dataset of 202 toxic conversations and 696 non-toxic ones to identify linguistic markers and patterns that indicate derailment. Based on these findings, it proposes a moderation approach using LLMs to detect potentially harmful conversations, achieving a 69% F1-Score in predicting derailment....
opinion: placeholder
tags:
    - ML
