date: "2025-03-03"
author: Zhixuan Lin
title: 'Forgetting Transformer: Softmax Attention with a Forget Gate'
thumbnail: ""
link: https://huggingface.co/papers/2503.02130
summary: The study introduces a new attention mechanism, Forgetting Attention, for Transformer models, which incorporates a forget gate. This results in a model called Forgetting Transformer (FoX) that outperforms the original Transformer in several tasks, such as long-context language modeling and length extrapolation, while retaining long-context capabilities over recurrent sequence models. Additionally, a 'Pro' block design is proposed to improve the performance of both FoX and the Transformer....
opinion: placeholder
tags:
    - ML
