date: "2024-08-28"
author: Junxiong Wang
title: 'The Mamba in the Llama: Distilling and Accelerating Hybrid Models'
thumbnail: ""
link: https://huggingface.co/papers/2408.15237
summary: This paper introduces a method to convert large Transformer models into smaller, more efficient linear RNN models. The resulting hybrid model performs well in chat benchmarks and can be accelerated using a new hardware-aware decoding algorithm. The authors show that their top-performing model, distilled from Llama3-8B-Instruct, outperforms other linear RNN models and even GPT-4 in some benchmarks....
opinion: placeholder
tags:
    - ML
