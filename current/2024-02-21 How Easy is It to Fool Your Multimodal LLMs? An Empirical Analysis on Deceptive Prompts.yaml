date: "2024-02-21"
author: Yusu Qian
title: How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive Prompts
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/S2qy3Sw_vQjElbg1-hyt7.png
link: https://huggingface.co/papers/2402.13220
summary: The paper presents a benchmark called MAD-Bench to evaluate the vulnerability of Multimodal Large Language Models to deceptive prompts. The benchmark contains 850 test samples divided into 6 categories, and the paper presents a remedy that adds an additional paragraph to the deceptive prompts to encourage models to think twice before answering the question....
opinion: placeholder
tags:
    - Supervised Learning
    - Natural Language Processing
    - Computer Vision
    - Speech Recognition and Synthesis
