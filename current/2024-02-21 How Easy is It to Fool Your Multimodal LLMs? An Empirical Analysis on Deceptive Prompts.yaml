author: Yusu Qian
date: '2024-02-21'
link: https://huggingface.co/papers/2402.13220
opinion: placeholder
summary: The paper presents a benchmark called MAD-Bench to evaluate the vulnerability
  of Multimodal Large Language Models to deceptive prompts. The benchmark contains
  850 test samples divided into 6 categories, and the paper presents a remedy that
  adds an additional paragraph to the deceptive prompts to encourage models to think
  twice before answering the question....
tags:
- Supervised Learning
- Natural Language Processing
- Computer Vision
- Speech Recognition and Synthesis
thumbnail: https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/S2qy3Sw_vQjElbg1-hyt7.png
title: How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive
  Prompts
translated_paths:
  INT: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.13220/paper.en.html
  KR: https://raw.githack.com/deep-diver/hf-daily-paper-newsletter/main/translated-papers/2402.13220/paper.ko.html
